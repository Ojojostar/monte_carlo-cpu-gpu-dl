{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model time!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src = pics/OIP.jpg width = 400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "cupy.cuda.set_allocator(None)       # no clue\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "\n",
    "import numba\n",
    "from numba import cuda\n",
    "\n",
    "import os, os.path\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset\n",
    "Each monte carlo simulation run is equivalent to one data point being made, so to generate a large dataset, we have to run monte carlo simulations lots of times, and batches can hypotheitcally make doing this faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here,the mc model from mc_snow, cuda version was imported and cleaned up a bit.\n",
    "note that due to the existence of batches, some of the varaibles now need a bit of extra finagling to access properly. (s_0, Ki, Ko, mu, sigma, pot,r, d_normals, snowball_path_holder). Overall design is very close to original, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit               # defualt GPU\n",
    "def monte_carlo_andtheholygrail_gpu(d_s, s_0, Ki, Ko, mu, sigma, pot,r,\n",
    "                                    d_normals, snowball_path_holder, MONTHS,\n",
    "                                    N_STEPS, N_PATHS, N_BATCH):\n",
    "    \n",
    "\n",
    "    # for shared memory (non)optimization\n",
    "    # shared = cuda.shared.array(shape=0, dtype=numba.float32)\n",
    "    # # load to shared memory\n",
    "    # path_offset = cuda.blockIdx.x * cuda.blockDim.x\n",
    "\n",
    "    # ii - overall thread index\n",
    "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
    "\n",
    "    for n in range(ii, N_PATHS * N_BATCH, stride):\n",
    "        # newly added vars for N_BATCH calculations\n",
    "        batch_id = n // N_PATHS\n",
    "        path_id = n % N_PATHS       # equivalent to n in old code \n",
    "\n",
    "        snowball_path_holder[n][0] = s_0[batch_id]\n",
    "        earlyexit = False\n",
    "        ki = False\n",
    "        mald = False\n",
    "        for t in range(N_STEPS):\n",
    "            # pre shared memory b_motion    \n",
    "            #                                                   \n",
    "            b_motion = d_normals[path_id + batch_id * N_PATHS +  t * N_PATHS * N_BATCH]\n",
    "\n",
    "            # post shared memory b_motion\n",
    "            # shared[cuda.threadIdx.x] = d_normals[path_offset + cuda.threadIdx.x + t * N_PATHS]\n",
    "\n",
    "            dt = 1/N_STEPS\n",
    "            # pre shared memory b_motion\n",
    "            ds = snowball_path_holder[n][t] * mu[batch_id] * dt + snowball_path_holder[n][t] \\\n",
    "                                                * sigma[batch_id] * b_motion * math.sqrt(dt) \n",
    "            # post shared memory b_motion\n",
    "            # ds = snowball_path_holder[n][t] * mu[batch_id] * dt + snowball_path_holder[n][t] * sigma[batch_id] * shared[cuda.threadIdx.x] * math.sqrt(dt) \n",
    "                    # no adjusting list sizes in cuda :(\n",
    "                    \n",
    "            snowball_path_holder[n][t+1] = snowball_path_holder[n][t] + ds\n",
    "            \n",
    "            if snowball_path_holder[n][t+1] <= Ki[batch_id]:\n",
    "                ki = True\n",
    "\n",
    "            if not mald:\n",
    "                for month in (0,1,2,3,4,5,6,7,8,9,10,11):                # need to do this instead because contains (in) and range are disabled\n",
    "                    if t+1 == MONTHS[month]:     #startday no longer used to fake a start date in code\n",
    "                        if snowball_path_holder[n][t+1] >= Ko[batch_id]:\n",
    "                            price =  pot[batch_id] * t/365     # should turn t into int\n",
    "                            # return snowball_path, price\n",
    "                            d_s[n] =  price * math.exp(-r[batch_id] * t/N_STEPS)   # accounting for r\n",
    "                            snowball_path_holder[n][-1] = d_s[n]            \n",
    "                            earlyexit = True\n",
    "                            mald = True\n",
    "                            break\n",
    "            else: # if mald\n",
    "                break\n",
    "        \n",
    "        if not earlyexit:       # to prevent early exit getting out of bdds error\n",
    "            # did not get knocked up or down\n",
    "            price = pot[batch_id]\n",
    "            if ki and snowball_path_holder[n][N_STEPS] <= s_0[batch_id]:          # blo got knocked down and never recovered\n",
    "                price = snowball_path_holder[n][N_STEPS] - s_0[batch_id]\n",
    "            elif ki and snowball_path_holder[n][N_STEPS] <= Ko[batch_id]:          # blo got knocked down for a bit but finished above Ki\n",
    "                price =0\n",
    "            d_s[n] = price * math.exp(-r[batch_id])\n",
    "            snowball_path_holder[n][-1] = d_s[n]    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for some reason, increasing the number of paths and/or the number of batches greatly slows down my monte's carlo's computational speed. I have no definitive proof that this is the case, but I strongly belive it to be because threads are becoming unsynched as the code runs on, making both greater paths and greater batches than my current settings have much slower run times than their current values. Not that my current code isn't slower than it should be, either. <br>\n",
    " Max len controls the number of data points, path controls how accurate each data point is. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once finished running, data is saved into a directory.\n",
    "<br>\n",
    "If you think running a large number like 1 mil mcs takes way too long, throw these first three cells into a python file (datasetgen.py) and let it run in its own terminal while going forward in the notebook with a smaller set for test purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for demonstration purposes, only a small amount of data is generated here. If large amounts of data are to be generated, **PLEASE** go to ***datasetgen.py*** instead. there is much more stuff there that isnt incorporated here since i dont like scrolling htat much that would make generating data a bit easier (generates data in chunks so that its safer, ability to run mutliple process of program at the same time thru currnumm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding files starting from 101\n",
      "Num batches: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.882806777954102]\n",
      "[2.3875019550323486]\n",
      "[5.110105991363525]\n",
      "[2.0633299350738525]\n",
      "[2.978761672973633]\n",
      "[2.733570098876953]\n",
      "[2.55251407623291]\n",
      "[2.243455648422241]\n",
      "[2.852839469909668]\n",
      "[3.885261058807373]\n",
      "time 2.7585599422454834 v 3.885261 avg time 5.517119884490967e-06\n",
      "[]\n",
      "tensor([])\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "#               make sure max_len is large enough or else divide by zero error occurs (at least 100 batches must be run)\n",
    "limiter = True\n",
    "# max_len = 1000000              \n",
    "max_len = 10                 # test value\n",
    "number_path = 500000\n",
    "batch = 1\n",
    "threads = 256\n",
    "seed  =1999 \n",
    "num = 0\n",
    "max_length = max_len\n",
    "N_PATHS = number_path\n",
    "N_STEPS = 365\n",
    "N_BATCH  =batch\n",
    "\n",
    "max_length = max_length // N_BATCH\n",
    "percenter  =100\n",
    "percent = max_length // percenter\n",
    "\n",
    "#           uncomment if u want less batches, the percent will just be wrong\n",
    "if percent == 0:\n",
    "    percent = 1\n",
    "\n",
    "# we will not be calculating a starting date since the difference is negligible and I aint rigging up\n",
    "# a system to check if a certain day is a weekend or not\n",
    "MONTHS = cupy.asnumpy([0, 31,59,90,120,151,181,212,243, 273,304,334])\n",
    "snowball_path_holder =  np.zeros(N_BATCH*N_PATHS, dtype=(np.float32,N_STEPS+1))\n",
    "output = cupy.zeros(N_BATCH*N_PATHS, dtype = cupy.float32)\n",
    "num_blocks  =(N_PATHS * N_BATCH -1) // threads +1\n",
    "num_threads = threads\n",
    "\n",
    "Xss = []\n",
    "Yss = []\n",
    "\n",
    "currnum = len(os.listdir('snow_data_tensor_train'))//2+1\n",
    "print(\"Adding files starting from\", currnum)\n",
    "\n",
    "print(\"Num batches:\", N_BATCH)\n",
    "\n",
    "# making sure self.snowball_path_holder is zeroed to avoid bug\n",
    "# self.snowball_path_holder.fill(0)\n",
    "s = time.time()\n",
    "\n",
    "for i in range(1,max_length+1):\n",
    "        randoms = cupy.random.normal(0,1, N_BATCH * N_PATHS * N_STEPS, dtype= cupy.float32)\n",
    "\n",
    "        Xpre = cupy.random.rand(N_BATCH, 7, dtype = cupy.float32)\n",
    "        #                        s_0,  Ki, Ko,  mu, sigma, pot, r\n",
    "        Xpre = Xpre * cupy.array([4,  -2,  1,  .01,  .15,  10, .01], dtype=cupy.float32)\n",
    "        X = Xpre +    cupy.array([8,   0,  0,  .02, .275,  15, .02], dtype=cupy.float32)\n",
    "        # Ki and Ko will be set down here instead of the previous line to make them relative to s_0.\n",
    "        X[:, 1] = X[:,0] -1         # overriding Ki and Ko \n",
    "        X[:, 2] = X[:,0] -.2        \n",
    "        X[:, 1] += Xpre[:,1]        # adding back the offset in Xpre after it gets overrided\n",
    "        X[:, 2] += Xpre[:,2] \n",
    "\n",
    "        snowball_path_holder.fill(0)\n",
    "                                        # d_s, s_0, Ki, Ko, mu, sigma, pot,r,\n",
    "                                        # d_normals, snowball_path_holder, MONTHS,\n",
    "                                        # N_STEPS, N_PATHS, N_BATCH):\n",
    "        monte_carlo_andtheholygrail_gpu[(num_blocks,), (num_threads,)](\n",
    "                                        output, X[:, 0], X[:, 1], X[:, 2], X[:, 3], \n",
    "                                        X[:, 4], X[:, 5], X[:, 6],\n",
    "                                        randoms, snowball_path_holder, MONTHS,\n",
    "                                        N_STEPS, N_PATHS, N_BATCH)\n",
    "        # o = output.reshape(N_BATCH, N_PATHS)\n",
    "        # Y  =o.mean(axis =1)         # getting the average of each batch\n",
    "        Y = output.mean()\n",
    "        # Y = output\n",
    "        X = X.mean(axis=0)\n",
    "        Xss.append(X.tolist())\n",
    "        Yss.append(Y.tolist())\n",
    "        print(Yss)\n",
    "        # Xss.append(X)\n",
    "        # Yss.append(Y)\n",
    "\n",
    "        # have following turned off. go to datasetgen.py for a better view.\n",
    "        # if(i%percent==0):\n",
    "        #     if limiter:\n",
    "        #         if currnum > percenter:\n",
    "        #             print(\"premature exit, burunyu~\")\n",
    "        #             break\n",
    "        #     e = time.time()\n",
    "        #     print(i/(percent), \"percent of the way there! Time is now:\", (e-s)/60/60, \"hours\")\n",
    "        #     # print(i/(percent*10), \"percent of the way there! Time is now:\", e-s, \"secs\")\n",
    "        #     print(\"now saving tsnowX_{}.pt\".format(currnum) )\n",
    "        #     tensorX = np.array(Xss)\n",
    "        #     tensorY = np.array(Yss)\n",
    "        #     tensorX = torch.Tensor(tensorX)\n",
    "        #     tensorY = torch.Tensor(tensorY)\n",
    "        #     torch.save(tensorX, f\"snow_data_tensor_train/tsnowX_{currnum}.pt\")\n",
    "        #     torch.save(tensorY, f\"snow_data_tensor_train/tsnowY_{currnum}.pt\")\n",
    "        #     Xss.clear()\n",
    "        #     Yss.clear()\n",
    "        #     currnum += 1\n",
    "\n",
    "        num+=1          #actually useless\n",
    "        # print((from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))) \n",
    "        \n",
    "        Xss.clear()\n",
    "        Yss.clear()\n",
    "\n",
    "v = output.mean()\n",
    "cuda.synchronize()\n",
    "e = time.time()\n",
    "print('time', e-s, 'v', v, 'avg time', (e-s)/500000)\n",
    "\n",
    "## i have following turned off but feel free to do stuff with it\n",
    "\n",
    "# torch.save(tensorX, \"snow_data_tensor_train/tsnowX.pt\")\n",
    "# torch.save(tensorY, \"snow_data_tensor_train/tsnowY.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small tangent on datasetgen\n",
    "So... after a lot of finagling with datasetgen.py, I have realized that it is most certianly the case that the code runs slower due to needing to run 500000 paths before it can synch back up again, causing great slowdown, espeicaly with larger batch/path numbers. However, this does not prevent us from running multiple different processes of the same code, filling up the gpu with power of more processes instead. Though there is some slowdown caused by having more processes, there is an almost 2 times improvement over runnign one process of the code when using 3 processes. Thats a lot! Of course, if you can get more processes runnign without gpu's memory going to 100% and locking up the program for extended periods of time, this should allow for as much speedup as you would get by using all of your gpu???? I think???? Ive been runnign datasetgen in its own power shells to prevent restarts of vs code from restarting it (pylance keeps crashing >:( ), but this can obviously be turned all into a single program that creates as many processes as you want! Therefore there is also datasetgen_multi.py which can do exactly that! its a slight modification of datasetgen.py but is a big quality of life update, greatly lowering the amount of power shells i need to open every time i run the program (u cant close em while they are runnign since they are locked in and dont like to stop at keyboard interrupts)\n",
    "\n",
    "<img src = pics\\big_scary_hacker_man.jpg width = 700> <br>\n",
    "\n",
    "**Fig 1. Me being big scary hacker man 😱** (6 process was a mistake and now all of them are frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to regularly scheduled programming (loading dataset made by **datasetgen(_multi).py**)\n",
    "Loading the data to confirm its existence! turning it into gpu monsters to prepare for throwing it into model to train! can prolly be done be done before saving, thouhg :/ <br>\n",
    "oh well, it takes like no time to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0785e+01, 9.7630e+00, 1.0941e+01,  ..., 4.1676e-01, 1.8827e+01,\n",
      "         2.7015e-02],\n",
      "        [1.1214e+01, 8.3553e+00, 1.1372e+01,  ..., 2.9712e-01, 1.7451e+01,\n",
      "         2.6089e-02],\n",
      "        [8.1927e+00, 6.3303e+00, 8.4115e+00,  ..., 4.1421e-01, 2.0627e+01,\n",
      "         2.6026e-02],\n",
      "        ...,\n",
      "        [8.1546e+00, 5.6825e+00, 8.4402e+00,  ..., 4.2203e-01, 2.1776e+01,\n",
      "         2.4651e-02],\n",
      "        [9.7141e+00, 7.2488e+00, 9.5389e+00,  ..., 2.9207e-01, 2.4110e+01,\n",
      "         2.0016e-02],\n",
      "        [8.2834e+00, 5.3780e+00, 8.1775e+00,  ..., 2.8856e-01, 1.7493e+01,\n",
      "         2.0874e-02]])\n",
      "torch.Size([4997559, 7])\n",
      "tensor([2.2016, 2.8075, 2.7976,  ..., 3.4159, 3.4042, 3.5705])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms, utils, datasets\n",
    "import os\n",
    "\n",
    "path = \"snow_data\"\n",
    "\n",
    "finnum = len(os.listdir(f\"{path}\"))//2+1          # equivalent to currnum\n",
    "\n",
    "tensor_x_L = torch.load(f\"{path}/tsnowX_1.pt\")\n",
    "tensor_y_L = torch.load(f\"{path}/tsnowY_1.pt\")\n",
    "\n",
    "\n",
    "for tensor_num  in range(2, finnum):\n",
    "    tensor_x_R = torch.load(f\"{path}/tsnowX_{tensor_num}.pt\")\n",
    "    tensor_y_R = torch.load(f\"{path}/tsnowY_{tensor_num}.pt\")   \n",
    "\n",
    "    #   cat left side with right side (kiara and kamma???)\n",
    "    tensor_x_L = torch.cat((tensor_x_L, tensor_x_R), 0)\n",
    "    tensor_y_L = torch.cat((tensor_y_L, tensor_y_R), 0)\n",
    "\n",
    "print(tensor_x_L)\n",
    "print(tensor_x_L.size())\n",
    "print(tensor_y_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also split the data into more train, validation, and test before saving once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_data = tensor_x_L.size()[0]\n",
    "num_train = int(num_data*.6)\n",
    "num_val = int(num_data*.8)\n",
    "\n",
    "train_x, val_x, test_x = torch.tensor_split(tensor_x_L, (num_train, num_val))\n",
    "train_y, val_y, test_y = torch.tensor_split(tensor_y_L, (num_train, num_val))\n",
    "\n",
    "# print(train.size())\n",
    "\n",
    "\n",
    "dir_p = \"snow_data_processed\"\n",
    "\n",
    "torch.save(train_x, f\"{dir_p}\\\\train_x.pt\")\n",
    "torch.save(val_x, f\"{dir_p}\\\\val_x.pt\")\n",
    "torch.save(test_x, f\"{dir_p}\\\\test_x.pt\")\n",
    "\n",
    "torch.save(train_y, f\"{dir_p}\\\\train_y.pt\")\n",
    "torch.save(val_y, f\"{dir_p}\\\\val_y.pt\")\n",
    "torch.save(test_y, f\"{dir_p}\\\\test_y.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to load the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "dir_p = \"snow_data_processed\"\n",
    "batch_size = 100\n",
    "\n",
    "train_x = torch.load(f\"{dir_p}\\\\train_x.pt\")\n",
    "train_y = torch.load(f\"{dir_p}\\\\train_y.pt\")\n",
    "\n",
    "test_x = torch.load(f\"{dir_p}\\\\test_x.pt\")\n",
    "test_y = torch.load(f\"{dir_p}\\\\test_y.pt\")\n",
    "\n",
    "val_x = torch.load(f\"{dir_p}\\\\val_x.pt\")\n",
    "val_y = torch.load(f\"{dir_p}\\\\val_y.pt\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tr_set = TensorDataset(train_x.float().to(device),train_y.float().to(device)) # create your datset\n",
    "te_set = TensorDataset(test_x.float().to(device),test_y.float().to(device)) \n",
    "val_set = TensorDataset(val_x.float().to(device),val_y.float().to(device)) \n",
    "\n",
    "tr_loader = DataLoader(tr_set, batch_size)\n",
    "te_loader = DataLoader(te_set, batch_size)\n",
    "val_loader = DataLoader(val_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some reason, the guy who made the nvidia notebook decided it was a good idea to make the dataset generate itself while training, making the training process significantly slower than if the data was already prepared already. This works, but is not that good. I would not reccomend, 2/5 stars. <br>\n",
    "edit: he does it the better way in their next notbook :/<br>\n",
    "still leaving this in since its interesting to look at tho. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SnowballDataSet(object):\n",
    "\n",
    "    def __init__(self, max_len = 10, number_path = 1000, batch = 2, threads = 512, seed  =1999 ):\n",
    "        self.num = 0\n",
    "        self.max_length = max_len\n",
    "        self.N_PATHS = number_path\n",
    "        self.N_STEPS = 365\n",
    "        self.N_BATCH  =batch\n",
    "        # we will not be calculating a starting date since the difference is negligible and I aint rigging up\n",
    "        # a system to check if a certain day is a weekend or not\n",
    "        self.MONTHS = cupy.asnumpy([0, 31,59,90,120,151,181,212,243, 273,304,334])\n",
    "                # SHOULD THIS BE NP ARRAY INSTEAD????\n",
    "        self.snowball_path_holder =  np.zeros(self.N_BATCH*self.N_PATHS, dtype=(np.float32,self.N_STEPS+1))# extra 1 is no longer for storing payoff\n",
    "        # self.snowball_path_holder = cupy.array(self.snowball_path_holder)\n",
    "        # self.T  = np.float(365.0)         # nah id lose. \n",
    "        self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype = cupy.float32)\n",
    "        self.num_blocks  =(self.N_PATHS * self.N_BATCH -1) // threads +1\n",
    "        self.num_threads = threads\n",
    "\n",
    "        #  temp_months, snowball_path_holder both added now\n",
    "        cupy.random.seed(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.num = 0\n",
    "        return self\n",
    "\n",
    "    #   next basically takes the place of the cell running the mc. As such need to generate\n",
    "     # (d_s, s_0, Ki, Ko, mu, sigma,  pot,r, d_normals, snowball_path_holder, MONTHS, N_STEPS, N_PATHS, N_BATCH\n",
    "     # note that all but s_0, Ki, Ko, mu, sigma,  pot,r, d_normals have been generated in init due to their nonrandom nature\n",
    "    def __next__(self):\n",
    "        if self.num > self.max_length: \n",
    "            raise StopIteration      # nvidia notebook uses raise StopIteration here but p sure its deprecated???\n",
    "                                      # is used because return returns an extra None\n",
    "        # generating the variables\n",
    "        # d_normals\n",
    "        randoms = cupy.random.normal(0,1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype= cupy.float32)\n",
    "\n",
    "        Xpre = cupy.random.rand(self.N_BATCH, 7, dtype = cupy.float32)\n",
    "        #                        s_0,  Ki, Ko,  mu, sigma, pot, r\n",
    "        Xpre = Xpre * cupy.array([4,  -2,  1,  .01,  .15,  10, .01], dtype=cupy.float32)\n",
    "        X = Xpre +    cupy.array([8,   0,  0,  .02, .275,  15, .02], dtype=cupy.float32)\n",
    "        \n",
    "        # Ki and Ko will be set down here instead of the previous line to make them relative to s_0.\n",
    "        X[:, 1] = X[:,0] -1         # overriding Ki and Ko \n",
    "        X[:, 2] = X[:,0] -.2        \n",
    "        # print(X)\n",
    "        X[:, 1] += Xpre[:,1]        # adding back the offset in Xpre after it gets overrided\n",
    "        X[:, 2] += Xpre[:,2] \n",
    "\n",
    "        # making sure self.snowball_path_holder is zeroed to avoid bug\n",
    "        self.snowball_path_holder.fill(0)\n",
    "\n",
    "                                        # d_s, s_0, Ki, Ko, mu, sigma, pot,r,\n",
    "                                        # d_normals, snowball_path_holder, MONTHS,\n",
    "                                        # N_STEPS, N_PATHS, N_BATCH):\n",
    "        monte_carlo_andtheholygrail_gpu[(self.num_blocks,), (self.num_threads,)](\n",
    "                                        self.output, X[:, 0], X[:, 1], X[:, 2], X[:, 3], \n",
    "                                        X[:, 4], X[:, 5], X[:, 6],\n",
    "                                        randoms, self.snowball_path_holder, self.MONTHS,\n",
    "                                        self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
    "        \n",
    "        o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
    "        Y  =o.mean(axis =1)         # getting the average of each batch\n",
    "        self.num+=1\n",
    "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a small test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SnowballDataSet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ds = SnowballDataSet(10, number_path=500000, batch=16, seed=15)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m SnowballDataSet(\u001b[38;5;241m10\u001b[39m, number_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500000\u001b[39m, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ds:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# print(i, \"\\n\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;241m0\u001b[39m],i[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SnowballDataSet' is not defined"
     ]
    }
   ],
   "source": [
    "# ds = SnowballDataSet(10, number_path=500000, batch=16, seed=15)\n",
    "ds = SnowballDataSet(10, number_path=500000, batch=1, seed=15)\n",
    "for i in ds:\n",
    "    # print(i, \"\\n\")\n",
    "    print(i[0],i[1])     # printing the Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Erm pretty default model. Just making it have functionality. normalizing it accoriding to the average value of all of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting snow_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile snow_model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, hidden)\n",
    "        self.fc4 = nn.Linear(hidden, hidden)\n",
    "        self.fc5 = nn.Linear(hidden, hidden)\n",
    "        self.fc6 = nn.Linear(hidden, hidden)\n",
    "        self.fc7 = nn.Linear(hidden, 1)\n",
    "        self.register_buffer('norm',\n",
    "                             torch.tensor([10.0,\n",
    "                                           8.5,\n",
    "                                           10.4,\n",
    "                                           0.025,\n",
    "                                           0.35,\n",
    "                                           20,\n",
    "                                           0.025]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # normalize the parameter to range [0-1] \n",
    "        x = x / self.norm           # normalizing params makes for higher accuracy\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.elu(self.fc3(x))\n",
    "        x = F.elu(self.fc4(x))\n",
    "        x = F.elu(self.fc5(x))\n",
    "        x = F.elu(self.fc6(x))\n",
    "        return self.fc7(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "as the amount of data is relatively small, a smallish batch size will be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda:0\n",
      "2998535\n",
      "loss 0.22300417721271515 average time 0.002368915173364407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 88\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Error: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvg loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# @trainer.on(Events.GET_BATCH_STARTED)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# def log_training_loss(engine):\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m#     print(\"EPOCH!!!!!!!!!!!!\\n\")\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m trainer\u001b[38;5;241m.\u001b[39mrun(tr_loader, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:889\u001b[0m, in \u001b[0;36mEngine.run\u001b[1;34m(self, data, max_epochs, epoch_length)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupt_resume_enabled:\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run()\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:932\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_as_gen()\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator)\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:990\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 990\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(e)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:644\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:956\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_engine()\n\u001b[1;32m--> 956\u001b[0m epoch_time_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[0;32m    958\u001b[0m \u001b[38;5;66;03m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m epoch_time_taken\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:1077\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_STARTED)\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[1;32m-> 1077\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_COMPLETED)\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n",
      "Cell \u001b[1;32mIn[166], line 39\u001b[0m, in \u001b[0;36mtrain_update\u001b[1;34m(engine, batch)\u001b[0m\n\u001b[0;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred[:,\u001b[38;5;241m0\u001b[39m], y)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# assert y_pred.dtype is torch.float16 ##################\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# assert loss.dtype is torch.float32 ##################\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     40\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     41\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import Timer\n",
    "\n",
    "from ignite.handlers import CosineAnnealingScheduler\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler    # defunct\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from torch.cuda import amp       # apex.amp is deprecated. it cannot be regenerated.\n",
    "\n",
    "from snow_model import Net\n",
    "# from snow_model_module import Net\n",
    "# from snow_model_2 import Net\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used : {device}\")\n",
    "\n",
    "# from cupy_dataset import OptionDataSet\n",
    "timer = Timer(average=True)\n",
    "model = Net().cuda()\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "set_amp = True\n",
    "scaler = amp.GradScaler(enabled=set_amp)\n",
    "\n",
    "def train_update(engine, batch):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16): ########### automatic mixed precision\n",
    "        model.train()\n",
    "        # optimizer.zero_grad()\n",
    "        optimizer.zero_grad(set_to_none=True) # set_to_none=True here can modestly improve performance \n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred[:,0], y)\n",
    "        # assert y_pred.dtype is torch.float16 ##################\n",
    "        # assert loss.dtype is torch.float32 ##################\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True) # set_to_none=True here can modestly improve performance \n",
    "    # optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "print(len(tr_loader.dataset))\n",
    "trainer = Engine(train_update)\n",
    "# log_interval = 1000\n",
    "interval_count  = 5\n",
    "log_interval = len(tr_loader.dataset)//batch_size//interval_count\n",
    "\n",
    "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(tr_loader))   # length of a batch\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)  ## restarts the learning rate of cosine; each iteration is 1 batch long\n",
    "timer.attach(trainer,\n",
    "             start=Events.EPOCH_STARTED,\n",
    "             resume=Events.ITERATION_STARTED,\n",
    "             pause=Events.ITERATION_COMPLETED,\n",
    "             step=Events.ITERATION_COMPLETED)    \n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(tr_loader) + 1\n",
    "    if iter % log_interval == 0:\n",
    "        print('loss', engine.state.output, 'average time', timer.value())\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_test_loss(engine):\n",
    "    size = len(te_loader.dataset)\n",
    "    num_batches = len(te_loader)\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in te_loader:\n",
    "            # X,y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred[:,0],y).item() #item converts tensor to number\n",
    "            # print(y)\n",
    "            # print(pred)       # has some interesting results showing how argmax\n",
    "            # print(test_loss)\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\nAvg loss: {test_loss:>8f}\\n\")\n",
    "\n",
    "# @trainer.on(Events.GET_BATCH_STARTED)\n",
    "# def log_training_loss(engine):\n",
    "#     print(\"EPOCH!!!!!!!!!!!!\\n\")\n",
    "        \n",
    "trainer.run(tr_loader, max_epochs=5)\n",
    "# trainer.run(dataset, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent run using nn.Sequential for funsies. <br>\n",
    "nn.Module is also included for a trainerless comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "hidden = 1024\n",
    "\n",
    "mai_normals = [10.0, 8.5,10.4, 0.025, 0.35, 20, 0.025]\n",
    "cuda0 = torch.device(\"cuda:0\")\n",
    "\n",
    "class Normalizer(nn.Module):\n",
    "    def __init__(self, normals):\n",
    "        super(Normalizer,self).__init__()\n",
    "        self.norms = torch.tensor(normals, device=cuda0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / self.norms\n",
    "        # print(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model_o = nn.Sequential(\n",
    "    Normalizer(mai_normals),\n",
    "    nn.Linear(7, hidden),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(hidden, hidden),          #\n",
    "    nn.ELU(),                           # nn.ELU used for sequential\n",
    "    nn.Linear(hidden, hidden),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(hidden, hidden),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(hidden, hidden),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(hidden, hidden),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(hidden, 1),\n",
    "    nn.ELU(),\n",
    "    ).cuda()\n",
    "\n",
    "class Neth(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden=1024):\n",
    "        super(Neth, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, hidden)\n",
    "        self.fc4 = nn.Linear(hidden, hidden)\n",
    "        self.fc5 = nn.Linear(hidden, hidden)\n",
    "        self.fc6 = nn.Linear(hidden, hidden)\n",
    "        self.fc7 = nn.Linear(hidden, 1)\n",
    "        self.register_buffer('norm',\n",
    "                             torch.tensor([10.0,\n",
    "                                           8.5,\n",
    "                                           10.4,\n",
    "                                           0.025,\n",
    "                                           0.35,\n",
    "                                           20,\n",
    "                                           0.025]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # normalize the parameter to range [0-1] \n",
    "        x = x / self.norm           # normalizing params makes for higher accuracy\n",
    "        # print(x)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.elu(self.fc3(x))      # F.elu used for forward\n",
    "        x = F.elu(self.fc4(x))\n",
    "        x = F.elu(self.fc5(x))\n",
    "        x = F.elu(self.fc6(x))\n",
    "        return self.fc7(x)\n",
    "    \n",
    "model_h = Neth().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda:0\n",
      "loss: 11.361467 [100]\n",
      "loss: 0.737593 [100100]\n",
      "loss: 1.192102 [200100]\n",
      "loss: 0.695697 [300100]\n",
      "loss: 0.792687 [400100]\n",
      "loss: 0.667904 [500100]\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model_h.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(te_loader), 1e-6, )\n",
    "scheduler2 = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(te_loader))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used : {device}\")\n",
    "\n",
    "\n",
    "for ep in range(epoch):\n",
    "    model_h.train()\n",
    "    for batch, (x,y) in enumerate(te_loader):\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        pred = model_h(x)\n",
    "        loss = loss_fn( y, pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch %1000 ==0:\n",
    "            loss, current = loss.item(), (batch+1)*len(x)\n",
    "            print(f\"loss: {loss:>7f} [{current}]\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us test how good our model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      "Avg loss: 0.014540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "size = len(val_loader.dataset)\n",
    "num_batches = len(val_loader)\n",
    "model.eval()\n",
    "test_loss=0\n",
    "with torch.no_grad():\n",
    "    for X,y in val_loader:\n",
    "        # X,y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        test_loss += loss_fn(pred[:,0],y).item() #item converts tensor to number\n",
    "        # print(y)\n",
    "        # print(pred[:,0])       # has some interesting results showing how argmax\n",
    "        # print(test_loss)\n",
    "test_loss /= num_batches\n",
    "print(f\"Test Error: \\nAvg loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the validation result is pretty close to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tryhard Mode\n",
    "hopefully i am comptent at coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda:0\n",
      "~~MEMORY RESTORED~~\n",
      "LOSS REVERTED ∞ -> 0.013436572626233101\n",
      "loss 0.2563014030456543 average time 0.0023578070366619686\n",
      "loss 0.029935473576188087 average time 0.0023471111076957905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 123\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~~MEMORY RESTORED~~\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOSS REVERTED ∞ -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m trainer\u001b[38;5;241m.\u001b[39mrun(tr_loader, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:889\u001b[0m, in \u001b[0;36mEngine.run\u001b[1;34m(self, data, max_epochs, epoch_length)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupt_resume_enabled:\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run()\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:932\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_as_gen()\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator)\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:990\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 990\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(e)\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:644\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:956\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_engine()\n\u001b[1;32m--> 956\u001b[0m epoch_time_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[0;32m    958\u001b[0m \u001b[38;5;66;03m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m epoch_time_taken\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\ignite\\engine\\engine.py:1077\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_STARTED)\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[1;32m-> 1077\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_COMPLETED)\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n",
      "Cell \u001b[1;32mIn[183], line 39\u001b[0m, in \u001b[0;36mtrain_update\u001b[1;34m(engine, batch)\u001b[0m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     38\u001b[0m y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 39\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred[:,\u001b[38;5;241m0\u001b[39m], y)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# assert y_pred.dtype is torch.float16 ##################\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# assert loss.dtype is torch.float32 ##################\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\intern2\\deep sea fish\\snow_model.py:31\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(x))\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc5(x))\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import Timer\n",
    "import torch\n",
    "import ignite\n",
    "from ignite.handlers import CosineAnnealingScheduler\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler    # defunct\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from torch.cuda import amp       # apex.amp is deprecated. it cannot be regenerated.\n",
    "\n",
    "from snow_model import Net\n",
    "# from snow_model_module import Net\n",
    "# from snow_model_2 import Net\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used : {device}\")\n",
    "\n",
    "# from cupy_dataset import OptionDataSet\n",
    "timer = Timer(average=True)\n",
    "model = Net().cuda()\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "set_amp = True\n",
    "scaler = amp.GradScaler(enabled=set_amp)\n",
    "\n",
    "\n",
    "\n",
    "### model training method for ~~tammy~~ ENGINE\n",
    "def train_update(engine, batch):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16): ########### automatic mixed precision\n",
    "        model.train()\n",
    "        # optimizer.zero_grad()\n",
    "        optimizer.zero_grad(set_to_none=True) # set_to_none=True here can modestly improve performance \n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred[:,0], y)\n",
    "        # assert y_pred.dtype is torch.float16 ##################\n",
    "        # assert loss.dtype is torch.float32 ##################\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True) # set_to_none=True here can modestly improve performance \n",
    "    # optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_update)\n",
    "interval_count  = 10     ############# how many times loss message shows up every epoch!!!!!!!!!!\n",
    "log_interval = len(tr_loader.dataset)//batch_size//interval_count\n",
    "loss_history = []\n",
    "\n",
    "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(tr_loader))\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)  ## restarts the learning rate of cosine; each iteration is 1 batch long\n",
    "# trainer add events, attach metrics & others\n",
    "\n",
    "timer.attach(trainer,                       ##\n",
    "             start=Events.EPOCH_STARTED,\n",
    "             resume=Events.ITERATION_STARTED,\n",
    "             pause=Events.ITERATION_COMPLETED,\n",
    "             step=Events.ITERATION_COMPLETED)    \n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(tr_loader) + 1\n",
    "    if iter % log_interval == 0:\n",
    "        print('loss', engine.state.output, 'average time', timer.value())\n",
    "        loss_history.append( engine.state.output)\n",
    "\n",
    "\n",
    "loss_m = ignite.metrics.Loss(loss_fn)\n",
    "best_loss = 99.0\n",
    "epoctr = 0\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_test_loss(engine):\n",
    "    global best_loss\n",
    "    global epoctr\n",
    "    epoctr += 1\n",
    "    size = len(te_loader.dataset)\n",
    "    num_batches = len(te_loader)\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in te_loader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred[:,0],y).item() #item converts tensor to number\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f}\")\n",
    "    if best_loss > test_loss:\n",
    "        print(f\"new best model saved, UWU\\n{best_loss}->{test_loss}\")\n",
    "        best_loss = test_loss\n",
    "        torch.save({'epoch' : epoctr,\n",
    "                    'model' : model.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                    'loss history' : loss_history}, \"saved_model/model_weights.pth\")\n",
    "    \n",
    "    print(f\"Epoch: {epoctr} ↑\\n\")\n",
    "\n",
    "\n",
    "### model resumer (in case fucky wucky happens and model crashes)\n",
    "resume = True\n",
    "respth = \"saved_model/model_weights.pth\"\n",
    "\n",
    "if resume:\n",
    "    global loss_history\n",
    "    global epoctr\n",
    "    global best_loss\n",
    "    checkpoint = torch.load(respth)\n",
    "    epoctr = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    loss_history = checkpoint['loss history']\n",
    "    best_loss = loss_history[-1]\n",
    "    print(\"~~MEMORY RESTORED~~\")\n",
    "    print(f\"LOSS REVERTED ∞ -> {best_loss}\")\n",
    "\n",
    "\n",
    "        \n",
    "trainer.run(tr_loader, max_epochs=1000-epoctr)\n",
    "# trainer.run(dataset, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another run with slightly differerrent hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda:0\n",
      "loss 0.156869575381279 average time 0.002502479838991309\n",
      "loss 0.1806713044643402 average time 0.0025622565772155186\n",
      "loss 0.2916005849838257 average time 0.0025634957028975404\n",
      "loss 0.09543561935424805 average time 0.0025659717687517305\n",
      "loss 0.12556441128253937 average time 0.0025594804731888885\n",
      "Avg loss: 0.084691\n",
      "new best model saved, UWU\n",
      "99.0->0.08469122294791225\n",
      "Epoch: 1 ↑\n",
      "\n",
      "loss 0.13309644162654877 average time 0.0023799477584879954\n",
      "loss 0.1477399617433548 average time 0.002375305471493794\n",
      "loss 0.07398892939090729 average time 0.002408740800348244\n",
      "loss 0.0956515297293663 average time 0.002409395886550485\n",
      "loss 0.08479617536067963 average time 0.002418505933596362\n",
      "Avg loss: 0.061126\n",
      "new best model saved, UWU\n",
      "0.08469122294791225->0.06112602931790564\n",
      "Epoch: 2 ↑\n",
      "\n",
      "loss 0.15596365928649902 average time 0.002449271552367498\n",
      "loss 0.062362562865018845 average time 0.00244403787738258\n",
      "loss 0.07263195514678955 average time 0.0024372767376857935\n",
      "loss 0.06435955315828323 average time 0.002438566245412979\n",
      "loss 0.05929167568683624 average time 0.0024347532133601156\n",
      "Avg loss: 0.050558\n",
      "new best model saved, UWU\n",
      "0.06112602931790564->0.05055837782298108\n",
      "Epoch: 3 ↑\n",
      "\n",
      "loss 0.060112304985523224 average time 0.002515391881562296\n",
      "loss 0.057603996247053146 average time 0.0024448915800559263\n",
      "loss 0.05689222365617752 average time 0.002428170340508409\n",
      "loss 0.03859857842326164 average time 0.002413445143053553\n",
      "loss 0.048695772886276245 average time 0.0024132386456314045\n",
      "Avg loss: 0.038533\n",
      "new best model saved, UWU\n",
      "0.05055837782298108->0.038532630868014596\n",
      "Epoch: 4 ↑\n",
      "\n",
      "loss 0.03044595941901207 average time 0.0023803113199079686\n",
      "loss 0.15924367308616638 average time 0.0023713235857138497\n",
      "loss 0.05734189972281456 average time 0.002393125556008172\n",
      "loss 0.033540382981300354 average time 0.0023900900983977805\n",
      "loss 0.037037018686532974 average time 0.0023865416233842624\n",
      "Avg loss: 0.026866\n",
      "new best model saved, UWU\n",
      "0.038532630868014596->0.02686585143099309\n",
      "Epoch: 5 ↑\n",
      "\n",
      "loss 0.11608938127756119 average time 0.002391717624343523\n",
      "loss 0.03412555158138275 average time 0.0023875714603546565\n",
      "loss 0.0279826819896698 average time 0.0023684152459832273\n",
      "loss 0.027570446953177452 average time 0.0023698043852247988\n",
      "loss 0.026977157220244408 average time 0.002369952352785229\n",
      "Avg loss: 0.022217\n",
      "new best model saved, UWU\n",
      "0.02686585143099309->0.02221707815510117\n",
      "Epoch: 6 ↑\n",
      "\n",
      "loss 0.2771106958389282 average time 0.0023700007647791516\n",
      "loss 0.04459449276328087 average time 0.002400278046247154\n",
      "loss 0.024848375469446182 average time 0.0024155978261127214\n",
      "loss 0.02314704842865467 average time 0.0024147606672109885\n",
      "loss 0.022835807874798775 average time 0.002416752452272732\n",
      "Avg loss: 0.011500\n",
      "new best model saved, UWU\n",
      "0.02221707815510117->0.011500271526873395\n",
      "Epoch: 7 ↑\n",
      "\n",
      "loss 0.0725393071770668 average time 0.0023879555459103995\n",
      "loss 0.023994453251361847 average time 0.0023703322249022246\n",
      "loss 0.018462272360920906 average time 0.0023878963210988407\n",
      "loss 0.017485767602920532 average time 0.0023945654038220582\n",
      "loss 0.0183608066290617 average time 0.002404486486652227\n",
      "Avg loss: 0.013772\n",
      "Epoch: 8 ↑\n",
      "\n",
      "loss 0.130971759557724 average time 0.0024363396528018716\n",
      "loss 0.03518086299300194 average time 0.0024361034431438088\n",
      "loss 0.015983035787940025 average time 0.0024191676452533332\n",
      "loss 0.012443693354725838 average time 0.0024226713145498982\n",
      "loss 0.012208296917378902 average time 0.0024216080237101103\n",
      "Avg loss: 0.008668\n",
      "new best model saved, UWU\n",
      "0.011500271526873395->0.008668202681469919\n",
      "Epoch: 9 ↑\n",
      "\n",
      "loss 0.20413486659526825 average time 0.0025079262635689394\n",
      "loss 0.03711940720677376 average time 0.002450440729187411\n",
      "loss 0.03593317046761513 average time 0.0024232324058138534\n",
      "loss 0.018052440136671066 average time 0.0024221942437554424\n",
      "loss 0.021900352090597153 average time 0.002412645663129318\n",
      "Avg loss: 0.010313\n",
      "Epoch: 10 ↑\n",
      "\n",
      "loss 0.14430701732635498 average time 0.002424806188552831\n",
      "loss 0.020893020555377007 average time 0.0024118917629910716\n",
      "loss 0.013080560602247715 average time 0.002425706798370076\n",
      "loss 0.01097424328327179 average time 0.0024162709733811533\n",
      "loss 0.006001108326017857 average time 0.0024162871068539506\n",
      "Avg loss: 0.005882\n",
      "new best model saved, UWU\n",
      "0.008668202681469919->0.0058820259640365614\n",
      "Epoch: 11 ↑\n",
      "\n",
      "loss 0.032626379281282425 average time 0.0024518230122420734\n",
      "loss 0.0364798977971077 average time 0.0024264404126114513\n",
      "loss 0.009267015382647514 average time 0.0024227485190935153\n",
      "loss 0.010678058490157127 average time 0.00242189722418422\n",
      "loss 0.01253790594637394 average time 0.0024144068437744765\n",
      "Avg loss: 0.010372\n",
      "Epoch: 12 ↑\n",
      "\n",
      "loss 0.023581137880682945 average time 0.0025100472903993227\n",
      "loss 0.023345189169049263 average time 0.0024510293732552093\n",
      "loss 0.007809095084667206 average time 0.0024307812576110596\n",
      "loss 0.012376747094094753 average time 0.002415030778342448\n",
      "loss 0.006214785389602184 average time 0.002424795387998566\n",
      "Avg loss: 0.005224\n",
      "new best model saved, UWU\n",
      "0.0058820259640365614->0.005223597463921179\n",
      "Epoch: 13 ↑\n",
      "\n",
      "loss 0.04930031672120094 average time 0.002377083772102025\n",
      "loss 0.023435048758983612 average time 0.002374467840113835\n",
      "loss 0.007780145853757858 average time 0.002387153219264331\n",
      "loss 0.010302761569619179 average time 0.0024034226017992317\n",
      "loss 0.010749532841145992 average time 0.002393325252033868\n",
      "Avg loss: 0.005169\n",
      "new best model saved, UWU\n",
      "0.005223597463921179->0.0051687700610489355\n",
      "Epoch: 14 ↑\n",
      "\n",
      "loss 0.04111561179161072 average time 0.0024214631810129603\n",
      "loss 0.01568659022450447 average time 0.0023932021754533386\n",
      "loss 0.007355774287134409 average time 0.002381749129460462\n",
      "loss 0.009654369205236435 average time 0.0023802805191062934\n",
      "loss 0.004607386887073517 average time 0.0023694342869301777\n",
      "Avg loss: 0.003971\n",
      "new best model saved, UWU\n",
      "0.0051687700610489355->0.0039710342388425135\n",
      "Epoch: 15 ↑\n",
      "\n",
      "loss 0.03068428672850132 average time 0.00242207400201515\n",
      "loss 0.09825458377599716 average time 0.002427152615809732\n",
      "loss 0.01687747985124588 average time 0.0024019089536564853\n",
      "loss 0.011023360304534435 average time 0.00239477240635703\n",
      "loss 0.011189715936779976 average time 0.0023954514416712103\n",
      "Avg loss: 0.013020\n",
      "Epoch: 16 ↑\n",
      "\n",
      "loss 0.012178124859929085 average time 0.0023886151919683403\n",
      "loss 0.01481271255761385 average time 0.0023600727793609494\n",
      "loss 0.009168073534965515 average time 0.0023765087214367756\n",
      "loss 0.009318073280155659 average time 0.0023869757425370483\n",
      "loss 0.004200063645839691 average time 0.002384141874792356\n",
      "Avg loss: 0.003952\n",
      "new best model saved, UWU\n",
      "0.0039710342388425135->0.003951863998000477\n",
      "Epoch: 17 ↑\n",
      "\n",
      "loss 0.016231030225753784 average time 0.0024044404392734317\n",
      "loss 0.012704071588814259 average time 0.002403849543616448\n",
      "loss 0.006190703250467777 average time 0.0023895377423498306\n",
      "loss 0.011576013639569283 average time 0.002385701360053241\n",
      "loss 0.009385509416460991 average time 0.0023796141779599054\n",
      "Avg loss: 0.009390\n",
      "Epoch: 18 ↑\n",
      "\n",
      "loss 0.019545206800103188 average time 0.0023864358331997563\n",
      "loss 0.01424375455826521 average time 0.0023892094951563424\n",
      "loss 0.013018869794905186 average time 0.002374845616277497\n",
      "loss 0.01037226989865303 average time 0.0023822745742474007\n",
      "loss 0.008853922598063946 average time 0.0023742578317263244\n",
      "Avg loss: 0.008435\n",
      "Epoch: 19 ↑\n",
      "\n",
      "loss 0.05785819888114929 average time 0.0024049280962428615\n",
      "loss 0.015403106808662415 average time 0.002423780089366466\n",
      "loss 0.009627078659832478 average time 0.0024023076256295133\n",
      "loss 0.009570017457008362 average time 0.0024067049153171337\n",
      "loss 0.007905233651399612 average time 0.002402434357200784\n",
      "Avg loss: 0.006698\n",
      "Epoch: 20 ↑\n",
      "\n",
      "loss 0.01287031639367342 average time 0.0023737928654755653\n",
      "loss 0.015488197095692158 average time 0.0023824255396159293\n",
      "loss 0.011006898246705532 average time 0.0024071230520633273\n",
      "loss 0.010977317579090595 average time 0.0024017646503296203\n",
      "loss 0.009029124863445759 average time 0.002398704973206158\n",
      "Avg loss: 0.006760\n",
      "Epoch: 21 ↑\n",
      "\n",
      "loss 0.01628345623612404 average time 0.0023865947135386666\n",
      "loss 0.020322538912296295 average time 0.002460840995065844\n",
      "loss 0.005842375569045544 average time 0.002452494246746137\n",
      "loss 0.008422380313277245 average time 0.002463322878227508\n",
      "loss 0.0031356019899249077 average time 0.002466692009682017\n",
      "Avg loss: 0.003186\n",
      "new best model saved, UWU\n",
      "0.003951863998000477->0.0031855662348026148\n",
      "Epoch: 22 ↑\n",
      "\n",
      "loss 0.14536291360855103 average time 0.0024161247119331743\n",
      "loss 0.011342505924403667 average time 0.0024462226126288657\n",
      "loss 0.007096316199749708 average time 0.002429159108230374\n",
      "loss 0.00848325528204441 average time 0.0024560622861545967\n",
      "loss 0.005298408214002848 average time 0.002472032627365068\n",
      "Avg loss: 0.006420\n",
      "Epoch: 23 ↑\n",
      "\n",
      "loss 0.014061333611607552 average time 0.002504570700955941\n",
      "loss 0.015298699028789997 average time 0.0024767398113800733\n",
      "loss 0.005507550667971373 average time 0.0024571272910781567\n",
      "loss 0.00628084409981966 average time 0.0024305634938413547\n",
      "loss 0.004578265827149153 average time 0.0024185223776206676\n",
      "Avg loss: 0.004749\n",
      "Epoch: 24 ↑\n",
      "\n",
      "loss 0.006017344072461128 average time 0.00255642481190484\n",
      "loss 0.025691866874694824 average time 0.0025728983905216702\n",
      "loss 0.0050136493518948555 average time 0.0025065658268061185\n",
      "loss 0.0029631333891302347 average time 0.0024648344292767734\n",
      "loss 0.0032397201284766197 average time 0.002448045442126814\n",
      "Avg loss: 0.002381\n",
      "new best model saved, UWU\n",
      "0.0031855662348026148->0.002380504105272017\n",
      "Epoch: 25 ↑\n",
      "\n",
      "loss 0.021210527047514915 average time 0.002475863599589746\n",
      "loss 0.00974799320101738 average time 0.0024479257465019194\n",
      "loss 0.0034058778546750546 average time 0.002446534433964041\n",
      "loss 0.006819508969783783 average time 0.002450705757403067\n",
      "loss 0.004356388933956623 average time 0.002436071212914562\n",
      "Avg loss: 0.002565\n",
      "Epoch: 26 ↑\n",
      "\n",
      "loss 0.006083689164370298 average time 0.0032148478232086205\n",
      "loss 0.0036083196755498648 average time 0.002793195522963356\n",
      "loss 0.002716749906539917 average time 0.0026636772498161887\n",
      "loss 0.0021685443352907896 average time 0.0028282342594252345\n",
      "loss 0.0027797853108495474 average time 0.0029810360282562\n",
      "Avg loss: 0.001771\n",
      "new best model saved, UWU\n",
      "0.002380504105272017->0.0017706089011055189\n",
      "Epoch: 27 ↑\n",
      "\n",
      "loss 0.010775445029139519 average time 0.0024467461223143952\n"
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import Timer\n",
    "import torch\n",
    "import ignite\n",
    "from ignite.handlers import CosineAnnealingScheduler\n",
    "\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "# from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler    # defunct\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from torch.cuda import amp       # apex.amp is deprecated. it cannot be regenerated.\n",
    "\n",
    "from snow_model import Net\n",
    "# from snow_model_module import Net\n",
    "# from snow_model_2 import Net\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used : {device}\")\n",
    "\n",
    "# from cupy_dataset import OptionDataSet\n",
    "timer = Timer(average=True)\n",
    "model = Net().cuda()\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "set_amp = True\n",
    "scaler = amp.GradScaler(enabled=set_amp)\n",
    "\n",
    "\n",
    "\n",
    "### model training method for ~~tammy~~ ENGINE\n",
    "def train_update(engine, batch):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16): ########### automatic mixed precision\n",
    "        model.train()\n",
    "        # optimizer.zero_grad()\n",
    "        optimizer.zero_grad(set_to_none=True) # set_to_none=True here can modestly improve performance \n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred[:,0], y)\n",
    "        # assert y_pred.dtype is torch.float16 ##################\n",
    "        # assert loss.dtype is torch.float32 ##################\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True) # set_to_none=True here can modestly improve performance \n",
    "    # optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_update)\n",
    "interval_count  = 5     ############# how many times loss message shows up every epoch!!!!!!!!!!\n",
    "log_interval = len(tr_loader.dataset)//batch_size//interval_count\n",
    "loss_history = []\n",
    "\n",
    "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-5, 1e-6, len(tr_loader))\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)  ## restarts the learning rate of cosine; each iteration is 1 batch long\n",
    "# trainer add events, attach metrics & others\n",
    "\n",
    "timer.attach(trainer,                       ##\n",
    "             start=Events.EPOCH_STARTED,\n",
    "             resume=Events.ITERATION_STARTED,\n",
    "             pause=Events.ITERATION_COMPLETED,\n",
    "             step=Events.ITERATION_COMPLETED)    \n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(tr_loader) + 1\n",
    "    if iter % log_interval == 0:\n",
    "        print('loss', engine.state.output, 'average time', timer.value())\n",
    "        loss_history.append( engine.state.output)\n",
    "\n",
    "\n",
    "loss_m = ignite.metrics.Loss(loss_fn)\n",
    "best_loss = 99.0\n",
    "epoctr = 0\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_test_loss(engine):\n",
    "    global best_loss\n",
    "    global epoctr\n",
    "    epoctr += 1\n",
    "    size = len(te_loader.dataset)\n",
    "    num_batches = len(te_loader)\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in te_loader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred[:,0],y).item() #item converts tensor to number\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f}\")\n",
    "    if best_loss > test_loss:\n",
    "        print(f\"new best model saved, UWU\\n{best_loss}->{test_loss}\")\n",
    "        best_loss = test_loss\n",
    "        torch.save({'epoch' : epoctr,\n",
    "                    'model' : model.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                    'loss history' : loss_history}, \"saved_model/model_weights2.pth\")\n",
    "    \n",
    "    print(f\"Epoch: {epoctr} ↑\\n\")\n",
    "\n",
    "\n",
    "### model resumer (in case fucky wucky happens and model crashes)\n",
    "resume = False\n",
    "respth = \"saved_model/model_weights2.pth\"\n",
    "\n",
    "if resume:\n",
    "    global loss_history\n",
    "    global epoctr\n",
    "    global best_loss\n",
    "    checkpoint = torch.load(respth)\n",
    "    epoctr = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    loss_history = checkpoint['loss history']\n",
    "    best_loss = loss_history[-1]\n",
    "    print(\"~~MEMORY RESTORED~~\")\n",
    "    print(f\"LOSS REVERTED ∞ -> {best_loss}\")\n",
    "\n",
    "\n",
    "        \n",
    "trainer.run(tr_loader, max_epochs=1000-epoctr)\n",
    "# trainer.run(dataset, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a plot of the loss. (current picuture is of like only 5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~MEMORY RESTORED~~\n",
      "LOSS REVERTED ∞ -> 0.013436572626233101\n",
      "~~MEMORY RESTORED~~\n",
      "LOSS REVERTED ∞ -> 0.015050362795591354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c659e6d910>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "checkpoint = torch.load(\"saved_model/model_weights.pth\")\n",
    "epoctr = checkpoint['epoch']\n",
    "lh = checkpoint['loss history']\n",
    "best_loss = lh[-1]\n",
    "print(\"~~MEMORY RESTORED~~\")\n",
    "print(f\"LOSS REVERTED ∞ -> {best_loss}\")\n",
    "\n",
    "checkpoint2 = torch.load(\"saved_model/model_weights2.pth\")\n",
    "epoctr2 = checkpoint2['epoch']\n",
    "lh2 = checkpoint2['loss history']\n",
    "best_loss2 = lh2[-1]\n",
    "print(\"~~MEMORY RESTORED~~\")\n",
    "print(f\"LOSS REVERTED ∞ -> {best_loss2}\")\n",
    "\n",
    "\n",
    "plt.plot(lh, label = \"1e^-4\")\n",
    "plt.plot(lh2, label = \"1e^-5\")\n",
    "# plt.plot(lh3, label = \"1e^-4 but long\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a test of the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Error: \n",
      "Avg loss: 0.000058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "size = len(val_loader.dataset)\n",
    "num_batches = len(val_loader)\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"saved_model/model_weights_around650.pth\")\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "model.eval()\n",
    "test_loss=0\n",
    "with torch.no_grad():\n",
    "    for X,y in val_loader:\n",
    "        # X,y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        test_loss += loss_fn(pred[:,0],y).item() #item converts tensor to number\n",
    "        # print(y)\n",
    "        # print(pred[:,0])       # has some interesting results showing how argmax\n",
    "        # print(test_loss)\n",
    "test_loss /= num_batches\n",
    "print(f\"Val Error: \\nAvg loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eh basically the test loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nah, id lose\n",
    "stuff is still not finished yet :/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = pics\\galaxy-angel-mint-blancmanche.gif>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on the bright side, that means theres more to come! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=pics/reisen-udongein.gif width = 500>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
