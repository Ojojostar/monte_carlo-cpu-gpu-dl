{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model time!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src = pics/OIP.jpg width = 400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "cupy.cuda.set_allocator(None)       # no clue\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "\n",
    "import numba\n",
    "from numba import cuda\n",
    "\n",
    "import os, os.path\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset\n",
    "Each monte carlo simulation run is equivalent to one data point being made, so to generate a large dataset, we have to run monte carlo simulations lots of times, and batches can hypotheitcally make doing this faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here,the mc model from mc_snow, cuda version was imported and cleaned up a bit.\n",
    "note that due to the existence of batches, some of the varaibles now need a bit of extra finagling to access properly. (s_0, Ki, Ko, mu, sigma, pot,r, d_normals, snowball_path_holder). Overall design is very close to original, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit               # defualt GPU\n",
    "def monte_carlo_andtheholygrail_gpu(d_s, s_0, Ki, Ko, mu, sigma, pot,r,\n",
    "                                    d_normals, snowball_path_holder, MONTHS,\n",
    "                                    N_STEPS, N_PATHS, N_BATCH):\n",
    "    \n",
    "\n",
    "    # for shared memory (non)optimization\n",
    "    # shared = cuda.shared.array(shape=0, dtype=numba.float32)\n",
    "    # # load to shared memory\n",
    "    # path_offset = cuda.blockIdx.x * cuda.blockDim.x\n",
    "\n",
    "    # ii - overall thread index\n",
    "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
    "\n",
    "    for n in range(ii, N_PATHS * N_BATCH, stride):\n",
    "        # newly added vars for N_BATCH calculations\n",
    "        batch_id = n // N_PATHS\n",
    "        path_id = n % N_PATHS       # equivalent to n in old code \n",
    "\n",
    "        snowball_path_holder[n][0] = s_0[batch_id]\n",
    "        earlyexit = False\n",
    "        ki = False\n",
    "        mald = False\n",
    "        for t in range(N_STEPS):\n",
    "            # pre shared memory b_motion    \n",
    "            #                                                   \n",
    "            b_motion = d_normals[path_id + batch_id * N_PATHS +  t * N_PATHS * N_BATCH]\n",
    "\n",
    "            # post shared memory b_motion\n",
    "            # shared[cuda.threadIdx.x] = d_normals[path_offset + cuda.threadIdx.x + t * N_PATHS]\n",
    "\n",
    "            dt = 1/N_STEPS\n",
    "            # pre shared memory b_motion\n",
    "            ds = snowball_path_holder[n][t] * mu[batch_id] * dt + snowball_path_holder[n][t] \\\n",
    "                                                * sigma[batch_id] * b_motion * math.sqrt(dt) \n",
    "            # post shared memory b_motion\n",
    "            # ds = snowball_path_holder[n][t] * mu[batch_id] * dt + snowball_path_holder[n][t] * sigma[batch_id] * shared[cuda.threadIdx.x] * math.sqrt(dt) \n",
    "                    # no adjusting list sizes in cuda :(\n",
    "            # snowball_path.append(snowball_path[t]+ds)\n",
    "            snowball_path_holder[n][t+1] = snowball_path_holder[n][t] + ds\n",
    "            \n",
    "\n",
    "            # ki = snowball_path[t] + ds\n",
    "            if snowball_path_holder[n][t+1] <= Ki[batch_id]:\n",
    "                ki = True\n",
    "\n",
    "            if not mald:\n",
    "                for month in (0,1,2,3,4,5,6,7,8,9,10,11):                # need to do this instead because contains (in) and range are disabled\n",
    "                    if t+1 == MONTHS[month]:     #startday no longer used to fake a start date in code\n",
    "                        # price = t+1+startday\n",
    "                        if snowball_path_holder[n][t+1] >= Ko[batch_id]:\n",
    "                            price =  pot[batch_id] * t/365     # should turn t into int\n",
    "                            # return snowball_path, price\n",
    "                            d_s[n] =  price * math.exp(-r[batch_id] * t/N_STEPS)   # accounting for r\n",
    "                            snowball_path_holder[n][-1] = d_s[n]            \n",
    "                            earlyexit = True\n",
    "                            mald = True\n",
    "                            # print(\"blo got fucked\\n\")\n",
    "                            break\n",
    "            else: # if mald\n",
    "                break\n",
    "        \n",
    "        if not earlyexit:       # to prevent early exit getting out of bdds error\n",
    "            # did not get knocked up or down\n",
    "            price = pot[batch_id]\n",
    "            # t  =T \n",
    "                        # CAN'T USE T CUZ CUDA IS FUCKING SHIT so use -1 instead\n",
    "                        # or not ig T works now :sob:\n",
    "            if ki and snowball_path_holder[n][N_STEPS] <= s_0[batch_id]:          # blo got knocked down and never recovered\n",
    "                price = snowball_path_holder[n][N_STEPS] - s_0[batch_id]\n",
    "            elif ki and snowball_path_holder[n][N_STEPS] <= Ko[batch_id]:          # blo got knocked down for a bit but finished above Ki\n",
    "                price =0\n",
    "            d_s[n] = price * math.exp(-r[batch_id])\n",
    "            snowball_path_holder[n][-1] = d_s[n]    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for some reason, increasing the number of paths and/or the number of batches greatly slows down my monte's carlo's computational speed. I have no definitive proof that this is the case, but I strongly belive it to be because threads are becoming unsynched as the code runs on, making both greater paths and greater batches than my current settings have much slower run times than their current values. Not that my current code isn't slower than it should be, either. <br>\n",
    " Max len controls the number of data points, path controls how accurate each data point is. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once finished running, data is saved into a directory.\n",
    "<br>\n",
    "If you think running a large number like 1 mil mcs takes way too long, throw these first three cells into a python file (datasetgen.py) and let it run in its own terminal while going forward in the notebook with a smaller set for test purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for demonstration purposes, only a small amount of data is generated here. If large amounts of data are to be generated, **PLEASE** go to datasetgen.py instead. there is much more stuff there that isnt incorporated here since i dont like scrolling htat much that would make generating data a bit easier (generates data in chunks so that its safer, ability to run mutliple process of program at the same time thru currnumm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding files starting from 101\n",
      "Num batches: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.882806777954102]\n",
      "[2.3875019550323486]\n",
      "[5.110105991363525]\n",
      "[2.0633299350738525]\n",
      "[2.978761672973633]\n",
      "[2.733570098876953]\n",
      "[2.55251407623291]\n",
      "[2.243455648422241]\n",
      "[2.852839469909668]\n",
      "[3.885261058807373]\n",
      "time 2.7585599422454834 v 3.885261 avg time 5.517119884490967e-06\n",
      "[]\n",
      "tensor([])\n",
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "#               make sure max_len is large enough or else divide by zero error occurs (at least 100 batches must be run)\n",
    "limiter = True\n",
    "# max_len = 1000000                 # hundo thousand data points, final speed is ~40 min for 1000 data.\n",
    "max_len = 10                 # hundo thousand data points, final speed is ~40 min for 1000 data.\n",
    "number_path = 500000\n",
    "batch = 1\n",
    "threads = 256\n",
    "seed  =1999 \n",
    "num = 0\n",
    "max_length = max_len\n",
    "N_PATHS = number_path\n",
    "N_STEPS = 365\n",
    "N_BATCH  =batch\n",
    "\n",
    "max_length = max_length // N_BATCH\n",
    "percenter  =100\n",
    "percent = max_length // percenter\n",
    "\n",
    "#           uncomment if u want less batches, the percent will just be wrong\n",
    "if percent == 0:\n",
    "    percent = 1\n",
    "\n",
    "# we will not be calculating a starting date since the difference is negligible and I aint rigging up\n",
    "# a system to check if a certain day is a weekend or not\n",
    "MONTHS = cupy.asnumpy([0, 31,59,90,120,151,181,212,243, 273,304,334])\n",
    "        # SHOULD THIS BE NP ARRAY INSTEAD????\n",
    "snowball_path_holder =  np.zeros(N_BATCH*N_PATHS, dtype=(np.float32,N_STEPS+1))# extra 1 is no longer for storing payoff\n",
    "# self.snowball_path_holder = cupy.array(self.snowball_path_holder)\n",
    "# self.T  = np.float(365.0)         # nah id lose. \n",
    "output = cupy.zeros(N_BATCH*N_PATHS, dtype = cupy.float32)\n",
    "num_blocks  =(N_PATHS * N_BATCH -1) // threads +1\n",
    "num_threads = threads\n",
    "\n",
    "# torch.cuda._sanitizer.enable_cuda_sanitizer()\n",
    "# Xs =  np.zeros(max_length, dtype=(np.float32,7))#\n",
    "# Ys =  np.zeros(max_length, dtype=(np.float32))#          storing final data\n",
    "\n",
    "Xss = []\n",
    "Yss = []\n",
    "# Xss = np.zeros((max_length, 7))\n",
    "# Yss = np.zeros((max_length, 1))\n",
    "currnum = len(os.listdir('snow_data_tensor_train'))//2+1\n",
    "print(\"Adding files starting from\", currnum)\n",
    "\n",
    "print(\"Num batches:\", N_BATCH)\n",
    "\n",
    "# making sure self.snowball_path_holder is zeroed to avoid bug\n",
    "# self.snowball_path_holder.fill(0)\n",
    "s = time.time()\n",
    "\n",
    "for i in range(1,max_length+1):\n",
    "        randoms = cupy.random.normal(0,1, N_BATCH * N_PATHS * N_STEPS, dtype= cupy.float32)\n",
    "\n",
    "        Xpre = cupy.random.rand(N_BATCH, 7, dtype = cupy.float32)\n",
    "        #                        s_0,  Ki, Ko,  mu, sigma, pot, r\n",
    "        Xpre = Xpre * cupy.array([4,  -2,  1,  .01,  .15,  10, .01], dtype=cupy.float32)\n",
    "        X = Xpre +    cupy.array([8,   0,  0,  .02, .275,  15, .02], dtype=cupy.float32)\n",
    "        # Ki and Ko will be set down here instead of the previous line to make them relative to s_0.\n",
    "        X[:, 1] = X[:,0] -1         # overriding Ki and Ko \n",
    "        X[:, 2] = X[:,0] -.2        \n",
    "        X[:, 1] += Xpre[:,1]        # adding back the offset in Xpre after it gets overrided\n",
    "        X[:, 2] += Xpre[:,2] \n",
    "\n",
    "        snowball_path_holder.fill(0)\n",
    "                                        # d_s, s_0, Ki, Ko, mu, sigma, pot,r,\n",
    "                                        # d_normals, snowball_path_holder, MONTHS,\n",
    "                                        # N_STEPS, N_PATHS, N_BATCH):\n",
    "        monte_carlo_andtheholygrail_gpu[(num_blocks,), (num_threads,)](\n",
    "                                        output, X[:, 0], X[:, 1], X[:, 2], X[:, 3], \n",
    "                                        X[:, 4], X[:, 5], X[:, 6],\n",
    "                                        randoms, snowball_path_holder, MONTHS,\n",
    "                                        N_STEPS, N_PATHS, N_BATCH)\n",
    "        # o = output.reshape(N_BATCH, N_PATHS)\n",
    "        # Y  =o.mean(axis =1)         # getting the average of each batch\n",
    "        Y = output.mean()\n",
    "        # Y = output\n",
    "        X = X.mean(axis=0)\n",
    "        Xss.append(X.tolist())\n",
    "        Yss.append(Y.tolist())\n",
    "        print(Yss)\n",
    "        # Xss.append(X)\n",
    "        # Yss.append(Y)\n",
    "\n",
    "        # have following turned off. go to datasetgen.py for a better view.\n",
    "        # if(i%percent==0):\n",
    "        #     if limiter:\n",
    "        #         if currnum > percenter:\n",
    "        #             print(\"premature exit, burunyu~\")\n",
    "        #             break\n",
    "        #     e = time.time()\n",
    "        #     print(i/(percent), \"percent of the way there! Time is now:\", (e-s)/60/60, \"hours\")\n",
    "        #     # print(i/(percent*10), \"percent of the way there! Time is now:\", e-s, \"secs\")\n",
    "        #     print(\"now saving tsnowX_{}.pt\".format(currnum) )\n",
    "        #     tensorX = np.array(Xss)\n",
    "        #     tensorY = np.array(Yss)\n",
    "        #     tensorX = torch.Tensor(tensorX)\n",
    "        #     tensorY = torch.Tensor(tensorY)\n",
    "        #     torch.save(tensorX, f\"snow_data_tensor_train/tsnowX_{currnum}.pt\")\n",
    "        #     torch.save(tensorY, f\"snow_data_tensor_train/tsnowY_{currnum}.pt\")\n",
    "        #     Xss.clear()\n",
    "        #     Yss.clear()\n",
    "        #     currnum += 1\n",
    "\n",
    "        num+=1          #actually useless\n",
    "        # print((from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))) \n",
    "        \n",
    "        Xss.clear()\n",
    "        Yss.clear()\n",
    "\n",
    "v = output.mean()\n",
    "cuda.synchronize()\n",
    "e = time.time()\n",
    "print('time', e-s, 'v', v, 'avg time', (e-s)/500000)\n",
    "\n",
    "# Xs = np.array(Xss)\n",
    "# Ys = np.array(Yss)\n",
    "Xss = np.array(Xss)\n",
    "Yss = np.array(Yss)\n",
    "# print(Xss)\n",
    "print(Yss)\n",
    "\n",
    "tensorX = torch.Tensor(Xss)\n",
    "tensorY = torch.Tensor(Yss)\n",
    "print(tensorX)\n",
    "print(tensorY)\n",
    "\n",
    "## i have following turned off but feel free to do stuff with it\n",
    "\n",
    "# torch.save(tensorX, \"snow_data_tensor_train/tsnowX.pt\")\n",
    "# torch.save(tensorY, \"snow_data_tensor_train/tsnowY.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small tangent on datasetgen\n",
    "So... after a lot of finagling with datasetgen.py, I have realized that it is most certianly the case that the code runs slower due to needing to run 500000 paths before it can synch back up again, causing great slowdown, espeicaly with larger batch/path numbers. However, this does not prevent us from running multiple different processes of the same code, filling up the gpu with power of more processes instead. Though there is some slowdown caused by having more processes, there is an almost 2 times improvement over runnign one process of the code when using 3 processes. Thats a lot! Of course, if you can get more processes runnign without gpu's memory going to 100% and locking up the program for extended periods of time, this should allow for as much speedup as you would get by using all of your gpu???? I think???? Ive been runnign datasetgen in its own power shells to prevent restarts of vs code from restarting it (pylance keeps crashing >:( ), but this can obviously be turned all into a single program that creates as many processes as you want! Therefore there is also datasetgen_multi.py which can do exactly that! its a slight modification of datasetgen.py but is a big quality of life update, greatly lowering the amount of power shells i need to open every time i run the program (u cant close em while they are runnign since they are locked in and dont like to stop at keyboard interrupts)\n",
    "\n",
    "<img src = pics\\big_scary_hacker_man.jpg width = 700> <br>\n",
    "\n",
    "**Fig 1. Me being big scary hacker man ðŸ˜±** (6 process was a mistake and now all of them are frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to regularly scheduled programming (loading dataset made by datasetgen.py)\n",
    "Loading the data to confirm its existence! turning it into gpu monsters to prepare for throwing it into model to train! can prolly be done be done before saving, thouhg :/ <br>\n",
    "oh well, it takes like no time to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.7849,  9.7630, 10.9411,  ...,  0.4168, 18.8266,  0.0270],\n",
      "        [11.2135,  8.3553, 11.3719,  ...,  0.2971, 17.4514,  0.0261],\n",
      "        [ 8.1927,  6.3303,  8.4115,  ...,  0.4142, 20.6274,  0.0260],\n",
      "        ...,\n",
      "        [ 8.5286,  5.6047,  8.3977,  ...,  0.4016, 19.3757,  0.0229],\n",
      "        [10.5141,  7.9688, 10.7822,  ...,  0.3437, 22.5634,  0.0237],\n",
      "        [11.9229,  9.6655, 12.6072,  ...,  0.2978, 19.4558,  0.0234]])\n",
      "tensor([2.2016, 2.8075, 2.7976,  ..., 2.7598, 3.3981, 3.1653])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms, utils, datasets\n",
    "import os\n",
    "\n",
    "path = \"snow_data_tensor\"\n",
    "\n",
    "def npy_loader(path):\n",
    "    sample = torch.load(path)\n",
    "    return sample\n",
    "\n",
    "# dataset = datasets.DatasetFolder(\n",
    "#     root=path,\n",
    "#     loader=npy_loader,\n",
    "#     extensions=['.pt']\n",
    "#     )\n",
    "# tensor_x = torch.Tensor(Xss)\n",
    "# tensor_y = torch.Tensor(Yss)\n",
    "\n",
    "finnum = len(os.listdir(\"snow_data_tensor_train\"))//2+1          # equivalent to currnum of a previous box\n",
    "\n",
    "tensor_x_L = torch.load(\"snow_data_tensor_train/tsnowX_1.pt\")\n",
    "tensor_y_L = torch.load(\"snow_data_tensor_train/tsnowY_1.pt\")\n",
    "\n",
    "\n",
    "for tensor_num  in range(2, finnum):\n",
    "    tensor_x_R = torch.load(f\"snow_data_tensor_train/tsnowX_{tensor_num}.pt\")\n",
    "    tensor_y_R = torch.load(f\"snow_data_tensor_train/tsnowY_{tensor_num}.pt\")   \n",
    "\n",
    "    #   cat left side with right side (kiara and kamma???)\n",
    "    tensor_x_L = torch.cat((tensor_x_L, tensor_x_R), 0)\n",
    "    tensor_y_L = torch.cat((tensor_y_L, tensor_y_R), 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(tensor_x_L)\n",
    "print(tensor_y_L)\n",
    "\n",
    "# print(tdataset)\n",
    "\n",
    "# for i in tdataset:\n",
    "# #     # print(i, \"\\n\")\n",
    "#     print(i)     # printing the Ys\n",
    "# print(len(dataset)/2)\n",
    "# print(dataset)\n",
    "# inputs = tensor_x.float().to(device)\n",
    "# # values = tensor_y.float().to(device)\n",
    "# print(f\"Input device is : cuda:{tensor_x.get_device()}\")\n",
    "# print(f\"Target value device is : cuda:{tensor_y.get_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also split the data into more train, validation, and test before saving once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_data = tensor_x_L.size()[0]\n",
    "num_train = int(num_data*.6)\n",
    "num_val = int(num_data*.8)\n",
    "\n",
    "train_x, val_x, test_x = torch.tensor_split(tensor_x_L, (num_train, num_val))\n",
    "train_y, val_y, test_y = torch.tensor_split(tensor_y_L, (num_train, num_val))\n",
    "\n",
    "# print(train.size())\n",
    "\n",
    "\n",
    "dir_p = \"snow_data_processed\"\n",
    "\n",
    "torch.save(train_x, f\"{dir_p}\\\\train_x.pt\")\n",
    "torch.save(val_x, f\"{dir_p}\\\\val_x.pt\")\n",
    "torch.save(test_x, f\"{dir_p}\\\\test_x.pt\")\n",
    "\n",
    "torch.save(train_y, f\"{dir_p}\\\\train_y.pt\")\n",
    "torch.save(val_y, f\"{dir_p}\\\\val_y.pt\")\n",
    "torch.save(test_y, f\"{dir_p}\\\\test_y.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to load the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_p = \"snow_data_processed\"\n",
    "\n",
    "\n",
    "train_x = torch.load(f\"{dir_p}\\\\train_x.pt\")\n",
    "train_y = torch.load(f\"{dir_p}\\\\train_y.pt\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = train_x.float().to(device)\n",
    "values = train_y.float().to(device)\n",
    "\n",
    "\n",
    "tdataset = TensorDataset(inputs,values) # create your datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some reason, the guy who made the nvidia notebook decided it was a good idea to make the dataset generate itself while training, making the training process significantly slower than if the data was already prepared already. This works, but is not that good. I would not reccomend, 2/5 stars. <br>\n",
    "edit: he does it the better way in their next notbook :/<br>\n",
    "still leaving this in since its interesting to look at tho. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SnowballDataSet(object):\n",
    "\n",
    "    def __init__(self, max_len = 10, number_path = 1000, batch = 2, threads = 512, seed  =1999 ):\n",
    "        self.num = 0\n",
    "        self.max_length = max_len\n",
    "        self.N_PATHS = number_path\n",
    "        self.N_STEPS = 365\n",
    "        self.N_BATCH  =batch\n",
    "        # we will not be calculating a starting date since the difference is negligible and I aint rigging up\n",
    "        # a system to check if a certain day is a weekend or not\n",
    "        self.MONTHS = cupy.asnumpy([0, 31,59,90,120,151,181,212,243, 273,304,334])\n",
    "                # SHOULD THIS BE NP ARRAY INSTEAD????\n",
    "        self.snowball_path_holder =  np.zeros(self.N_BATCH*self.N_PATHS, dtype=(np.float32,self.N_STEPS+1))# extra 1 is no longer for storing payoff\n",
    "        # self.snowball_path_holder = cupy.array(self.snowball_path_holder)\n",
    "        # self.T  = np.float(365.0)         # nah id lose. \n",
    "        self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype = cupy.float32)\n",
    "        self.num_blocks  =(self.N_PATHS * self.N_BATCH -1) // threads +1\n",
    "        self.num_threads = threads\n",
    "\n",
    "        #  temp_months, snowball_path_holder both added now\n",
    "        cupy.random.seed(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.num = 0\n",
    "        return self\n",
    "\n",
    "    #   next basically takes the place of the cell running the mc. As such need to generate\n",
    "     # (d_s, s_0, Ki, Ko, mu, sigma,  pot,r, d_normals, snowball_path_holder, MONTHS, N_STEPS, N_PATHS, N_BATCH\n",
    "     # note that all but s_0, Ki, Ko, mu, sigma,  pot,r, d_normals have been generated in init due to their nonrandom nature\n",
    "    def __next__(self):\n",
    "        if self.num > self.max_length: \n",
    "            raise StopIteration      # nvidia notebook uses raise StopIteration here but p sure its deprecated???\n",
    "                                      # is used because return returns an extra None\n",
    "        # generating the variables\n",
    "        # d_normals\n",
    "        randoms = cupy.random.normal(0,1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype= cupy.float32)\n",
    "\n",
    "        Xpre = cupy.random.rand(self.N_BATCH, 7, dtype = cupy.float32)\n",
    "        #                        s_0,  Ki, Ko,  mu, sigma, pot, r\n",
    "        Xpre = Xpre * cupy.array([4,  -2,  1,  .01,  .15,  10, .01], dtype=cupy.float32)\n",
    "        X = Xpre +    cupy.array([8,   0,  0,  .02, .275,  15, .02], dtype=cupy.float32)\n",
    "        \n",
    "        # Ki and Ko will be set down here instead of the previous line to make them relative to s_0.\n",
    "        X[:, 1] = X[:,0] -1         # overriding Ki and Ko \n",
    "        X[:, 2] = X[:,0] -.2        \n",
    "        # print(X)\n",
    "        X[:, 1] += Xpre[:,1]        # adding back the offset in Xpre after it gets overrided\n",
    "        X[:, 2] += Xpre[:,2] \n",
    "\n",
    "        # making sure self.snowball_path_holder is zeroed to avoid bug\n",
    "        self.snowball_path_holder.fill(0)\n",
    "\n",
    "                                        # d_s, s_0, Ki, Ko, mu, sigma, pot,r,\n",
    "                                        # d_normals, snowball_path_holder, MONTHS,\n",
    "                                        # N_STEPS, N_PATHS, N_BATCH):\n",
    "        monte_carlo_andtheholygrail_gpu[(self.num_blocks,), (self.num_threads,)](\n",
    "                                        self.output, X[:, 0], X[:, 1], X[:, 2], X[:, 3], \n",
    "                                        X[:, 4], X[:, 5], X[:, 6],\n",
    "                                        randoms, self.snowball_path_holder, self.MONTHS,\n",
    "                                        self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
    "        \n",
    "        o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
    "        Y  =o.mean(axis =1)         # getting the average of each batch\n",
    "        self.num+=1\n",
    "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a small test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SnowballDataSet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ds = SnowballDataSet(10, number_path=500000, batch=16, seed=15)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ds \u001b[38;5;241m=\u001b[39m SnowballDataSet(\u001b[38;5;241m10\u001b[39m, number_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500000\u001b[39m, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ds:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# print(i, \"\\n\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;241m0\u001b[39m],i[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SnowballDataSet' is not defined"
     ]
    }
   ],
   "source": [
    "# ds = SnowballDataSet(10, number_path=500000, batch=16, seed=15)\n",
    "ds = SnowballDataSet(10, number_path=500000, batch=1, seed=15)\n",
    "for i in ds:\n",
    "    # print(i, \"\\n\")\n",
    "    print(i[0],i[1])     # printing the Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Erm pretty default model. Just making it have functionality. normalizing it accoriding to the average value of all of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting snow_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile snow_model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, hidden)\n",
    "        self.fc4 = nn.Linear(hidden, hidden)\n",
    "        self.fc5 = nn.Linear(hidden, hidden)\n",
    "        self.fc6 = nn.Linear(hidden, hidden)\n",
    "        self.fc7 = nn.Linear(hidden, 1)\n",
    "        self.register_buffer('norm',\n",
    "                             torch.tensor([10.0,\n",
    "                                           8.5,\n",
    "                                           10.4,\n",
    "                                           0.025,\n",
    "                                           0.35,\n",
    "                                           0.20,\n",
    "                                           0.025]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # normalize the parameter to range [0-1] \n",
    "        x = x / self.norm\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.elu(self.fc3(x))\n",
    "        x = F.elu(self.fc4(x))\n",
    "        x = F.elu(self.fc5(x))\n",
    "        x = F.elu(self.fc6(x))\n",
    "        return self.fc7(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "as the amount of data is relatively small, a smallish batch size will be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda:0\n",
      "loss 0.8380540013313293 average time 0.002695952901325654\n",
      "loss 0.22348438203334808 average time 0.0024961021010240073\n",
      "loss 0.70233154296875 average time 0.002430481867166236\n",
      "loss 0.3667253851890564 average time 0.002397824225219665\n",
      "loss 0.5217218399047852 average time 0.0023931232399656437\n",
      "loss 0.19837328791618347 average time 0.0023958347500301898\n",
      "loss 0.22395247220993042 average time 0.0024080648715192055\n",
      "loss 0.2895205616950989 average time 0.0024176712874686927\n",
      "loss 0.23500633239746094 average time 0.0024345940334153256\n",
      "loss 0.9038403034210205 average time 0.0025410603992058896\n",
      "loss 0.16764132678508759 average time 0.00249455269947066\n",
      "loss 0.32438400387763977 average time 0.0025345728329848496\n",
      "loss 0.21896147727966309 average time 0.002564192599413218\n",
      "loss 0.33764052391052246 average time 0.00257362581982743\n",
      "loss 0.14903752505779266 average time 0.0025618552666952987\n",
      "loss 0.16698071360588074 average time 0.0025267774428024757\n",
      "loss 0.16793979704380035 average time 0.002499892350009759\n",
      "loss 0.13894908130168915 average time 0.0024770402445250914\n",
      "loss 0.692692220211029 average time 0.002255572599824518\n",
      "loss 0.11987894773483276 average time 0.002294772200228181\n",
      "loss 0.30869585275650024 average time 0.0023488397995824924\n",
      "loss 0.1433497816324234 average time 0.0023344040996889815\n",
      "loss 0.2647590637207031 average time 0.002322831879754085\n",
      "loss 0.12076828628778458 average time 0.0023136703666338386\n",
      "loss 0.12537889182567596 average time 0.0023139440430012263\n",
      "loss 0.10583451390266418 average time 0.0023153982125732\n",
      "loss 0.0971689224243164 average time 0.002310462755700832\n",
      "loss 0.21633736789226532 average time 0.0023076954985153863\n",
      "loss 0.08665625751018524 average time 0.002326319299376337\n",
      "loss 0.10502384603023529 average time 0.0023032574993364203\n",
      "loss 0.11650076508522034 average time 0.002304232124282862\n",
      "loss 0.1284283995628357 average time 0.0023101066194241866\n",
      "loss 0.07207604497671127 average time 0.0023128643164236564\n",
      "loss 0.06938300281763077 average time 0.0023260118282716056\n",
      "loss 0.08670764416456223 average time 0.0023203341748958338\n",
      "loss 0.07160001248121262 average time 0.002317827511048462\n",
      "loss 0.1088816449046135 average time 0.0022939207996823825\n",
      "loss 0.07596031576395035 average time 0.0022876321499352343\n",
      "loss 0.2830974757671356 average time 0.002286395333164061\n",
      "loss 0.12739600241184235 average time 0.0022789752745011357\n",
      "loss 0.0727701336145401 average time 0.0022851563596748745\n",
      "loss 0.05457139387726784 average time 0.002298959133089132\n",
      "loss 0.0404188334941864 average time 0.0022939989282583285\n",
      "loss 0.047843609005212784 average time 0.0023019881748696206\n",
      "loss 0.05664584040641785 average time 0.0023069460443350385\n",
      "loss 0.07625497132539749 average time 0.002380902999895625\n",
      "loss 0.10561678558588028 average time 0.002368238349619787\n",
      "loss 0.23168817162513733 average time 0.002331687333159304\n",
      "loss 0.05890057608485222 average time 0.0023141631999897072\n",
      "loss 0.060947202146053314 average time 0.0023106801799847743\n",
      "loss 0.041604235768318176 average time 0.002304943566598619\n",
      "loss 0.03138734772801399 average time 0.0022969378141985673\n",
      "loss 0.039208583533763885 average time 0.0022915457374838295\n",
      "loss 0.04998450726270676 average time 0.002291760111098281\n",
      "loss 0.0659877210855484 average time 0.002349011999729555\n",
      "loss 0.0649176612496376 average time 0.002311695200216491\n",
      "loss 0.07433432340621948 average time 0.00231037850024101\n",
      "loss 0.07864093780517578 average time 0.002344098850095179\n",
      "loss 0.04706089571118355 average time 0.0023515554001671264\n",
      "loss 0.03270415589213371 average time 0.0023830755669623613\n",
      "loss 0.028189361095428467 average time 0.002446802414463101\n",
      "loss 0.03395593911409378 average time 0.002474054175145284\n",
      "loss 0.044843655079603195 average time 0.0024933276667401917\n",
      "loss 0.09767728298902512 average time 0.002697619901271537\n",
      "loss 0.057030871510505676 average time 0.0026782761502545326\n",
      "loss 0.041513070464134216 average time 0.0026319981663837097\n",
      "loss 0.09891166538000107 average time 0.0027184195998561336\n",
      "loss 0.039184216409921646 average time 0.002729539159941487\n",
      "loss 0.029028143733739853 average time 0.0027562057167136423\n",
      "loss 0.02353167161345482 average time 0.002754360971490054\n",
      "loss 0.029693298041820526 average time 0.0027482590876024915\n",
      "loss 0.038118455559015274 average time 0.0027753205667185184\n",
      "loss 0.05602976679801941 average time 0.002623111399647314\n",
      "loss 0.043853748589754105 average time 0.002720416850002948\n",
      "loss 0.05141529068350792 average time 0.0028198996658902616\n",
      "loss 0.03793999180197716 average time 0.0028805746742582416\n",
      "loss 0.04297633096575737 average time 0.002882251719245687\n",
      "loss 0.026109836995601654 average time 0.00288168629941841\n",
      "loss 0.02049816958606243 average time 0.002875670856621582\n",
      "loss 0.026011230424046516 average time 0.0028951610620933936\n",
      "loss 0.03358987346291542 average time 0.002916892110803423\n",
      "loss 0.13773980736732483 average time 0.002739668700727634\n",
      "loss 0.03918953984975815 average time 0.0027866144004510714\n",
      "loss 0.042832620441913605 average time 0.0028060168672236615\n",
      "loss 0.03169398754835129 average time 0.0028411334255361\n",
      "loss 0.03686406463384628 average time 0.0028341652802191673\n",
      "loss 0.02077857404947281 average time 0.0028857139666215517\n",
      "loss 0.01807323843240738 average time 0.002910173142885989\n",
      "loss 0.02384144440293312 average time 0.002891772487470007\n",
      "loss 0.029656443744897842 average time 0.0028835917556037506\n",
      "loss 0.040082897990942 average time 0.003228494100214448\n",
      "loss 0.07420940697193146 average time 0.0031766952493344435\n",
      "loss 0.04937759414315224 average time 0.003073188432647536\n",
      "loss 0.028801510110497475 average time 0.0030787591245752992\n",
      "loss 0.03131489083170891 average time 0.003031498459505383\n",
      "loss 0.01975160278379917 average time 0.0029955551996439073\n",
      "loss 0.014550588093698025 average time 0.002967290028239534\n",
      "loss 0.021112628281116486 average time 0.0029524932497442934\n",
      "loss 0.025145454332232475 average time 0.002947730066429358\n",
      "loss 0.10059889405965805 average time 0.002984783299791161\n",
      "loss 0.029295384883880615 average time 0.002980256049981108\n",
      "loss 0.062233202159404755 average time 0.0029624543665365006\n",
      "loss 0.02282131463289261 average time 0.0029495377498387826\n",
      "loss 0.02201080322265625 average time 0.0029594558398704975\n",
      "loss 0.03119237907230854 average time 0.0029793748497128642\n",
      "loss 0.01254960522055626 average time 0.0029767460855987986\n",
      "loss 0.019648974761366844 average time 0.0029679385874551373\n",
      "loss 0.02224387787282467 average time 0.002947083088866849\n",
      "loss 0.0332268550992012 average time 0.0030989433997892775\n",
      "loss 0.024353332817554474 average time 0.0029529273999214637\n",
      "loss 0.022675158455967903 average time 0.002901104733192672\n",
      "loss 0.018464216962456703 average time 0.0029346600498101907\n",
      "loss 0.01763206720352173 average time 0.002957843359815888\n",
      "loss 0.0572526790201664 average time 0.0029398775834124534\n",
      "loss 0.013383077457547188 average time 0.002937515400084002\n",
      "loss 0.01683942601084709 average time 0.0029456012752561945\n",
      "loss 0.019231829792261124 average time 0.002939627355650171\n",
      "loss 0.038818228989839554 average time 0.002817938300257083\n",
      "loss 0.03475365787744522 average time 0.0027799003497639206\n",
      "loss 0.0144825903698802 average time 0.0028484367004518087\n",
      "loss 0.020652243867516518 average time 0.002849696325371042\n",
      "loss 0.01600007526576519 average time 0.0028420182402129285\n",
      "loss 0.055315084755420685 average time 0.0028665889000403693\n",
      "loss 0.0124299181625247 average time 0.0028822644001421783\n",
      "loss 0.014970943331718445 average time 0.0028954814751341474\n",
      "loss 0.017590321600437164 average time 0.0029102688223550405\n",
      "loss 0.027959667146205902 average time 0.002838682000467088\n",
      "loss 0.022772597149014473 average time 0.002874696200015023\n",
      "loss 0.027748603373765945 average time 0.002869537866480338\n",
      "loss 0.015462794341146946 average time 0.002938343774920213\n",
      "loss 0.013122905045747757 average time 0.002933517920144368\n",
      "loss 0.03640449792146683 average time 0.0029243243001207397\n",
      "loss 0.010411873459815979 average time 0.0029147325429200593\n",
      "loss 0.012986664660274982 average time 0.002889589787562727\n",
      "loss 0.015516524203121662 average time 0.002897993566733526\n",
      "loss 0.0229189433157444 average time 0.002845281100133434\n",
      "loss 0.02036214806139469 average time 0.0028295679497823585\n",
      "loss 0.018509911373257637 average time 0.0028627564997295847\n",
      "loss 0.01357501931488514 average time 0.0028331714747910153\n",
      "loss 0.012743014842271805 average time 0.0028084337398642674\n",
      "loss 0.04086913913488388 average time 0.0027960663500028507\n",
      "loss 0.00927822943776846 average time 0.0027647071144892834\n",
      "loss 0.01210847869515419 average time 0.0027337272127260803\n",
      "loss 0.014542071148753166 average time 0.0027478818781091832\n",
      "loss 0.01817976124584675 average time 0.0029044484005426056\n",
      "loss 0.03126704692840576 average time 0.002869820350460941\n",
      "loss 0.01055053435266018 average time 0.0029018108002492227\n",
      "loss 0.01263249572366476 average time 0.0028768636502936717\n",
      "loss 0.0119480537250638 average time 0.0028839716804563067\n",
      "loss 0.017811493948101997 average time 0.0028626212170444584\n",
      "loss 0.008559501729905605 average time 0.002850081128741814\n",
      "loss 0.011254379525780678 average time 0.002854894737502036\n",
      "loss 0.013053755275905132 average time 0.0028621303667249677\n",
      "loss 0.01786698028445244 average time 0.0028105299002490936\n",
      "loss 0.025899924337863922 average time 0.002845917050843127\n",
      "loss 0.010594676248729229 average time 0.0028374384338773475\n",
      "loss 0.012446784414350986 average time 0.0028438873255654472\n",
      "loss 0.012716026045382023 average time 0.002829452460212633\n",
      "loss 0.0206572525203228 average time 0.002855754550148655\n",
      "loss 0.008644177578389645 average time 0.002861134142939201\n",
      "loss 0.011435744352638721 average time 0.0028728361124740333\n",
      "loss 0.013224252499639988 average time 0.002864816266694106\n",
      "loss 0.013129460625350475 average time 0.002633539700123947\n",
      "loss 0.02248529903590679 average time 0.002692119800398359\n",
      "loss 0.00983321201056242 average time 0.0027536588332538183\n",
      "loss 0.011937166564166546 average time 0.0027555293997575062\n",
      "loss 0.024105390533804893 average time 0.002789525039924774\n",
      "loss 0.017171436920762062 average time 0.0028343482831842265\n",
      "loss 0.006211957894265652 average time 0.002865534071469613\n",
      "loss 0.008509418927133083 average time 0.0028998092250403715\n",
      "loss 0.009174445644021034 average time 0.002886901066638529\n",
      "loss 0.021324405446648598 average time 0.002893947900563944\n",
      "loss 0.018555941060185432 average time 0.0029334537503891626\n",
      "loss 0.03231542557477951 average time 0.002927202433774558\n",
      "loss 0.020413273945450783 average time 0.0029658513006579597\n",
      "loss 0.021169405430555344 average time 0.0029572504605166615\n",
      "loss 0.018754320219159126 average time 0.0029404741003139253\n",
      "loss 0.0057219248265028 average time 0.002941188685853766\n",
      "loss 0.007871070876717567 average time 0.0029314995750028172\n",
      "loss 0.008049387484788895 average time 0.0029155417221287884\n",
      "loss 0.012239323928952217 average time 0.0029041483010514638\n",
      "loss 0.012165922671556473 average time 0.0029393249503918925\n",
      "loss 0.011746921576559544 average time 0.002982464766886551\n",
      "loss 0.015837984159588814 average time 0.0029899830251379172\n",
      "loss 0.020064838230609894 average time 0.002973239620018285\n",
      "loss 0.020065302029252052 average time 0.002960989766686301\n",
      "loss 0.005712222773581743 average time 0.0029458009285070665\n",
      "loss 0.00858129933476448 average time 0.0029295545374625363\n",
      "loss 0.008881252259016037 average time 0.002936331900030685\n",
      "loss 0.017073551192879677 average time 0.0031708029998117125\n",
      "loss 0.011989850550889969 average time 0.0030482467499678024\n",
      "loss 0.01089585293084383 average time 0.0030328088332898916\n",
      "loss 0.010890460573136806 average time 0.0030172785998292964\n",
      "loss 0.014223067089915276 average time 0.0029844991596532054\n",
      "loss 0.02014629729092121 average time 0.002967165966537626\n",
      "loss 0.005676486529409885 average time 0.002970008656956322\n",
      "loss 0.007746636867523193 average time 0.0029756703748134894\n",
      "loss 0.008346915245056152 average time 0.002982190477532438\n",
      "loss 0.01347956620156765 average time 0.0027489255999098533\n",
      "loss 0.01774534210562706 average time 0.0027484790001763033\n",
      "loss 0.02140759862959385 average time 0.002774089733119278\n",
      "loss 0.010342104360461235 average time 0.0027037552998663158\n",
      "loss 0.019547047093510628 average time 0.002674395959521644\n",
      "loss 0.01818091794848442 average time 0.002652874916335956\n",
      "loss 0.004995010327547789 average time 0.0026336615708590087\n",
      "loss 0.006937657482922077 average time 0.0026226954619269238\n",
      "loss 0.005774638615548611 average time 0.0026055853662417376\n",
      "loss 0.013347592204809189 average time 0.0024749397004488857\n",
      "loss 0.016937054693698883 average time 0.0024839146508893466\n",
      "loss 0.007920398376882076 average time 0.0024942654338665307\n",
      "loss 0.02866322360932827 average time 0.002486844625644153\n",
      "loss 0.013612236827611923 average time 0.0024818480205140076\n",
      "loss 0.01763985864818096 average time 0.0024906399005170292\n",
      "loss 0.005233656149357557 average time 0.0024919941861465175\n",
      "loss 0.007153593935072422 average time 0.002504202325253573\n",
      "loss 0.006243519484996796 average time 0.0025013811667320422\n",
      "loss 0.0092954495921731 average time 0.0025197368011577056\n",
      "loss 0.011468415148556232 average time 0.00251133180045872\n",
      "loss 0.007700584828853607 average time 0.002518109766941052\n",
      "loss 0.021594271063804626 average time 0.0024849410749593517\n",
      "loss 0.014143097214400768 average time 0.0024775014800718052\n",
      "loss 0.01568623259663582 average time 0.0024701702334568834\n",
      "loss 0.0034796905238181353 average time 0.002472281828763828\n",
      "loss 0.006185497622936964 average time 0.0024729687500948784\n",
      "loss 0.004313033074140549 average time 0.002468501011168377\n",
      "loss 0.014370929449796677 average time 0.002442419200553559\n",
      "loss 0.013998760841786861 average time 0.0024618354506092148\n",
      "loss 0.005079342983663082 average time 0.002475856100344875\n",
      "loss 0.009091651067137718 average time 0.0024714937499084044\n",
      "loss 0.013685259968042374 average time 0.0024856034797965548\n",
      "loss 0.01838577352464199 average time 0.002494993349915603\n",
      "loss 0.0054339636117219925 average time 0.0025015033570484125\n",
      "loss 0.006224723998457193 average time 0.0025022542873412022\n",
      "loss 0.004247025586664677 average time 0.002504310344358803\n",
      "loss 0.010079827159643173 average time 0.0024165497010108085\n",
      "loss 0.006671588867902756 average time 0.0024216296008671635\n",
      "loss 0.024555787444114685 average time 0.002427193867372504\n",
      "loss 0.02084249258041382 average time 0.0024382112502789825\n",
      "loss 0.007552335504442453 average time 0.0024382977799745275\n",
      "loss 0.017327912151813507 average time 0.0024419918999968406\n",
      "loss 0.005201233550906181 average time 0.0024466024856748324\n",
      "loss 0.006152139976620674 average time 0.0024570712249333154\n",
      "loss 0.004034360405057669 average time 0.002467085444381357\n",
      "loss 0.022840436547994614 average time 0.002392656599229667\n",
      "loss 0.017150703817605972 average time 0.002411357349192258\n",
      "loss 0.015502233058214188 average time 0.0024407476664055137\n",
      "loss 0.017237501218914986 average time 0.002425221774887177\n",
      "loss 0.008906139060854912 average time 0.0024326982796890662\n",
      "loss 0.02095705457031727 average time 0.00242979348320902\n",
      "loss 0.004927837289869785 average time 0.002439264042751997\n",
      "loss 0.006849642843008041 average time 0.0024451101997838122\n",
      "loss 0.005082304589450359 average time 0.0024437959553777343\n",
      "loss 0.13183775544166565 average time 0.0023827794000389987\n",
      "loss 0.028689496219158173 average time 0.0024095595001708717\n",
      "loss 0.015756146982312202 average time 0.0023937029336035874\n",
      "loss 0.03801102191209793 average time 0.002409855425285059\n",
      "loss 0.013489683158695698 average time 0.0024507487002993003\n",
      "loss 0.0123692462220788 average time 0.002469866250583436\n",
      "loss 0.00424171844497323 average time 0.002471797386275804\n",
      "loss 0.006441719364374876 average time 0.0024748743880845725\n",
      "loss 0.004495344590395689 average time 0.002475436711703272\n",
      "loss 0.010476440191268921 average time 0.002382460500812158\n",
      "loss 0.016662053763866425 average time 0.002397274150804151\n",
      "loss 0.006852277554571629 average time 0.002417306667174368\n",
      "loss 0.00879235565662384 average time 0.0024128148006129775\n",
      "loss 0.012281481176614761 average time 0.0024256455203751103\n",
      "loss 0.014355849474668503 average time 0.0024572052002282967\n",
      "loss 0.004859274718910456 average time 0.0024567958286830357\n",
      "loss 0.006441078614443541 average time 0.002451626375092019\n",
      "loss 0.00425156531855464 average time 0.002446083511057724\n",
      "loss 0.014997299760580063 average time 0.0025482323989854195\n",
      "loss 0.011460322886705399 average time 0.0025086209998407866\n",
      "loss 0.013097194954752922 average time 0.0024860939664261726\n",
      "loss 0.027350110933184624 average time 0.0025040916497673605\n",
      "loss 0.011311405338346958 average time 0.002502674699923955\n",
      "loss 0.02041831612586975 average time 0.002514831716611904\n",
      "loss 0.005084909498691559 average time 0.0025285741857036816\n",
      "loss 0.007063671015202999 average time 0.002521232849830994\n",
      "loss 0.005977759137749672 average time 0.002524641499751144\n",
      "loss 0.622529149055481 average time 0.002451863001449965\n",
      "loss 0.18088367581367493 average time 0.0024634943009878042\n",
      "loss 0.19171880185604095 average time 0.002451471267462087\n",
      "loss 0.0874086543917656 average time 0.002478635700419545\n",
      "loss 0.06567893177270889 average time 0.0024794515203451738\n",
      "loss 0.021479496732354164 average time 0.0024736409001343417\n",
      "loss 0.009438646025955677 average time 0.002474920728692918\n",
      "loss 0.006713474169373512 average time 0.0024747960373642853\n",
      "loss 0.004171696957200766 average time 0.0024734376665372917\n",
      "loss 0.05714542418718338 average time 0.0024245935999206267\n",
      "loss 0.013837771490216255 average time 0.0024439317492651754\n",
      "loss 0.007669711951166391 average time 0.0024712852660256127\n",
      "loss 0.01443448755890131 average time 0.0024825509247311858\n",
      "loss 0.008522383868694305 average time 0.0024875904998742046\n",
      "loss 0.013149730861186981 average time 0.0024980912833707405\n",
      "loss 0.0035577474627643824 average time 0.0024946394428677324\n",
      "loss 0.006906591355800629 average time 0.0024954811248753686\n",
      "loss 0.004369429312646389 average time 0.002496589433214265\n",
      "loss 0.01970372349023819 average time 0.002465968199656345\n",
      "loss 0.021905923262238503 average time 0.0024303564995352646\n",
      "loss 0.01071801595389843 average time 0.002443168899548861\n",
      "loss 0.009934790432453156 average time 0.00245333592472889\n",
      "loss 0.013014490716159344 average time 0.0024616655398975126\n",
      "loss 0.011837117373943329 average time 0.0024618201500852593\n",
      "loss 0.003984603099524975 average time 0.002456531500061309\n",
      "loss 0.006755094975233078 average time 0.0024616757625190075\n",
      "loss 0.004517690744251013 average time 0.0024534117998878677\n",
      "loss 0.009088930673897266 average time 0.0024403907998348587\n",
      "loss 0.009017422795295715 average time 0.0024424716001667547\n",
      "loss 0.007614745758473873 average time 0.002482202499775061\n",
      "loss 0.010868508368730545 average time 0.0024762763497565175\n",
      "loss 0.00805822666734457 average time 0.0025127798800705934\n",
      "loss 0.012094542384147644 average time 0.0025186077668816627\n",
      "loss 0.00449953181669116 average time 0.0025081556858827492\n",
      "loss 0.006087571382522583 average time 0.002518197150020569\n",
      "loss 0.004010121803730726 average time 0.002522032177800106\n",
      "loss 0.008106150664389133 average time 0.0024797889002948067\n",
      "loss 0.005972769111394882 average time 0.0024992303002800325\n",
      "loss 0.012070098891854286 average time 0.0024959446672582998\n",
      "loss 0.02181157097220421 average time 0.0024934970756294206\n",
      "loss 0.009523659944534302 average time 0.002505541100690607\n",
      "loss 0.01527418103069067 average time 0.0025043610173161143\n",
      "loss 0.004029593896120787 average time 0.002517528729255511\n",
      "loss 0.006241586059331894 average time 0.002511307700595353\n",
      "loss 0.003925900906324387 average time 0.0025036196559796936\n",
      "loss 0.008108740672469139 average time 0.0024515309006674217\n",
      "loss 0.0068419864401221275 average time 0.0024866423003550154\n",
      "loss 0.01882501319050789 average time 0.0024905356666422448\n",
      "loss 0.007346786092966795 average time 0.0024974622502195415\n",
      "loss 0.008530046790838242 average time 0.00250316482008202\n",
      "loss 0.011479349806904793 average time 0.0024805683501278207\n",
      "loss 0.004885237198323011 average time 0.0024889687286673247\n",
      "loss 0.005663271527737379 average time 0.0024776876374453423\n",
      "loss 0.003965680953115225 average time 0.0024699841664987615\n",
      "loss 0.008450272493064404 average time 0.0023644146994920446\n",
      "loss 0.009593360126018524 average time 0.002412363149865996\n",
      "loss 0.009653639979660511 average time 0.0024583873996355883\n",
      "loss 0.014638583175837994 average time 0.0024850693750049684\n",
      "loss 0.008034230209887028 average time 0.002486219880194403\n",
      "loss 0.015384399332106113 average time 0.0024940186000215665\n",
      "loss 0.004150182008743286 average time 0.0024971668001463904\n",
      "loss 0.006096988916397095 average time 0.002488283025166311\n",
      "loss 0.0037700736429542303 average time 0.0024718771778732642\n",
      "loss 0.009168417192995548 average time 0.002361806400353089\n",
      "loss 0.0059675476513803005 average time 0.002454564750776626\n",
      "loss 0.014405704103410244 average time 0.0024340491673210634\n",
      "loss 0.022635070607066154 average time 0.002445775750093162\n",
      "loss 0.005467266775667667 average time 0.002463439440028742\n",
      "loss 0.014471347443759441 average time 0.0024600007500654706\n",
      "loss 0.003621931653469801 average time 0.002476132900321058\n",
      "loss 0.00597759336233139 average time 0.0024675659378335696\n",
      "loss 0.0035269700456410646 average time 0.002473526400100026\n",
      "loss 0.007016099523752928 average time 0.0024191795990918764\n",
      "loss 0.008159774355590343 average time 0.002480744249507552\n",
      "loss 0.013856898993253708 average time 0.0024555132664584867\n",
      "loss 0.010763592086732388 average time 0.002452787700254703\n",
      "loss 0.006968081928789616 average time 0.0024576240603695625\n",
      "loss 0.013005747459828854 average time 0.0024645100336735293\n",
      "loss 0.00480338791385293 average time 0.0024502244146840115\n",
      "loss 0.005879548843950033 average time 0.00245475487549993\n",
      "loss 0.0036891389172524214 average time 0.0024494461004570542\n",
      "loss 0.01088207121938467 average time 0.002470321499800775\n",
      "loss 0.013981747440993786 average time 0.0024826285995659418\n",
      "loss 0.008736792951822281 average time 0.0024959857326078538\n",
      "loss 0.013212897814810276 average time 0.0024850317244854525\n",
      "loss 0.008193803951144218 average time 0.002475884119735565\n",
      "loss 0.02088197134435177 average time 0.0024874011332770654\n",
      "loss 0.004472138825803995 average time 0.002483670842916971\n",
      "loss 0.0063015460036695 average time 0.002472418450153782\n",
      "loss 0.0040397197008132935 average time 0.002460587489037102\n",
      "loss 0.007575789466500282 average time 0.002395980200381018\n",
      "loss 0.007573404815047979 average time 0.002426449850492645\n",
      "loss 0.027566833421587944 average time 0.00242991646679972\n",
      "loss 0.008260824717581272 average time 0.0024291845002153423\n",
      "loss 0.005248049274086952 average time 0.0024404750402318314\n",
      "loss 0.02449321746826172 average time 0.0024490835835749747\n",
      "loss 0.003213078249245882 average time 0.002448930485922444\n",
      "loss 0.006024353206157684 average time 0.0024542596125611453\n",
      "loss 0.003399715293198824 average time 0.002451465944326224\n",
      "loss 0.39319801330566406 average time 0.002457352398894727\n",
      "loss 0.11706112325191498 average time 0.0024714991995424497\n",
      "loss 0.0927390530705452 average time 0.0024753947663703002\n",
      "loss 0.04977821186184883 average time 0.0024693416496738793\n",
      "loss 0.018741263076663017 average time 0.002456968859839253\n",
      "loss 0.013978371396660805 average time 0.0024709112495960046\n",
      "loss 0.005948057863861322 average time 0.002468107371025586\n",
      "loss 0.00823023822158575 average time 0.0024749389371208964\n",
      "loss 0.0045168474316596985 average time 0.002460995166355537\n",
      "loss 0.2051354944705963 average time 0.0025296295990119688\n",
      "loss 0.035322096198797226 average time 0.0025223609996319284\n",
      "loss 0.02493305131793022 average time 0.002536279666509169\n",
      "loss 0.014542853459715843 average time 0.0025324410498142242\n",
      "loss 0.008545407094061375 average time 0.0025169109199778177\n",
      "loss 0.021371249109506607 average time 0.002515484299869665\n",
      "loss 0.004452705383300781 average time 0.0024983096569360765\n",
      "loss 0.006096109747886658 average time 0.0024968837998312664\n",
      "loss 0.005777140147984028 average time 0.0024864364220476194\n",
      "loss 0.010397947393357754 average time 0.0024688720997655764\n",
      "loss 0.007853344082832336 average time 0.0024728734494419767\n",
      "loss 0.008887010626494884 average time 0.0024840503331118573\n",
      "loss 0.00981551967561245 average time 0.0024925334496510914\n",
      "loss 0.005969937425106764 average time 0.0025121811997029\n",
      "loss 0.008011367172002792 average time 0.0024924373997685808\n",
      "loss 0.0024394600186496973 average time 0.0024972955426201225\n",
      "loss 0.004670633003115654 average time 0.002486706949857762\n",
      "loss 0.0031217550858855247 average time 0.002490688755396857\n",
      "loss 0.00989074818789959 average time 0.002303919098805636\n",
      "loss 0.005176662467420101 average time 0.0023302669001568576\n",
      "loss 0.024224476888775826 average time 0.002311985633219592\n",
      "loss 0.011614203453063965 average time 0.002322040874307277\n",
      "loss 0.008567669428884983 average time 0.0023129885592963547\n",
      "loss 0.013332166709005833 average time 0.0023202194328187033\n",
      "loss 0.004015179350972176 average time 0.002311407542330146\n",
      "loss 0.0058952211402356625 average time 0.0023014106245173023\n",
      "loss 0.0037745432928204536 average time 0.002293261821782734\n",
      "loss 0.010012602433562279 average time 0.0023004114018403926\n",
      "loss 0.0067916554398834705 average time 0.0023014212509733624\n",
      "loss 0.010977339930832386 average time 0.002310912666764731\n",
      "loss 0.00615346897393465 average time 0.0023273780748422723\n",
      "loss 0.0064738839864730835 average time 0.002326135899883229\n",
      "loss 0.011571858078241348 average time 0.0023280454333192515\n",
      "loss 0.003833054332062602 average time 0.0023199887000124103\n",
      "loss 0.005675910972058773 average time 0.002312652712331328\n",
      "loss 0.003383891424164176 average time 0.0023113099554514824\n",
      "loss 0.006304844282567501 average time 0.0023116063002380544\n",
      "loss 0.004494825378060341 average time 0.002386857650068123\n",
      "loss 0.01559444423764944 average time 0.002444216366740875\n",
      "loss 0.007974019274115562 average time 0.002471481000131462\n",
      "loss 0.006962255109101534 average time 0.0024905549801536838\n",
      "loss 0.006537348031997681 average time 0.002490562666760525\n",
      "loss 0.0021091667003929615 average time 0.0024822729999598647\n",
      "loss 0.0037034659180790186 average time 0.0024821133124132756\n",
      "loss 0.0028361512813717127 average time 0.002474533355594354\n",
      "loss 0.021512441337108612 average time 0.0024957111011026426\n",
      "loss 0.009581544436514378 average time 0.0025242356513044796\n",
      "loss 0.01665641926229 average time 0.002493221534008626\n",
      "loss 0.008248001337051392 average time 0.002473609650711296\n",
      "loss 0.008406157605350018 average time 0.002460088540671859\n",
      "loss 0.019695516675710678 average time 0.0024664686004592417\n",
      "loss 0.004750844556838274 average time 0.0024502805290394464\n",
      "loss 0.005877876188606024 average time 0.0024403059253818356\n",
      "loss 0.005108516663312912 average time 0.0024405811669874107\n",
      "loss 0.00922477152198553 average time 0.0025117007999215274\n",
      "loss 0.00686680618673563 average time 0.0025289794494747185\n",
      "loss 0.01182597503066063 average time 0.0024961222664957555\n",
      "loss 0.007635057903826237 average time 0.002515191749800579\n",
      "loss 0.003229227615520358 average time 0.002483767679601442\n",
      "loss 0.0028136991895735264 average time 0.002474644182783474\n",
      "loss 0.0013673653593286872 average time 0.0024680179851379112\n",
      "loss 0.002161469077691436 average time 0.0024624046744502266\n",
      "loss 0.0017386986874043941 average time 0.002465532366231653\n",
      "loss 0.12187370657920837 average time 0.002509485700575169\n",
      "loss 0.012235624715685844 average time 0.002510655000223778\n",
      "loss 0.014436845667660236 average time 0.0025022623001714236\n",
      "loss 0.01165082398802042 average time 0.0025211201497586442\n",
      "loss 0.008537943474948406 average time 0.002514322439779062\n",
      "loss 0.02305874042212963 average time 0.0025160120665095745\n",
      "loss 0.005490373354405165 average time 0.0025026516429331553\n",
      "loss 0.007433093152940273 average time 0.002487195287540089\n",
      "loss 0.0068793389946222305 average time 0.0024778494110990626\n",
      "loss 0.12286276370286942 average time 0.002483378599572461\n",
      "loss 0.010006420314311981 average time 0.002447865499911131\n",
      "loss 0.010938500054180622 average time 0.002485073266822534\n",
      "loss 0.008518067188560963 average time 0.0024737968248664402\n",
      "loss 0.012033376842737198 average time 0.0024591588796465656\n",
      "loss 0.023896094411611557 average time 0.00245127648304333\n",
      "loss 0.0036892907228320837 average time 0.0024481396996915076\n",
      "loss 0.006482854951173067 average time 0.002437967637190013\n",
      "loss 0.005423674359917641 average time 0.002438202210876625\n",
      "loss 0.07404089719057083 average time 0.0024666659014765174\n",
      "loss 0.010005111806094646 average time 0.0024817682011635043\n",
      "loss 0.014228723011910915 average time 0.002491827201078801\n",
      "loss 0.010282935574650764 average time 0.0024904226757207653\n",
      "loss 0.012140599079430103 average time 0.0024900417003314943\n",
      "loss 0.015742911025881767 average time 0.002472759116955179\n",
      "loss 0.0033579766750335693 average time 0.0024625721001648344\n",
      "loss 0.006164591759443283 average time 0.002450510400238272\n",
      "loss 0.004670127294957638 average time 0.0024412249447001767\n",
      "loss 0.005944509990513325 average time 0.002389063199632801\n",
      "loss 0.012648914009332657 average time 0.0024043619494186715\n",
      "loss 0.01857069507241249 average time 0.0024224751331882242\n",
      "loss 0.010568525642156601 average time 0.002449038174949237\n",
      "loss 0.007083946373313665 average time 0.0024695842601242476\n",
      "loss 0.006049587391316891 average time 0.00246228490009283\n",
      "loss 0.0018181286286562681 average time 0.0024477374858023334\n",
      "loss 0.0028981580398976803 average time 0.0024373188625322656\n",
      "loss 0.0024919952265918255 average time 0.0024319953779858125\n",
      "loss 0.1463261991739273 average time 0.0024535537009942347\n",
      "loss 0.013522246852517128 average time 0.002459867000754457\n",
      "loss 0.016534429043531418 average time 0.002444478267279919\n",
      "loss 0.009789245203137398 average time 0.002436649900453631\n",
      "loss 0.012887079268693924 average time 0.002454908640461508\n",
      "loss 0.02054969221353531 average time 0.002453219650167739\n",
      "loss 0.004130031913518906 average time 0.0024580529431862358\n",
      "loss 0.006228590849786997 average time 0.0024495905128715093\n",
      "loss 0.005480934400111437 average time 0.002458093500476227\n",
      "loss 0.007328683976083994 average time 0.0024299730003695\n",
      "loss 0.006172363646328449 average time 0.0024620155494776553\n",
      "loss 0.008442903868854046 average time 0.0025042196327704005\n",
      "loss 0.008421757258474827 average time 0.002477297874633223\n",
      "loss 0.00910347793251276 average time 0.002478432239918038\n",
      "loss 0.016009243205189705 average time 0.0024709114164676673\n",
      "loss 0.003585438011214137 average time 0.002467047599801195\n",
      "loss 0.005838155746459961 average time 0.0024849568498539155\n",
      "loss 0.00400712201371789 average time 0.0025109820555274686\n",
      "loss 0.14949128031730652 average time 0.0024935559998848474\n",
      "loss 0.016909733414649963 average time 0.0025504845998075326\n",
      "loss 0.024007728323340416 average time 0.002545071566477418\n",
      "loss 0.009363237768411636 average time 0.0025201164998288734\n",
      "loss 0.007336302660405636 average time 0.002508421559666749\n",
      "loss 0.02304253913462162 average time 0.002490858049801318\n",
      "loss 0.003825586289167404 average time 0.0024828687855208823\n",
      "loss 0.006914135068655014 average time 0.0024714910874245105\n",
      "loss 0.006173708476126194 average time 0.002470234699728381\n",
      "loss 0.005762364249676466 average time 0.002435801499406807\n",
      "loss 0.004979026969522238 average time 0.002433532799972454\n",
      "loss 0.004565351642668247 average time 0.002448275599919725\n",
      "loss 0.0069265589118003845 average time 0.0024470117501477944\n",
      "loss 0.00752211082726717 average time 0.002434462580445688\n",
      "loss 0.011068098247051239 average time 0.0024243912838962083\n",
      "loss 0.0028592352755367756 average time 0.0024189235147454645\n",
      "loss 0.004544302821159363 average time 0.0024252071879818688\n",
      "loss 0.0030307250563055277 average time 0.002438783389354487\n",
      "loss 0.007977140136063099 average time 0.002496399799187202\n",
      "loss 0.006463943514972925 average time 0.002501046600082191\n",
      "loss 0.02614184282720089 average time 0.0025052427000094514\n",
      "loss 0.00848241988569498 average time 0.0025034276496444362\n",
      "loss 0.012500972487032413 average time 0.002476842059683986\n",
      "loss 0.012125114910304546 average time 0.0024752366166309607\n",
      "loss 0.003471395233646035 average time 0.0024792455428978427\n",
      "loss 0.0052606468088924885 average time 0.002486435287479253\n",
      "loss 0.004306040704250336 average time 0.002500431266555097\n",
      "loss 0.005789211951196194 average time 0.0025296433998155407\n",
      "loss 0.004823760129511356 average time 0.0025463875493733214\n",
      "loss 0.004577567335218191 average time 0.002545359233045019\n",
      "loss 0.011821059510111809 average time 0.0025300152242707552\n",
      "loss 0.0041889590211212635 average time 0.002527709539385978\n",
      "loss 0.005647141952067614 average time 0.002509269916161429\n",
      "loss 0.0016454170690849423 average time 0.002502583342363193\n",
      "loss 0.0026154478546231985 average time 0.0025004202744894426\n",
      "loss 0.0020169594790786505 average time 0.0025021420327765455\n",
      "loss 0.012308105826377869 average time 0.002485160399693996\n",
      "loss 0.005100810434669256 average time 0.0024616914002399425\n",
      "loss 0.019042503088712692 average time 0.002482005567018253\n",
      "loss 0.010021543130278587 average time 0.002492925500337151\n",
      "loss 0.006972632370889187 average time 0.002477309460134711\n",
      "loss 0.01223120279610157 average time 0.0024676905831050437\n",
      "loss 0.0036542844027280807 average time 0.0024631333711191213\n",
      "loss 0.005320434458553791 average time 0.002470509437516739\n",
      "loss 0.004714554641395807 average time 0.002496344855630822\n",
      "loss 0.08497026562690735 average time 0.002571296000853181\n",
      "loss 0.014402732253074646 average time 0.002556410200253595\n",
      "loss 0.014774864539504051 average time 0.0025440657000872307\n",
      "loss 0.009050264954566956 average time 0.002540893600205891\n",
      "loss 0.012900867499411106 average time 0.0025513571000192314\n",
      "loss 0.03304677829146385 average time 0.002557497866606961\n",
      "loss 0.004984678700566292 average time 0.002536794328272143\n",
      "loss 0.006813293322920799 average time 0.0025467899496579775\n",
      "loss 0.0071298228576779366 average time 0.0025347068220855566\n",
      "loss 0.006813501473516226 average time 0.0024253524006926453\n",
      "loss 0.005915932357311249 average time 0.002499517500255024\n",
      "loss 0.008867035619914532 average time 0.002479535833854849\n",
      "loss 0.0057934788055717945 average time 0.002466429450461874\n",
      "loss 0.006660721264779568 average time 0.002443270340457093\n",
      "loss 0.01520098652690649 average time 0.002440038617341391\n",
      "loss 0.0036615817807614803 average time 0.002439990414913544\n",
      "loss 0.005081668961793184 average time 0.0024505891755616177\n",
      "loss 0.004177615512162447 average time 0.0024421435004721086\n",
      "loss 0.005692577920854092 average time 0.0024334977989201433\n",
      "loss 0.005225648637861013 average time 0.002451974199531833\n",
      "loss 0.013021967373788357 average time 0.002461036632885225\n",
      "loss 0.009218455292284489 average time 0.002444449249698664\n",
      "loss 0.006682302802801132 average time 0.00245454637974035\n",
      "loss 0.015554890036582947 average time 0.002448477016548471\n",
      "loss 0.003644598415121436 average time 0.0024598301855730826\n",
      "loss 0.005471070762723684 average time 0.0024777445873551186\n",
      "loss 0.004061251878738403 average time 0.0024746313888010466\n",
      "loss 0.008418364450335503 average time 0.0023714352016104387\n",
      "loss 0.005958453752100468 average time 0.0023732109511911403\n",
      "loss 0.0056542158126831055 average time 0.002372735200857278\n",
      "loss 0.008985093794763088 average time 0.002414681500653387\n",
      "loss 0.0049407766200602055 average time 0.0023987171006505377\n",
      "loss 0.010434933938086033 average time 0.0024178677839130007\n",
      "loss 0.003647997509688139 average time 0.0024212801718302736\n",
      "loss 0.004677479155361652 average time 0.0024263579878970633\n",
      "loss 0.0033921590074896812 average time 0.002430614500319482\n",
      "loss 0.006989369168877602 average time 0.0024851169005851263\n",
      "loss 0.005088301841169596 average time 0.0024780641501129137\n",
      "loss 0.0074350181967020035 average time 0.0024778845668382322\n",
      "loss 0.012503493577241898 average time 0.00244869224994909\n",
      "loss 0.005492670927196741 average time 0.002447268279932905\n",
      "loss 0.011160767637193203 average time 0.002438206683388368\n",
      "loss 0.003760921536013484 average time 0.002441791371409116\n",
      "loss 0.005387307144701481 average time 0.0024549117874194054\n",
      "loss 0.003403295762836933 average time 0.00244583765543454\n",
      "loss 0.006977367214858532 average time 0.002433050999825355\n",
      "loss 0.004362320061773062 average time 0.0024499130506883374\n",
      "loss 0.008579999208450317 average time 0.0024300997339887546\n",
      "loss 0.017813924700021744 average time 0.002434830925412825\n",
      "loss 0.005062514916062355 average time 0.0024341816804371776\n",
      "loss 0.007994864135980606 average time 0.0024398387836893866\n",
      "loss 0.0035160689149051905 average time 0.0024511272716169646\n",
      "loss 0.003577429335564375 average time 0.002448099850276776\n",
      "loss 0.0030039374250918627 average time 0.002446772944764234\n",
      "loss 0.005700038745999336 average time 0.0025031291994964703\n",
      "loss 0.0037528264801949263 average time 0.0024958012992574367\n",
      "loss 0.009738304652273655 average time 0.0024509564663943214\n",
      "loss 0.009839871898293495 average time 0.0024324457749171414\n",
      "loss 0.006171844899654388 average time 0.0024225184396374972\n",
      "loss 0.008577389642596245 average time 0.002428624799688502\n",
      "loss 0.0035324101336300373 average time 0.0024482017139338757\n",
      "loss 0.004375249147415161 average time 0.0024501208247238535\n",
      "loss 0.003029374172911048 average time 0.002455266955302149\n",
      "loss 0.006613295990973711 average time 0.0024577078015427106\n",
      "loss 0.004288831260055304 average time 0.0024344194007571785\n",
      "loss 0.00818607211112976 average time 0.002465863400468758\n",
      "loss 0.01532090175896883 average time 0.0024407036251359385\n",
      "loss 0.005981405731290579 average time 0.002439147480111569\n",
      "loss 0.007705476135015488 average time 0.002442349550159027\n",
      "loss 0.00353812868706882 average time 0.0024472750001836435\n",
      "loss 0.0038073239848017693 average time 0.0024502452751985404\n",
      "loss 0.003065051045268774 average time 0.002451819344643607\n",
      "loss 0.00519955437630415 average time 0.002417449499480426\n",
      "loss 0.004322705790400505 average time 0.002439853350195335\n",
      "loss 0.016316575929522514 average time 0.0024711261000290204\n",
      "loss 0.007644845172762871 average time 0.002459068399984972\n",
      "loss 0.0031854126136749983 average time 0.0024897110996418633\n",
      "loss 0.009662164375185966 average time 0.002496430932971028\n",
      "loss 0.0027128641959279776 average time 0.002486268514102059\n",
      "loss 0.0035778486635535955 average time 0.002488883487407293\n",
      "loss 0.0030341350939124823 average time 0.002488036099743719\n",
      "loss 0.005649481900036335 average time 0.0024682891005068084\n",
      "loss 0.003897898131981492 average time 0.002413028200680856\n",
      "loss 0.013805185444653034 average time 0.002398609567200765\n",
      "loss 0.00778260687366128 average time 0.002399592050322099\n",
      "loss 0.0062326365150511265 average time 0.0024189294201903975\n",
      "loss 0.019142502918839455 average time 0.002430400150216883\n",
      "loss 0.0029654945246875286 average time 0.002445078257246808\n",
      "loss 0.005645678378641605 average time 0.002451894874990103\n",
      "loss 0.0030501708388328552 average time 0.0024485928333532582\n",
      "loss 0.00687863165512681 average time 0.0025350295006064697\n",
      "loss 0.005317531060427427 average time 0.0024403970003477296\n",
      "loss 0.012082654982805252 average time 0.002429604867143401\n",
      "loss 0.005206200759857893 average time 0.002418054275069153\n",
      "loss 0.003499900223687291 average time 0.002405690179974772\n",
      "loss 0.007743365131318569 average time 0.002423975383503906\n",
      "loss 0.002815838437527418 average time 0.002435591900072593\n",
      "loss 0.0031305518932640553 average time 0.0024393108625372405\n",
      "loss 0.0023469729349017143 average time 0.002451967566519872\n",
      "loss 0.00646580196917057 average time 0.002526638999406714\n",
      "loss 0.004172059707343578 average time 0.002443057899334235\n",
      "loss 0.006581734865903854 average time 0.0024230150658792506\n",
      "loss 0.014990990050137043 average time 0.002410989949552459\n",
      "loss 0.0029536616057157516 average time 0.002422446519485675\n",
      "loss 0.008604316972196102 average time 0.002414959249756066\n",
      "loss 0.0027672462165355682 average time 0.002418169928456856\n",
      "loss 0.0031122814398258924 average time 0.0024512726374014164\n",
      "loss 0.002153401030227542 average time 0.00246515974425064\n",
      "loss 0.004061552695930004 average time 0.002517286100250203\n",
      "loss 0.00801723450422287 average time 0.0024420930000196676\n",
      "loss 0.010562832467257977 average time 0.002433184166283657\n",
      "loss 0.0029591855127364397 average time 0.0024396245995449133\n",
      "loss 0.0033332514576613903 average time 0.0024529559995396994\n",
      "loss 0.004667255561798811 average time 0.002453903282643296\n",
      "loss 0.00242652022279799 average time 0.002460213170827566\n",
      "loss 0.0026519664097577333 average time 0.002468849449469417\n",
      "loss 0.001918917871080339 average time 0.0024732048439262953\n",
      "loss 0.004812239669263363 average time 0.002481322000210639\n",
      "loss 0.0032214203383773565 average time 0.0024465172495984006\n",
      "loss 0.009125676937401295 average time 0.0024324759996961804\n",
      "loss 0.0033464464358985424 average time 0.0024273552743543404\n",
      "loss 0.0034066259395331144 average time 0.002428736259404104\n",
      "loss 0.0034724785946309566 average time 0.002422761066321982\n",
      "loss 0.0020056082867085934 average time 0.0024228314710032593\n",
      "loss 0.0021020814310759306 average time 0.0024327919246279633\n",
      "loss 0.0014274377608671784 average time 0.0024368998886978564\n",
      "loss 0.011810657568275928 average time 0.002511369000421837\n",
      "loss 0.0059148273430764675 average time 0.0024505668502533807\n",
      "loss 0.020259296521544456 average time 0.0024374079334860046\n",
      "loss 0.010210537351667881 average time 0.0024673998252837917\n",
      "loss 0.0041888440027832985 average time 0.002472909400064964\n",
      "loss 0.006056296639144421 average time 0.0024723443168234854\n",
      "loss 0.0019073113799095154 average time 0.0024672175000554747\n",
      "loss 0.002176116919144988 average time 0.002472874025123019\n",
      "loss 0.001668872544541955 average time 0.0024780226334631963\n",
      "loss 0.005466872826218605 average time 0.0023276672001811675\n",
      "loss 0.0028839227743446827 average time 0.002342073700012406\n",
      "loss 0.01259954459965229 average time 0.0023537977669232835\n",
      "loss 0.00616106390953064 average time 0.002368845825039898\n",
      "loss 0.0023465356789529324 average time 0.002397114640218206\n",
      "loss 0.0020420264918357134 average time 0.00241385458370011\n",
      "loss 0.0009579853503964841 average time 0.002412473943300678\n",
      "loss 0.0011895911302417517 average time 0.002424156412766024\n",
      "loss 0.0007781174499541521 average time 0.002439695933607355\n",
      "loss 0.006910034455358982 average time 0.002366168700391427\n",
      "loss 0.0060769254341721535 average time 0.002334339100460056\n",
      "loss 0.03092196211218834 average time 0.0023144903673091902\n",
      "loss 0.006782271899282932 average time 0.002326538975525182\n",
      "loss 0.0027815597131848335 average time 0.002368436740303878\n",
      "loss 0.0058971731923520565 average time 0.0023675334668757085\n",
      "loss 0.002722245641052723 average time 0.002397032143024262\n",
      "loss 0.003050882136449218 average time 0.002410458787657262\n",
      "loss 0.002357962541282177 average time 0.002415879877917986\n",
      "loss 0.006556277628988028 average time 0.002358869099698495\n",
      "loss 0.003533371724188328 average time 0.0024082172501075547\n",
      "loss 0.007880040444433689 average time 0.002408273066704472\n",
      "loss 0.009663469158113003 average time 0.002420873625189415\n",
      "loss 0.0033675432205200195 average time 0.0024148016202962027\n",
      "loss 0.004778577946126461 average time 0.0024201278834661936\n",
      "loss 0.0016562369419261813 average time 0.0024155005856633317\n",
      "loss 0.002205991419032216 average time 0.0024140400749529363\n",
      "loss 0.0015590530820190907 average time 0.002414163666667365\n",
      "loss 0.0049796998500823975 average time 0.002389824299432803\n",
      "loss 0.003539342898875475 average time 0.0024477227994357236\n",
      "loss 0.006887987721711397 average time 0.0024595417998692333\n",
      "loss 0.012595204636454582 average time 0.0024605580745701446\n",
      "loss 0.004783811047673225 average time 0.0024544914996367877\n",
      "loss 0.012628590688109398 average time 0.002457731449782538\n",
      "loss 0.0033433926291763783 average time 0.002457885042864031\n",
      "loss 0.0051760426722466946 average time 0.0024524445124407064\n",
      "loss 0.002820034045726061 average time 0.0024640781777755666\n",
      "loss 0.004633646924048662 average time 0.0024185580997145733\n",
      "loss 0.0029603259172290564 average time 0.002418142799549969\n",
      "loss 0.007609812077134848 average time 0.0024372437665006147\n",
      "loss 0.00341088161803782 average time 0.0024415419496799586\n",
      "loss 0.0012592739658430219 average time 0.002449754139757715\n",
      "loss 0.0005451664328575134 average time 0.0024531811165992017\n",
      "loss 0.0005254317657090724 average time 0.002467862785752264\n",
      "loss 0.0005396572523750365 average time 0.0024718133625065094\n",
      "loss 0.00023948300804477185 average time 0.002470299011109293\n",
      "loss 0.024381889030337334 average time 0.0024252284010290166\n",
      "loss 0.010386687703430653 average time 0.002414049199898727\n",
      "loss 0.01363853458315134 average time 0.0024020282332009326\n",
      "loss 0.012288229539990425 average time 0.0024129326246766143\n",
      "loss 0.007410201244056225 average time 0.0024284899797290564\n",
      "loss 0.014358529821038246 average time 0.00242552638304187\n",
      "loss 0.00413883151486516 average time 0.0024401619285677693\n",
      "loss 0.0050450763665139675 average time 0.002437833212519763\n",
      "loss 0.005485645029693842 average time 0.0024362944555598206\n",
      "loss 0.09170983731746674 average time 0.0024802120990352703\n",
      "loss 0.00805027224123478 average time 0.0024515673995774705\n",
      "loss 0.008248872123658657 average time 0.0024568980666032684\n",
      "loss 0.005362744443118572 average time 0.002476775100003579\n",
      "loss 0.007327945437282324 average time 0.002471376500185579\n",
      "loss 0.006217119749635458 average time 0.002468802383547882\n",
      "loss 0.0037788557820022106 average time 0.0024845466574604093\n",
      "loss 0.0031131922733038664 average time 0.0024823966879193903\n",
      "loss 0.0034313781652599573 average time 0.002487550211397724\n",
      "loss 0.00475219776853919 average time 0.0023415335004101506\n",
      "loss 0.005785541608929634 average time 0.002388040400488535\n",
      "loss 0.008631081320345402 average time 0.0023805783669619512\n",
      "loss 0.015177109278738499 average time 0.002392314850134426\n",
      "loss 0.0036686311941593885 average time 0.002402243759960402\n",
      "loss 0.005366185214370489 average time 0.0024163774502521844\n",
      "loss 0.002477083122357726 average time 0.0024242745572285325\n",
      "loss 0.002763091353699565 average time 0.002441919062541274\n",
      "loss 0.0024403142742812634 average time 0.002438400200011933\n",
      "loss 0.004986961837857962 average time 0.002421148798952345\n",
      "loss 0.0057849702425301075 average time 0.002420500499458285\n",
      "loss 0.005189492367208004 average time 0.0024458838330659394\n",
      "loss 0.0029186473693698645 average time 0.0024510723998828325\n",
      "loss 0.0018014325760304928 average time 0.0024526713798288255\n",
      "loss 0.0012044068425893784 average time 0.0024612419666179145\n",
      "loss 0.0007250875351019204 average time 0.002454368942733189\n",
      "loss 0.0010698243277147412 average time 0.0024635565373901046\n",
      "loss 0.0006432200316339731 average time 0.00246345559985558\n",
      "loss 0.19101840257644653 average time 0.002355528900399804\n",
      "loss 0.011017708107829094 average time 0.0023700796007178723\n",
      "loss 0.010599255561828613 average time 0.002407096533648049\n",
      "loss 0.005669764708727598 average time 0.002411976450093789\n",
      "loss 0.004789206199347973 average time 0.0024164198998361825\n",
      "loss 0.006364812608808279 average time 0.002448082649886298\n",
      "loss 0.002545073861256242 average time 0.002447426871305132\n",
      "loss 0.002756139263510704 average time 0.002444581087329425\n",
      "loss 0.0024040802381932735 average time 0.0024372813665023488\n",
      "loss 0.0366811603307724 average time 0.0023078733999864196\n",
      "loss 0.004710696637630463 average time 0.002367225799680455\n",
      "loss 0.004146604333072901 average time 0.0023947664330674647\n",
      "loss 0.005901919677853584 average time 0.0023924822998524177\n",
      "loss 0.0033556055277585983 average time 0.002426855539903045\n",
      "loss 0.0051635256968438625 average time 0.002444825433398364\n",
      "loss 0.0021991876419633627 average time 0.002466931200103967\n",
      "loss 0.0023468974977731705 average time 0.002479347125023196\n",
      "loss 0.0021980577148497105 average time 0.0024834740222755094\n",
      "loss 0.06553424894809723 average time 0.0024521525002783166\n",
      "loss 0.0030909674242138863 average time 0.0024273514001106377\n",
      "loss 0.0037639334332197905 average time 0.0024728967332242366\n",
      "loss 0.006217143032699823 average time 0.00246487889981654\n",
      "loss 0.0024118609726428986 average time 0.0024659381797653623\n",
      "loss 0.004460700787603855 average time 0.0024790506164426916\n",
      "loss 0.0018594691064208746 average time 0.0024638687712223535\n",
      "loss 0.001785376574844122 average time 0.002460395174814039\n",
      "loss 0.0017253331607207656 average time 0.0024578205442715747\n",
      "loss 0.014566635712981224 average time 0.0023080163009581156\n",
      "loss 0.00379238766618073 average time 0.002361566150590079\n",
      "loss 0.00182953174225986 average time 0.0024026607002015227\n",
      "loss 0.0035885970573872328 average time 0.002415857024869183\n",
      "loss 0.002676516305655241 average time 0.0024125481198891065\n",
      "loss 0.0019835946150124073 average time 0.002412851099865899\n",
      "loss 0.0009213161538355052 average time 0.0024230192713190005\n",
      "loss 0.001120184431783855 average time 0.00242041688743484\n",
      "loss 0.0008173497044481337 average time 0.002415523666655645\n",
      "loss 0.006170658860355616 average time 0.0023159249001764694\n",
      "loss 0.0041797710582613945 average time 0.002465693649923196\n",
      "loss 0.00692589208483696 average time 0.0024958193331064346\n",
      "loss 0.005243360996246338 average time 0.0024874273498426192\n",
      "loss 0.004528178367763758 average time 0.0024967464797431603\n",
      "loss 0.004893166013062 average time 0.002507733199823027\n",
      "loss 0.002198884729295969 average time 0.0025106962715342107\n",
      "loss 0.002387844957411289 average time 0.002515163850184763\n",
      "loss 0.002166390884667635 average time 0.0025087592001396437\n",
      "loss 0.0028007656801491976 average time 0.0023509942004457116\n",
      "loss 0.002517889952287078 average time 0.0024174335497373248\n",
      "loss 0.0018660076893866062 average time 0.0024528914666734638\n",
      "loss 0.0060667311772704124 average time 0.002461141799998586\n",
      "loss 0.0023422958329319954 average time 0.0024538036400685087\n",
      "loss 0.0009193439618684351 average time 0.002458936016560377\n",
      "loss 0.0009487955248914659 average time 0.0024484126284119805\n",
      "loss 0.0008471134351566434 average time 0.0024569411248376127\n",
      "loss 0.0008355996105819941 average time 0.0024586130665265955\n",
      "loss 0.006284948438405991 average time 0.002355097200605087\n",
      "loss 0.004369625821709633 average time 0.0024054224003339187\n",
      "loss 0.006329707335680723 average time 0.002435425033404802\n",
      "loss 0.007900182157754898 average time 0.0024685572749731364\n",
      "loss 0.005374849773943424 average time 0.0024866453000693583\n",
      "loss 0.005562476813793182 average time 0.0024688065501201587\n",
      "loss 0.0022399842273443937 average time 0.002482670385964281\n",
      "loss 0.0023449750151485205 average time 0.002477188737510005\n",
      "loss 0.0018739679362624884 average time 0.00245854387770588\n",
      "loss 0.0018613293068483472 average time 0.002355195799842477\n",
      "loss 0.0016412450931966305 average time 0.0024281726501940285\n",
      "loss 0.0014844124671071768 average time 0.0024316300329131385\n",
      "loss 0.0023314617574214935 average time 0.0024299936248571613\n",
      "loss 0.0012176060117781162 average time 0.002466085599910002\n",
      "loss 0.001962153008207679 average time 0.0024817447333674255\n",
      "loss 0.0008510391344316304 average time 0.0024854712858712966\n",
      "loss 0.000596198602579534 average time 0.002480741200102784\n",
      "loss 0.0005583752645179629 average time 0.002469959022451399\n",
      "loss 0.05237913876771927 average time 0.0024276961992727593\n",
      "loss 0.00420153234153986 average time 0.0024752281496184876\n",
      "loss 0.0068256305530667305 average time 0.0024801761662626327\n",
      "loss 0.006297028623521328 average time 0.0025119692999433026\n",
      "loss 0.0034521257039159536 average time 0.00256412298012292\n",
      "loss 0.004262437112629414 average time 0.0026089803499344272\n",
      "loss 0.0020973440259695053 average time 0.002609705228483238\n",
      "loss 0.0018737289356067777 average time 0.002595929274910304\n",
      "loss 0.0018536221468821168 average time 0.002575227422122326\n",
      "loss 0.0036421793047338724 average time 0.0025224171990412287\n",
      "loss 0.003904554760083556 average time 0.002490489299991168\n",
      "loss 0.002633129945024848 average time 0.0024546358333318494\n",
      "loss 0.009748028591275215 average time 0.002462400674892706\n",
      "loss 0.0014942511916160583 average time 0.002474613719782792\n",
      "loss 0.0008543426520191133 average time 0.00246260673305369\n",
      "loss 0.0007204097928479314 average time 0.0024630681570353254\n",
      "loss 0.0007212880300357938 average time 0.00245872984995367\n",
      "loss 0.0005704503855668008 average time 0.002445314933241914\n",
      "loss 0.00949144922196865 average time 0.002592722899687942\n",
      "loss 0.005680898204445839 average time 0.0025690621997637208\n",
      "loss 0.007086787838488817 average time 0.0025353491998199996\n",
      "loss 0.012870918959379196 average time 0.00250887225009501\n",
      "loss 0.0021949755027890205 average time 0.0025256297599873506\n",
      "loss 0.004054742865264416 average time 0.002512780483647172\n",
      "loss 0.001814226619899273 average time 0.0025229305430964034\n",
      "loss 0.0017888015136122704 average time 0.0025130408751574576\n",
      "loss 0.001433227676898241 average time 0.002498821477994271\n",
      "loss 0.005690556485205889 average time 0.0024226615012157707\n",
      "loss 0.004605959635227919 average time 0.0024493260007002393\n",
      "loss 0.008174845017492771 average time 0.00244369530074376\n",
      "loss 0.00492594949901104 average time 0.0024653586256317794\n",
      "loss 0.0024483702145516872 average time 0.00245156054032268\n",
      "loss 0.0026619085110723972 average time 0.0024692632002115715\n",
      "loss 0.0010389216477051377 average time 0.002465823185813081\n",
      "loss 0.001073028426617384 average time 0.002455541562587314\n",
      "loss 0.0007291187066584826 average time 0.0024542594333890723\n",
      "loss 0.0050616394728422165 average time 0.0024384064992773345\n",
      "loss 0.003712043631821871 average time 0.0024635907497722654\n",
      "loss 0.0036385576240718365 average time 0.0024603037669633826\n",
      "loss 0.006425404455512762 average time 0.002456731075217249\n",
      "loss 0.002671355614438653 average time 0.0024727740401634946\n",
      "loss 0.004608178045600653 average time 0.002473563100211322\n",
      "loss 0.0022591566666960716 average time 0.002461435600292004\n",
      "loss 0.002129249507561326 average time 0.002444747287794598\n",
      "loss 0.0019420620519667864 average time 0.002434296222532996\n",
      "loss 0.005321149248629808 average time 0.0024430033992393872\n",
      "loss 0.0040947450324893 average time 0.0024538915496668777\n",
      "loss 0.004925510846078396 average time 0.0024772086997400036\n",
      "loss 0.0035343810450285673 average time 0.0024834073249221547\n",
      "loss 0.0021345342975109816 average time 0.002465273820026778\n",
      "loss 0.00195779069326818 average time 0.0024812338833580727\n",
      "loss 0.0009085528436116874 average time 0.002468286128506796\n",
      "loss 0.0007923506200313568 average time 0.0024502820374109435\n",
      "loss 0.0008794735185801983 average time 0.002442856166472969\n",
      "loss 0.0038620668929070234 average time 0.002411195299238898\n",
      "loss 0.0029996049124747515 average time 0.0024312110504833982\n",
      "loss 0.01057953480631113 average time 0.0024075607670044218\n",
      "loss 0.008006155490875244 average time 0.0024202379001362715\n",
      "loss 0.004271137993782759 average time 0.0024363533203140835\n",
      "loss 0.0053457473404705524 average time 0.0024541548835016634\n",
      "loss 0.0024862836580723524 average time 0.0024437692717266535\n",
      "loss 0.0028820419684052467 average time 0.0024360699753378868\n",
      "loss 0.002131194807589054 average time 0.0024307707891178627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 999400\n",
       "\tepoch: 100\n",
       "\tepoch_length: 9994\n",
       "\tmax_epochs: 100\n",
       "\toutput: 0.0018181210616603494\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import Timer\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from torch.cuda import amp       # apex.amp is deprecated. it cannot be regenerated.\n",
    "\n",
    "from snow_model import Net\n",
    "# from snow_model_module import Net\n",
    "# from snow_model_2 import Net\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used : {device}\")\n",
    "\n",
    "# from cupy_dataset import OptionDataSet\n",
    "timer = Timer(average=True)\n",
    "model = Net().cuda()\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "set_amp = True\n",
    "scaler = amp.GradScaler(enabled=set_amp)\n",
    "\n",
    "# dataset = OptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
    "# dataset = SnowballDataSet(max_len = 50000, number_path = 500000, batch = 4, threads = 512, seed  =1999 )\n",
    "# dataset = datasets.DatasetFolder(\n",
    "#     root=path,\n",
    "#     loader=npy_loader,\n",
    "#     extensions=['.pt']\n",
    "# )\n",
    "# dataset = atDataset()\n",
    "dataset = DataLoader(tdataset, 100)\n",
    "\n",
    "# dataset size is 10,000\n",
    "def train_update(engine, batch):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16): ########### automatic mixed precision\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred[:,0], y)\n",
    "        assert y_pred.dtype is torch.float16 ##################\n",
    "        assert loss.dtype is torch.float32 ##################\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    optimizer.zero_grad() # set_to_none=True here can modestly improve performance  WHAET DOES THIS DO\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_update)\n",
    "log_interval = 1000\n",
    "\n",
    "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "timer.attach(trainer,\n",
    "             start=Events.EPOCH_STARTED,\n",
    "             resume=Events.ITERATION_STARTED,\n",
    "             pause=Events.ITERATION_COMPLETED,\n",
    "             step=Events.ITERATION_COMPLETED)    \n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
    "    if iter % log_interval == 0:\n",
    "        print('loss', engine.state.output, 'average time', timer.value())\n",
    "\n",
    "# @trainer.on(Events.GET_BATCH_STARTED)\n",
    "# def log_training_loss(engine):\n",
    "#     print(\"EPOCH!!!!!!!!!!!!\\n\")\n",
    "        \n",
    "# trainer.run(dataset, max_epochs=100)\n",
    "trainer.run(dataset, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "hidden = 1024\n",
    "\n",
    "model_o = nn.Sequential(\n",
    "    nn.Linear(7, hidden),\n",
    "    F.elu(),\n",
    "    nn.Linear(hidden, hidden),\n",
    "    F.elu(),\n",
    "    nn.Linear(hidden, hidden),\n",
    "    F.elu(),\n",
    "    nn.Linear(hidden, hidden),\n",
    "    F.elu(),\n",
    "    nn.Linear(hidden, hidden),\n",
    "    F.elu(),\n",
    "    nn.Linear(hidden, 1),\n",
    "    F.elu(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(in_size, out_size, hidden,  num_layers):\n",
    "    layers = []\n",
    "    layers.append(torch.nn.Linear(in_size, hidden))\n",
    "    layers.append(torch.nn.functional.elu())\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(torch.nn.Linear(hidden, hidden))\n",
    "        layers.append(torch.nn.functional.elu())\n",
    "    layers.append(torch.nn.Linear(hidden, out_size))\n",
    "    return torch.nn.Sequential(*tuple(layers)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nah, id lose, and lose bad\n",
    "cudf is not supported on windows :/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = pics\\galaxy-angel-mint-blancmanche.gif>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_cudf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mset({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe.backend\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudf\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask_cudf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdelayed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_cuda\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalCUDACluster\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dask_cudf'"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "import dask_cudf\n",
    "from dask.delayed import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "cluster = LocalCUDACluster()\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
