{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning model time!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src = pics/OIP.jpg width = 400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "cupy.cuda.set_allocator(None)       # no clue\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "\n",
    "import numba\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset\n",
    "Each monte carlo simulation run is equivalent to one data point being made, so to generate a large dataset, we have to run monte carlo simulations lots of times, and batches can hypotheitcally make doing this faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here,the mc model from mc_snow, cuda version was imported and cleaned up a bit.\n",
    "note that due to the existence of batches, some of the varaibles now need a bit of extra finagling to access properly. (s_0, Ki, Ko, mu, sigma, pot,r, d_normals, snowball_path_holder). Overall design is very close to original, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit               # defualt GPU\n",
    "def monte_carlo_andtheholygrail_gpu(d_s, s_0, Ki, Ko, mu, sigma, pot,r,\n",
    "                                    d_normals, snowball_path_holder, MONTHS,\n",
    "                                    N_STEPS, N_PATHS, N_BATCH):\n",
    "    \n",
    "\n",
    "    # for shared memory (non)optimization\n",
    "    # shared = cuda.shared.array(shape=0, dtype=numba.float32)\n",
    "    # # load to shared memory\n",
    "    # path_offset = cuda.blockIdx.x * cuda.blockDim.x\n",
    "\n",
    "    # ii - overall thread index\n",
    "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
    "\n",
    "    for n in range(ii, N_PATHS * N_BATCH, stride):\n",
    "        # newly added vars for N_BATCH calculations\n",
    "        batch_id = n // N_PATHS\n",
    "        path_id = n % N_PATHS       # equivalent to n in old code \n",
    "\n",
    "        snowball_path_holder[n][0] = s_0[batch_id]\n",
    "        earlyexit = False\n",
    "        ki = False\n",
    "        mald = False\n",
    "        for t in range(N_STEPS):\n",
    "            # pre shared memory b_motion    \n",
    "            #                                                   \n",
    "            b_motion = d_normals[path_id + batch_id * N_PATHS +  t * N_PATHS * N_BATCH]\n",
    "\n",
    "            # post shared memory b_motion\n",
    "            # shared[cuda.threadIdx.x] = d_normals[path_offset + cuda.threadIdx.x + t * N_PATHS]\n",
    "\n",
    "            dt = 1/N_STEPS\n",
    "            # pre shared memory b_motion\n",
    "            ds = snowball_path_holder[n][t] * mu[batch_id] * dt + snowball_path_holder[n][t] \\\n",
    "                                                * sigma[batch_id] * b_motion * math.sqrt(dt) \n",
    "            # post shared memory b_motion\n",
    "            # ds = snowball_path_holder[n][t] * mu[batch_id] * dt + snowball_path_holder[n][t] * sigma[batch_id] * shared[cuda.threadIdx.x] * math.sqrt(dt) \n",
    "                    # no adjusting list sizes in cuda :(\n",
    "            # snowball_path.append(snowball_path[t]+ds)\n",
    "            snowball_path_holder[n][t+1] = snowball_path_holder[n][t] + ds\n",
    "            \n",
    "\n",
    "            # ki = snowball_path[t] + ds\n",
    "            if snowball_path_holder[n][t+1] <= Ki[batch_id]:\n",
    "                ki = True\n",
    "\n",
    "            if not mald:\n",
    "                for month in (0,1,2,3,4,5,6,7,8,9,10,11):                # need to do this instead because contains (in) and range are disabled\n",
    "                    if t+1 == MONTHS[month]:     #startday no longer used to fake a start date in code\n",
    "                        # price = t+1+startday\n",
    "                        if snowball_path_holder[n][t+1] >= Ko[batch_id]:\n",
    "                            price =  pot[batch_id] * t/365     # should turn t into int\n",
    "                            # return snowball_path, price\n",
    "                            d_s[n] =  price * math.exp(-r[batch_id] * t/N_STEPS)   # accounting for r\n",
    "                            snowball_path_holder[n][-1] = d_s[n]            \n",
    "                            earlyexit = True\n",
    "                            mald = True\n",
    "                            # print(\"blo got fucked\\n\")\n",
    "                            break\n",
    "\n",
    "        \n",
    "        if not earlyexit:       # to prevent early exit getting out of bdds error\n",
    "            # did not get knocked up or down\n",
    "            price = pot[batch_id]\n",
    "            # t  =T \n",
    "                        # CAN'T USE T CUZ CUDA IS FUCKING SHIT so use -1 instead\n",
    "                        # or not ig T works now :sob:\n",
    "            if ki and snowball_path_holder[n][N_STEPS] <= s_0[batch_id]:          # blo got knocked down and never recovered\n",
    "                price = snowball_path_holder[n][N_STEPS] - s_0[batch_id]\n",
    "            elif ki and snowball_path_holder[n][N_STEPS] <= Ko[batch_id]:          # blo got knocked down for a bit but finished above Ki\n",
    "                price =0\n",
    "            d_s[n] = price * math.exp(-r[batch_id])\n",
    "            snowball_path_holder[n][-1] = d_s[n]    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for some reason, increasing the number of paths and/or the number of batches greatly slows down my monte's carlo's computational speed. I have no clue why this is the case, but would guess that i ran out of threads or gpus to run it on idk. Max len controls the number of data points, path controls how accurate each data point is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once finished running, data is saved into a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 218.55252313613892 v 3.4649398 avg time 0.0004371050462722778\n",
      "[2.48650241 2.55197453 3.61434937 3.18972516 2.29606295 3.6561799\n",
      " 2.78525901 3.47168636 3.64172482 2.61491704 3.73917556 5.00415611\n",
      " 2.76766586 2.2322309  2.52930498 3.97677541 2.48851228 2.0818131\n",
      " 2.34652591 3.28007841 3.19151759 2.94404697 2.13764286 2.51589251\n",
      " 2.93998623 2.34121871 3.71826243 3.09563994 2.11125541 2.91977859\n",
      " 5.54066229 1.87857473 3.47983551 2.7077136  3.04440331 3.44855785\n",
      " 5.86951065 3.08669853 3.00474715 2.90706038 2.74558449 3.30613899\n",
      " 5.32979488 2.76495552 2.80513287 3.79385996 2.69700599 5.62145233\n",
      " 3.52202106 2.23062658 2.68645096 2.62305379 2.48622751 2.19815993\n",
      " 2.53211451 3.14271593 2.55764341 4.31880045 2.45253897 2.25858498\n",
      " 3.41656899 1.91439164 3.07305551 2.43276644 4.2993145  2.91624856\n",
      " 3.6151371  5.02526665 3.6280489  3.18084502 3.27265072 2.55380797\n",
      " 2.46093941 2.59254909 2.26893759 2.08010554 2.56983185 2.29685831\n",
      " 1.6850425  3.1227684  3.20789313 2.40722418 1.94819224 4.02403784\n",
      " 2.96871471 2.77916336 2.72094464 2.12727976 3.10076141 3.64356017\n",
      " 2.81456304 2.29984498 2.47417808 4.52234507 3.22902489 3.09147811\n",
      " 2.93304753 3.56214356 3.14877105 2.16398168 3.03018856 2.13970971\n",
      " 2.5755434  3.77483249 2.59811187 2.95206547 3.23604202 3.43282962\n",
      " 4.04611874 3.15293431 2.52806592 1.93495357 3.22159505 2.44160557\n",
      " 2.39139533 2.24127507 2.75239682 1.85675919 2.97917151 2.59803152\n",
      " 2.25819302 3.43586397 2.91614771 8.06955719 6.05174685 2.96349072\n",
      " 2.9132061  2.00045013 8.01063919 1.89453924 2.1407485  3.92876792\n",
      " 2.63154721 3.619807   1.73578572 2.93332195 2.06356478 2.79142857\n",
      " 3.73849654 2.92662239 2.36344266 2.76774049 2.96401548 4.14198446\n",
      " 5.21088362 4.14115143 4.85788441 3.24885416 2.5982039  2.35497499\n",
      " 2.76333761 2.01523042 2.63492036 2.11429667 2.40367103 2.63333797\n",
      " 2.40664744 2.87141991 2.03321886 3.00813961 2.59830809 2.97362304\n",
      " 3.42921019 1.95382631 3.89661789 2.39198518 2.55530453 2.23459578\n",
      " 2.77614021 5.48749638 2.12990808 3.1027689  3.07399511 2.86521697\n",
      " 2.13807893 2.84433866 4.74178457 3.27533293 5.68550205 2.60753393\n",
      " 2.73734546 3.50797558 2.6842494  2.68429542 3.55544996 2.86565948\n",
      " 2.88519692 2.82836246 2.444206   3.17334247 3.4082768  2.54473329\n",
      " 2.88872695 3.18511415 2.57193542 3.04188848 3.78055048 2.48680949\n",
      " 3.03853989 3.55356145 2.35958934 3.92916965 2.93290448 2.40670657\n",
      " 2.76156449 3.7849226  3.54930472 5.98246765 2.03185463 3.04398584\n",
      " 2.2128315  3.00816488 3.5362606  4.18886709 2.09864807 2.21243906\n",
      " 2.85005546 3.2664175  2.47473049 3.82576251 5.30987978 3.91397929\n",
      " 2.13161993 2.5665586  3.2639091  2.14943457 3.40556145 3.28954959\n",
      " 2.20433879 3.44182634 4.21401072 2.84010673 2.94515657 1.8444339\n",
      " 2.34804916 1.85255146 2.5386951  3.88837099 1.69840384 2.94895625\n",
      " 4.24771595 5.09591722 3.4366951  4.97906113 3.5378437  2.59873366\n",
      " 2.55474806 1.95883811 2.08344388 2.94910598 4.06345177 2.30396152\n",
      " 2.80504608 1.99937928 1.66320443 2.96089697 5.01022434 1.9764123\n",
      " 3.56293106 2.48064232 4.95951319 3.77419567 3.92148256 3.09138751\n",
      " 2.620507   2.94472361 2.17494798 4.29258871 3.15536141 4.40539503\n",
      " 5.1379118  2.67566299 3.24084091 3.03158236 4.81146288 2.07819152\n",
      " 3.20134544 3.14676642 2.891927   2.56310749 3.48119593 2.39844394\n",
      " 2.63187957 2.67448545 3.03222847 2.70632267 1.99006546 3.49353623\n",
      " 3.04232883 3.29635191 2.36993957 3.16185904 2.27677631 2.82254124\n",
      " 2.20630169 2.59319901 2.67748404 2.91700339 2.24816203 2.80843306\n",
      " 2.31854796 3.10458446 2.90062547 3.42549944 2.59460092 2.76266098\n",
      " 3.44145131 5.10352039 3.71844745 2.58158588 4.41579628 4.30063343\n",
      " 2.41877747 9.83578682 1.70273471 3.21132398 2.53532553 2.5781486\n",
      " 2.46680641 3.40271568 3.51322269 1.85835195 2.16994452 3.4367249\n",
      " 2.66671133 3.78381944 1.86545825 2.6363101  2.23881006 2.60445046\n",
      " 2.3153131  3.9586885  1.97720766 2.92848134 4.23921537 2.4220736\n",
      " 3.07181358 2.05063009 3.25510907 5.30481339 3.28979683 2.86642456\n",
      " 5.98842621 3.89784551 2.63506341 2.80877042 4.85103703 3.21556592\n",
      " 2.73969102 3.19208503 2.37791157 5.50142145 3.26508403 1.6884073\n",
      " 3.16492748 3.06127453 4.09486389 2.48516846 3.42474508 3.35802102\n",
      " 2.59232926 3.09407544 3.05311704 3.50842977 2.54193497 4.54127502\n",
      " 2.50046778 4.93945742 2.03032804 3.19902825 2.59813714 2.74676776\n",
      " 2.79438901 2.50372195 2.31108356 3.0204854  3.8644166  3.75168896\n",
      " 1.59780097 2.4714632  3.34121895 2.09622884 1.77205825 3.32289267\n",
      " 4.04224443 3.15782642 3.14744544 1.71179259 4.71983099 4.19642735\n",
      " 3.95120406 6.93611908 2.16185737 2.58254051 3.22352552 7.07105732\n",
      " 2.51504588 3.17618465 2.33557057 1.97656739 3.58076596 3.38737559\n",
      " 3.50447249 2.47448254 2.69471717 2.27411056 2.75239134 2.32393909\n",
      " 2.46675038 3.84944701 3.23482656 2.88242197 2.65769672 3.56763029\n",
      " 2.81257343 2.31412411 2.10271502 3.50552917 3.46061039 2.67634678\n",
      " 3.31072927 2.99469733 2.56716919 2.27618647 2.09531522 5.22388601\n",
      " 3.53636098 1.77726352 2.55406785 1.98141646 3.6017065  2.33796668\n",
      " 3.06197667 2.96906924 2.40433478 4.21180344 2.22933841 2.50802445\n",
      " 2.28835535 4.65532112 4.60035276 2.2472806  2.90183258 3.55793047\n",
      " 3.27374792 2.998698   4.46206141 1.70419979 2.60128188 3.27395558\n",
      " 4.76704359 2.13896441 3.78829908 3.35013866 5.3022418  2.12706733\n",
      " 3.0603435  2.23911476 1.91752756 3.03363657 3.35425472 2.80825686\n",
      " 3.16607475 2.70950651 3.93540096 2.40922761 3.42713261 2.56717896\n",
      " 2.06375265 2.48557711 2.38512564 3.58344364 2.14325309 2.948174\n",
      " 3.11011553 2.84363699 1.92825663 3.05943942 3.40343761 2.13098145\n",
      " 1.86476278 3.71365499 2.14687538 4.26105928 2.35759211 4.47673988\n",
      " 3.30206561 3.23160052 2.8298645  2.14590549 3.59042811 3.66722655\n",
      " 2.70942807 3.28591347 3.8673501  2.5372541  2.52051973 3.68456149\n",
      " 2.58112121 3.80140376 3.35471439 2.09004235 2.35959411 3.88532281\n",
      " 2.57360387 3.55721521 3.05799747 2.88198709 2.99760795 2.97347355\n",
      " 3.67960095 3.66318607 2.84871149 2.04892921 3.2376318  2.22356081\n",
      " 3.49609303 3.50349283 2.02363586 2.28379941 4.43146992 1.89097869\n",
      " 3.05213094 3.13959289 4.06059933 2.80627847 2.75657105 1.97789264\n",
      " 4.93774796 3.00875211 3.25648546 1.76705945 2.24729156 2.29994416\n",
      " 3.27433515 3.11076403 2.87211442 3.01347399 2.92622089 3.27388811\n",
      " 3.25188279 2.42148089 1.72956359 3.33385444 2.70464921 3.59738445\n",
      " 1.84857345 3.1013813  2.60018826 2.96181107 1.98785043 2.42883897\n",
      " 2.18544292 2.83362103 3.70464444 4.75288296 3.97026181 2.02984476\n",
      " 2.50680542 3.4375999  2.98296356 3.74334621 3.34927607 1.96831048\n",
      " 3.12725401 3.20379043 4.14874744 2.94297552 3.82053804 2.29638004\n",
      " 2.2786727  3.64270473 3.1982162  2.1507113  3.64757466 3.75846386\n",
      " 3.31795454 3.60219646 4.5239563  2.59624887 2.68195295 2.23886347\n",
      " 3.29864597 3.51136756 3.05666828 2.02359915 1.89369345 6.07126236\n",
      " 3.36757374 5.08540297 2.76988602 2.66215634 2.39411855 2.17054653\n",
      " 2.71793413 2.90561891 2.24142027 2.66219544 2.22605658 2.83285642\n",
      " 2.2529459  2.9602983  2.11588907 4.4359417  3.3588562  2.93401694\n",
      " 2.33017612 2.33405042 2.91639805 2.36561131 2.70964789 2.4599092\n",
      " 3.15287709 2.11298347 2.11331558 1.87071955 1.74854684 2.67545462\n",
      " 2.43207002 2.45270777 3.10040498 3.51028252 3.15219927 2.82934952\n",
      " 3.34323049 3.64249802 3.5301609  3.54405975 2.36625361 5.36612511\n",
      " 3.0518496  3.28739786 2.58205867 3.99177432 2.18304396 2.99377275\n",
      " 3.12904644 2.20314145 3.337466   2.72230697 3.06578493 2.53854609\n",
      " 3.51642346 3.15729403 2.38073158 4.77451658 2.99315596 3.48528242\n",
      " 3.48950148 3.01251888 4.44606686 2.508389   3.00707722 2.59484911\n",
      " 3.15281653 2.32273698 2.94994736 3.25512958 5.07732964 1.9835279\n",
      " 3.37919307 1.79030001 2.00063109 3.71038508 2.31714368 2.73515606\n",
      " 2.5772512  2.34454012 2.76813745 2.49905396 2.37142181 2.23168254\n",
      " 4.0586791  2.08436966 2.93005729 4.022614   3.83622098 2.28679299\n",
      " 2.51032948 2.33117533 4.1942997  2.41797757 3.67874956 3.5717175\n",
      " 3.4065249  2.18865871 2.832165   4.1718998  2.8155365  6.74388742\n",
      " 2.30286908 3.74625993 2.24733329 2.57492042 3.17309833 2.32003999\n",
      " 3.1918366  3.65132284 2.41384864 2.01229405 3.47565651 2.32067347\n",
      " 2.15853167 2.14047384 2.73734498 2.57845807 3.57503462 2.47339034\n",
      " 3.15446019 2.8387239  2.03517056 3.20370483 2.27193022 1.79535854\n",
      " 2.91659403 3.96467996 2.83953857 2.11292887 2.57623959 3.37913251\n",
      " 3.25662804 2.68945146 2.23375559 3.02324939 2.84451604 3.85554886\n",
      " 2.44364309 3.00794482 3.72574806 3.6832335  1.82212472 2.73463535\n",
      " 3.53415227 3.844594   2.44315434 4.00037813 2.53277063 2.78471804\n",
      " 3.33731079 5.71524096 3.45421743 2.40804195 4.56841183 2.12469053\n",
      " 2.648206   3.74760509 5.99804306 3.13732409 2.20099401 2.88277698\n",
      " 2.04375911 2.85237098 3.2326355  2.26892161 2.37053037 3.89976406\n",
      " 3.15073586 5.74733114 3.14253807 2.51906252 2.66381311 2.75076294\n",
      " 2.82490349 2.41428709 3.53650904 3.85587811 2.05800176 3.02404141\n",
      " 3.13564086 1.77198768 2.9598999  3.09987807 2.10668254 2.00393462\n",
      " 2.27708173 2.67723489 3.04227257 2.65931535 3.26681757 2.45057559\n",
      " 2.38417482 1.86754107 2.45045209 2.87369943 3.75073814 2.89671326\n",
      " 3.15286398 2.48448014 4.871521   2.4389286  3.59574294 2.72951388\n",
      " 2.97419548 2.37565947 2.71121502 2.9762311  4.24937773 3.80355239\n",
      " 2.97765756 3.01415443 3.50279498 3.45596004 2.76264024 2.05099416\n",
      " 2.45541239 3.15056634 2.28123331 2.2367754  2.37729144 3.28051424\n",
      " 3.52016354 2.06653357 3.0429368  2.27925467 2.32951641 4.8912096\n",
      " 3.04563355 2.23769641 2.59009361 2.65731931 2.44035935 2.21020794\n",
      " 2.96275377 5.40337276 1.97203255 2.03573012 2.93261099 5.08627033\n",
      " 3.67579746 2.22012424 3.10840583 2.1644423  2.30564427 3.95073175\n",
      " 1.99480271 2.4777441  2.91260052 1.94460809 5.49008799 2.6635201\n",
      " 5.26907015 2.48244405 2.9066093  1.95850253 2.00231314 2.70860147\n",
      " 1.87716901 1.81707561 2.7242589  2.64370346 2.16112232 4.73692417\n",
      " 1.93851125 2.5233736  2.37053657 3.08620811 3.32982612 2.7362926\n",
      " 2.60583091 3.03605986 2.82521319 3.4654665  2.05727029 5.23716021\n",
      " 2.97803092 3.33844423 2.68973398 5.93494844 2.85595727 3.28596854\n",
      " 2.45081735 3.26768708 1.85099983 3.13065481 1.89526498 1.80956817\n",
      " 2.1291635  3.45761442 2.66119289 2.87945366 2.12504601 3.98279142\n",
      " 4.51201057 2.58127761 3.26165915 3.29842782 3.1019311  2.19226241\n",
      " 2.20716739 2.97680855 2.50139689 2.77727818 3.42932606 3.83450556\n",
      " 2.31167006 3.43117809 2.8853271  2.29910159 2.05220985 2.31759548\n",
      " 3.46510291 3.71288562 2.27760482 1.92155313 3.16869712 2.93545604\n",
      " 2.78272796 3.54711843 3.19494104 5.31437969 3.75061679 3.32583594\n",
      " 4.94904757 3.95864391 7.6467247  2.74325466 2.03873038 2.52405667\n",
      " 2.23301339 3.53641105 2.21769094 1.7577523  2.5126667  2.74741912\n",
      " 4.15441847 2.75209999 2.81816959 2.79029226 6.77802944 2.46576595\n",
      " 4.01422548 3.25133061 3.13406301 4.5711751  3.60674739 5.66033506\n",
      " 1.79435098 2.31092405 2.02921057 3.98145676 2.74986553 2.88598561\n",
      " 3.30505943 3.19674349 2.47821474 2.79821396 2.59206557 4.85511303\n",
      " 1.76512909 3.53053904 3.58876538 3.22555494 3.12825465 2.39971781\n",
      " 3.13045907 2.22138429 2.7744081  2.91649055 2.61564898 4.88624287\n",
      " 2.5896666  2.47105432 2.91258407 3.31312704 3.55147004 3.22955155\n",
      " 3.29309106 2.97437501 2.73582792 4.08584976 4.826056   2.94851184\n",
      " 2.01294756 3.32641959 2.00627208 3.0236764  3.7003324  3.78039527\n",
      " 3.98360324 3.51067281 2.98633099 2.76151633 2.8898809  2.49846959\n",
      " 2.52625108 1.9448626  3.44505334 1.7170248  3.4096806  4.08055544\n",
      " 4.17182541 2.8485775  2.08954906 8.73998737 3.49793148 1.80518579\n",
      " 2.20340419 3.44153047 2.66641688 3.46493983]\n",
      "tensor([[1.0733e+01, 9.0606e+00, 1.0789e+01,  ..., 4.0824e-01, 2.0732e+01,\n",
      "         2.0770e-02],\n",
      "        [1.1373e+01, 8.8295e+00, 1.1733e+01,  ..., 3.4803e-01, 1.8660e+01,\n",
      "         2.5841e-02],\n",
      "        [8.7161e+00, 6.5597e+00, 8.8301e+00,  ..., 3.3020e-01, 2.3394e+01,\n",
      "         2.5138e-02],\n",
      "        ...,\n",
      "        [9.0981e+00, 7.2574e+00, 9.3817e+00,  ..., 3.4445e-01, 2.2986e+01,\n",
      "         2.8892e-02],\n",
      "        [1.0530e+01, 7.5457e+00, 1.0631e+01,  ..., 3.1255e-01, 1.6149e+01,\n",
      "         2.5381e-02],\n",
      "        [1.0632e+01, 9.0410e+00, 1.0909e+01,  ..., 3.2453e-01, 2.4333e+01,\n",
      "         2.6141e-02]])\n",
      "tensor([2.4865, 2.5520, 3.6143, 3.1897, 2.2961, 3.6562, 2.7853, 3.4717, 3.6417,\n",
      "        2.6149, 3.7392, 5.0042, 2.7677, 2.2322, 2.5293, 3.9768, 2.4885, 2.0818,\n",
      "        2.3465, 3.2801, 3.1915, 2.9440, 2.1376, 2.5159, 2.9400, 2.3412, 3.7183,\n",
      "        3.0956, 2.1113, 2.9198, 5.5407, 1.8786, 3.4798, 2.7077, 3.0444, 3.4486,\n",
      "        5.8695, 3.0867, 3.0047, 2.9071, 2.7456, 3.3061, 5.3298, 2.7650, 2.8051,\n",
      "        3.7939, 2.6970, 5.6215, 3.5220, 2.2306, 2.6865, 2.6231, 2.4862, 2.1982,\n",
      "        2.5321, 3.1427, 2.5576, 4.3188, 2.4525, 2.2586, 3.4166, 1.9144, 3.0731,\n",
      "        2.4328, 4.2993, 2.9162, 3.6151, 5.0253, 3.6280, 3.1808, 3.2727, 2.5538,\n",
      "        2.4609, 2.5925, 2.2689, 2.0801, 2.5698, 2.2969, 1.6850, 3.1228, 3.2079,\n",
      "        2.4072, 1.9482, 4.0240, 2.9687, 2.7792, 2.7209, 2.1273, 3.1008, 3.6436,\n",
      "        2.8146, 2.2998, 2.4742, 4.5223, 3.2290, 3.0915, 2.9330, 3.5621, 3.1488,\n",
      "        2.1640, 3.0302, 2.1397, 2.5755, 3.7748, 2.5981, 2.9521, 3.2360, 3.4328,\n",
      "        4.0461, 3.1529, 2.5281, 1.9350, 3.2216, 2.4416, 2.3914, 2.2413, 2.7524,\n",
      "        1.8568, 2.9792, 2.5980, 2.2582, 3.4359, 2.9161, 8.0696, 6.0517, 2.9635,\n",
      "        2.9132, 2.0005, 8.0106, 1.8945, 2.1407, 3.9288, 2.6315, 3.6198, 1.7358,\n",
      "        2.9333, 2.0636, 2.7914, 3.7385, 2.9266, 2.3634, 2.7677, 2.9640, 4.1420,\n",
      "        5.2109, 4.1412, 4.8579, 3.2489, 2.5982, 2.3550, 2.7633, 2.0152, 2.6349,\n",
      "        2.1143, 2.4037, 2.6333, 2.4066, 2.8714, 2.0332, 3.0081, 2.5983, 2.9736,\n",
      "        3.4292, 1.9538, 3.8966, 2.3920, 2.5553, 2.2346, 2.7761, 5.4875, 2.1299,\n",
      "        3.1028, 3.0740, 2.8652, 2.1381, 2.8443, 4.7418, 3.2753, 5.6855, 2.6075,\n",
      "        2.7373, 3.5080, 2.6842, 2.6843, 3.5554, 2.8657, 2.8852, 2.8284, 2.4442,\n",
      "        3.1733, 3.4083, 2.5447, 2.8887, 3.1851, 2.5719, 3.0419, 3.7806, 2.4868,\n",
      "        3.0385, 3.5536, 2.3596, 3.9292, 2.9329, 2.4067, 2.7616, 3.7849, 3.5493,\n",
      "        5.9825, 2.0319, 3.0440, 2.2128, 3.0082, 3.5363, 4.1889, 2.0986, 2.2124,\n",
      "        2.8501, 3.2664, 2.4747, 3.8258, 5.3099, 3.9140, 2.1316, 2.5666, 3.2639,\n",
      "        2.1494, 3.4056, 3.2895, 2.2043, 3.4418, 4.2140, 2.8401, 2.9452, 1.8444,\n",
      "        2.3480, 1.8526, 2.5387, 3.8884, 1.6984, 2.9490, 4.2477, 5.0959, 3.4367,\n",
      "        4.9791, 3.5378, 2.5987, 2.5547, 1.9588, 2.0834, 2.9491, 4.0635, 2.3040,\n",
      "        2.8050, 1.9994, 1.6632, 2.9609, 5.0102, 1.9764, 3.5629, 2.4806, 4.9595,\n",
      "        3.7742, 3.9215, 3.0914, 2.6205, 2.9447, 2.1749, 4.2926, 3.1554, 4.4054,\n",
      "        5.1379, 2.6757, 3.2408, 3.0316, 4.8115, 2.0782, 3.2013, 3.1468, 2.8919,\n",
      "        2.5631, 3.4812, 2.3984, 2.6319, 2.6745, 3.0322, 2.7063, 1.9901, 3.4935,\n",
      "        3.0423, 3.2964, 2.3699, 3.1619, 2.2768, 2.8225, 2.2063, 2.5932, 2.6775,\n",
      "        2.9170, 2.2482, 2.8084, 2.3185, 3.1046, 2.9006, 3.4255, 2.5946, 2.7627,\n",
      "        3.4415, 5.1035, 3.7184, 2.5816, 4.4158, 4.3006, 2.4188, 9.8358, 1.7027,\n",
      "        3.2113, 2.5353, 2.5781, 2.4668, 3.4027, 3.5132, 1.8584, 2.1699, 3.4367,\n",
      "        2.6667, 3.7838, 1.8655, 2.6363, 2.2388, 2.6045, 2.3153, 3.9587, 1.9772,\n",
      "        2.9285, 4.2392, 2.4221, 3.0718, 2.0506, 3.2551, 5.3048, 3.2898, 2.8664,\n",
      "        5.9884, 3.8978, 2.6351, 2.8088, 4.8510, 3.2156, 2.7397, 3.1921, 2.3779,\n",
      "        5.5014, 3.2651, 1.6884, 3.1649, 3.0613, 4.0949, 2.4852, 3.4247, 3.3580,\n",
      "        2.5923, 3.0941, 3.0531, 3.5084, 2.5419, 4.5413, 2.5005, 4.9395, 2.0303,\n",
      "        3.1990, 2.5981, 2.7468, 2.7944, 2.5037, 2.3111, 3.0205, 3.8644, 3.7517,\n",
      "        1.5978, 2.4715, 3.3412, 2.0962, 1.7721, 3.3229, 4.0422, 3.1578, 3.1474,\n",
      "        1.7118, 4.7198, 4.1964, 3.9512, 6.9361, 2.1619, 2.5825, 3.2235, 7.0711,\n",
      "        2.5150, 3.1762, 2.3356, 1.9766, 3.5808, 3.3874, 3.5045, 2.4745, 2.6947,\n",
      "        2.2741, 2.7524, 2.3239, 2.4668, 3.8494, 3.2348, 2.8824, 2.6577, 3.5676,\n",
      "        2.8126, 2.3141, 2.1027, 3.5055, 3.4606, 2.6763, 3.3107, 2.9947, 2.5672,\n",
      "        2.2762, 2.0953, 5.2239, 3.5364, 1.7773, 2.5541, 1.9814, 3.6017, 2.3380,\n",
      "        3.0620, 2.9691, 2.4043, 4.2118, 2.2293, 2.5080, 2.2884, 4.6553, 4.6004,\n",
      "        2.2473, 2.9018, 3.5579, 3.2737, 2.9987, 4.4621, 1.7042, 2.6013, 3.2740,\n",
      "        4.7670, 2.1390, 3.7883, 3.3501, 5.3022, 2.1271, 3.0603, 2.2391, 1.9175,\n",
      "        3.0336, 3.3543, 2.8083, 3.1661, 2.7095, 3.9354, 2.4092, 3.4271, 2.5672,\n",
      "        2.0638, 2.4856, 2.3851, 3.5834, 2.1433, 2.9482, 3.1101, 2.8436, 1.9283,\n",
      "        3.0594, 3.4034, 2.1310, 1.8648, 3.7137, 2.1469, 4.2611, 2.3576, 4.4767,\n",
      "        3.3021, 3.2316, 2.8299, 2.1459, 3.5904, 3.6672, 2.7094, 3.2859, 3.8674,\n",
      "        2.5373, 2.5205, 3.6846, 2.5811, 3.8014, 3.3547, 2.0900, 2.3596, 3.8853,\n",
      "        2.5736, 3.5572, 3.0580, 2.8820, 2.9976, 2.9735, 3.6796, 3.6632, 2.8487,\n",
      "        2.0489, 3.2376, 2.2236, 3.4961, 3.5035, 2.0236, 2.2838, 4.4315, 1.8910,\n",
      "        3.0521, 3.1396, 4.0606, 2.8063, 2.7566, 1.9779, 4.9377, 3.0088, 3.2565,\n",
      "        1.7671, 2.2473, 2.2999, 3.2743, 3.1108, 2.8721, 3.0135, 2.9262, 3.2739,\n",
      "        3.2519, 2.4215, 1.7296, 3.3339, 2.7046, 3.5974, 1.8486, 3.1014, 2.6002,\n",
      "        2.9618, 1.9879, 2.4288, 2.1854, 2.8336, 3.7046, 4.7529, 3.9703, 2.0298,\n",
      "        2.5068, 3.4376, 2.9830, 3.7433, 3.3493, 1.9683, 3.1273, 3.2038, 4.1487,\n",
      "        2.9430, 3.8205, 2.2964, 2.2787, 3.6427, 3.1982, 2.1507, 3.6476, 3.7585,\n",
      "        3.3180, 3.6022, 4.5240, 2.5962, 2.6820, 2.2389, 3.2986, 3.5114, 3.0567,\n",
      "        2.0236, 1.8937, 6.0713, 3.3676, 5.0854, 2.7699, 2.6622, 2.3941, 2.1705,\n",
      "        2.7179, 2.9056, 2.2414, 2.6622, 2.2261, 2.8329, 2.2529, 2.9603, 2.1159,\n",
      "        4.4359, 3.3589, 2.9340, 2.3302, 2.3341, 2.9164, 2.3656, 2.7096, 2.4599,\n",
      "        3.1529, 2.1130, 2.1133, 1.8707, 1.7485, 2.6755, 2.4321, 2.4527, 3.1004,\n",
      "        3.5103, 3.1522, 2.8293, 3.3432, 3.6425, 3.5302, 3.5441, 2.3663, 5.3661,\n",
      "        3.0518, 3.2874, 2.5821, 3.9918, 2.1830, 2.9938, 3.1290, 2.2031, 3.3375,\n",
      "        2.7223, 3.0658, 2.5385, 3.5164, 3.1573, 2.3807, 4.7745, 2.9932, 3.4853,\n",
      "        3.4895, 3.0125, 4.4461, 2.5084, 3.0071, 2.5948, 3.1528, 2.3227, 2.9499,\n",
      "        3.2551, 5.0773, 1.9835, 3.3792, 1.7903, 2.0006, 3.7104, 2.3171, 2.7352,\n",
      "        2.5773, 2.3445, 2.7681, 2.4991, 2.3714, 2.2317, 4.0587, 2.0844, 2.9301,\n",
      "        4.0226, 3.8362, 2.2868, 2.5103, 2.3312, 4.1943, 2.4180, 3.6787, 3.5717,\n",
      "        3.4065, 2.1887, 2.8322, 4.1719, 2.8155, 6.7439, 2.3029, 3.7463, 2.2473,\n",
      "        2.5749, 3.1731, 2.3200, 3.1918, 3.6513, 2.4138, 2.0123, 3.4757, 2.3207,\n",
      "        2.1585, 2.1405, 2.7373, 2.5785, 3.5750, 2.4734, 3.1545, 2.8387, 2.0352,\n",
      "        3.2037, 2.2719, 1.7954, 2.9166, 3.9647, 2.8395, 2.1129, 2.5762, 3.3791,\n",
      "        3.2566, 2.6895, 2.2338, 3.0232, 2.8445, 3.8555, 2.4436, 3.0079, 3.7257,\n",
      "        3.6832, 1.8221, 2.7346, 3.5342, 3.8446, 2.4432, 4.0004, 2.5328, 2.7847,\n",
      "        3.3373, 5.7152, 3.4542, 2.4080, 4.5684, 2.1247, 2.6482, 3.7476, 5.9980,\n",
      "        3.1373, 2.2010, 2.8828, 2.0438, 2.8524, 3.2326, 2.2689, 2.3705, 3.8998,\n",
      "        3.1507, 5.7473, 3.1425, 2.5191, 2.6638, 2.7508, 2.8249, 2.4143, 3.5365,\n",
      "        3.8559, 2.0580, 3.0240, 3.1356, 1.7720, 2.9599, 3.0999, 2.1067, 2.0039,\n",
      "        2.2771, 2.6772, 3.0423, 2.6593, 3.2668, 2.4506, 2.3842, 1.8675, 2.4505,\n",
      "        2.8737, 3.7507, 2.8967, 3.1529, 2.4845, 4.8715, 2.4389, 3.5957, 2.7295,\n",
      "        2.9742, 2.3757, 2.7112, 2.9762, 4.2494, 3.8036, 2.9777, 3.0142, 3.5028,\n",
      "        3.4560, 2.7626, 2.0510, 2.4554, 3.1506, 2.2812, 2.2368, 2.3773, 3.2805,\n",
      "        3.5202, 2.0665, 3.0429, 2.2793, 2.3295, 4.8912, 3.0456, 2.2377, 2.5901,\n",
      "        2.6573, 2.4404, 2.2102, 2.9628, 5.4034, 1.9720, 2.0357, 2.9326, 5.0863,\n",
      "        3.6758, 2.2201, 3.1084, 2.1644, 2.3056, 3.9507, 1.9948, 2.4777, 2.9126,\n",
      "        1.9446, 5.4901, 2.6635, 5.2691, 2.4824, 2.9066, 1.9585, 2.0023, 2.7086,\n",
      "        1.8772, 1.8171, 2.7243, 2.6437, 2.1611, 4.7369, 1.9385, 2.5234, 2.3705,\n",
      "        3.0862, 3.3298, 2.7363, 2.6058, 3.0361, 2.8252, 3.4655, 2.0573, 5.2372,\n",
      "        2.9780, 3.3384, 2.6897, 5.9349, 2.8560, 3.2860, 2.4508, 3.2677, 1.8510,\n",
      "        3.1307, 1.8953, 1.8096, 2.1292, 3.4576, 2.6612, 2.8795, 2.1250, 3.9828,\n",
      "        4.5120, 2.5813, 3.2617, 3.2984, 3.1019, 2.1923, 2.2072, 2.9768, 2.5014,\n",
      "        2.7773, 3.4293, 3.8345, 2.3117, 3.4312, 2.8853, 2.2991, 2.0522, 2.3176,\n",
      "        3.4651, 3.7129, 2.2776, 1.9216, 3.1687, 2.9355, 2.7827, 3.5471, 3.1949,\n",
      "        5.3144, 3.7506, 3.3258, 4.9490, 3.9586, 7.6467, 2.7433, 2.0387, 2.5241,\n",
      "        2.2330, 3.5364, 2.2177, 1.7578, 2.5127, 2.7474, 4.1544, 2.7521, 2.8182,\n",
      "        2.7903, 6.7780, 2.4658, 4.0142, 3.2513, 3.1341, 4.5712, 3.6067, 5.6603,\n",
      "        1.7944, 2.3109, 2.0292, 3.9815, 2.7499, 2.8860, 3.3051, 3.1967, 2.4782,\n",
      "        2.7982, 2.5921, 4.8551, 1.7651, 3.5305, 3.5888, 3.2256, 3.1283, 2.3997,\n",
      "        3.1305, 2.2214, 2.7744, 2.9165, 2.6156, 4.8862, 2.5897, 2.4711, 2.9126,\n",
      "        3.3131, 3.5515, 3.2296, 3.2931, 2.9744, 2.7358, 4.0858, 4.8261, 2.9485,\n",
      "        2.0129, 3.3264, 2.0063, 3.0237, 3.7003, 3.7804, 3.9836, 3.5107, 2.9863,\n",
      "        2.7615, 2.8899, 2.4985, 2.5263, 1.9449, 3.4451, 1.7170, 3.4097, 4.0806,\n",
      "        4.1718, 2.8486, 2.0895, 8.7400, 3.4979, 1.8052, 2.2034, 3.4415, 2.6664,\n",
      "        3.4649])\n"
     ]
    }
   ],
   "source": [
    "max_len = 1000                 # hundo thousand data points, final speed is ~40 min for 1000 data.\n",
    "number_path = 500000\n",
    "batch = 1\n",
    "threads = 256\n",
    "seed  =1999 \n",
    "num = 0\n",
    "max_length = max_len\n",
    "N_PATHS = number_path\n",
    "N_STEPS = 365\n",
    "N_BATCH  =batch\n",
    "# we will not be calculating a starting date since the difference is negligible and I aint rigging up\n",
    "# a system to check if a certain day is a weekend or not\n",
    "MONTHS = cupy.asnumpy([0, 31,59,90,120,151,181,212,243, 273,304,334])\n",
    "        # SHOULD THIS BE NP ARRAY INSTEAD????\n",
    "snowball_path_holder =  np.zeros(N_BATCH*N_PATHS, dtype=(np.float32,N_STEPS+1))# extra 1 is no longer for storing payoff\n",
    "# self.snowball_path_holder = cupy.array(self.snowball_path_holder)\n",
    "# self.T  = np.float(365.0)         # nah id lose. \n",
    "output = cupy.zeros(N_BATCH*N_PATHS, dtype = cupy.float32)\n",
    "num_blocks  =(N_PATHS * N_BATCH -1) // threads +1\n",
    "num_threads = threads\n",
    "\n",
    "\n",
    "# Xs =  np.zeros(max_length, dtype=(np.float32,7))#\n",
    "# Ys =  np.zeros(max_length, dtype=(np.float32))#          storing final data\n",
    "\n",
    "Xss = []\n",
    "Yss = []\n",
    "# Xss = np.zeros((max_length, 7))\n",
    "# Yss = np.zeros((max_length, 1))\n",
    "\n",
    "\n",
    "# making sure self.snowball_path_holder is zeroed to avoid bug\n",
    "# self.snowball_path_holder.fill(0)\n",
    "s = time.time()\n",
    "\n",
    "for i in range(max_length):\n",
    "        randoms = cupy.random.normal(0,1, N_BATCH * N_PATHS * N_STEPS, dtype= cupy.float32)\n",
    "\n",
    "        Xpre = cupy.random.rand(N_BATCH, 7, dtype = cupy.float32)\n",
    "        #                        s_0,  Ki, Ko,  mu, sigma, pot, r\n",
    "        Xpre = Xpre * cupy.array([4,  -2,  1,  .01,  .15,  10, .01], dtype=cupy.float32)\n",
    "        X = Xpre +    cupy.array([8,   0,  0,  .02, .275,  15, .02], dtype=cupy.float32)\n",
    "        # Ki and Ko will be set down here instead of the previous line to make them relative to s_0.\n",
    "        X[:, 1] = X[:,0] -1         # overriding Ki and Ko \n",
    "        X[:, 2] = X[:,0] -.2        \n",
    "        # print(X)\n",
    "        X[:, 1] += Xpre[:,1]        # adding back the offset in Xpre after it gets overrided\n",
    "        X[:, 2] += Xpre[:,2] \n",
    "\n",
    "        snowball_path_holder.fill(0)\n",
    "                                        # d_s, s_0, Ki, Ko, mu, sigma, pot,r,\n",
    "                                        # d_normals, snowball_path_holder, MONTHS,\n",
    "                                        # N_STEPS, N_PATHS, N_BATCH):\n",
    "        monte_carlo_andtheholygrail_gpu[(num_blocks,), (num_threads,)](\n",
    "                                        output, X[:, 0], X[:, 1], X[:, 2], X[:, 3], \n",
    "                                        X[:, 4], X[:, 5], X[:, 6],\n",
    "                                        randoms, snowball_path_holder, MONTHS,\n",
    "                                        N_STEPS, N_PATHS, N_BATCH)\n",
    "        # o = output.reshape(N_BATCH, N_PATHS)\n",
    "        # Y  =o.mean(axis =1)         # getting the average of each batch\n",
    "        Y = output.mean()\n",
    "        X = X.mean(axis=0)\n",
    "        Xss.append(X.tolist())\n",
    "        Yss.append(Y.tolist())\n",
    "        # Xss.append(X)\n",
    "        # Yss.append(Y)\n",
    "        \n",
    "        # torch.save(from_dlpack(X.toDlpack()), f\"snow_data/snowX_data/tensor{i}.pt\")\n",
    "        # torch.save(from_dlpack(Y.toDlpack()), f\"snow_data/snowY_data/tensor{i}.pt\")\n",
    "\n",
    "        num+=1\n",
    "        # print((from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))) \n",
    "\n",
    "v = output.mean()\n",
    "cuda.synchronize()\n",
    "e = time.time()\n",
    "print('time', e-s, 'v', v, 'avg time', (e-s)/500000)\n",
    "\n",
    "# Xs = np.array(Xss)\n",
    "# Ys = np.array(Yss)\n",
    "Xss = np.array(Xss)\n",
    "Yss = np.array(Yss)\n",
    "# print(Xss)\n",
    "print(Yss)\n",
    "\n",
    "tensorX = torch.Tensor(Xss)\n",
    "tensorY = torch.Tensor(Yss)\n",
    "print(tensorX)\n",
    "print(tensorY)\n",
    "torch.save(tensorX, \"snow_data_tensor_test/tsnowX.pt\")\n",
    "torch.save(tensorY, \"snow_data_tensor_test/tsnowY.pt\")\n",
    "# np.save(\"snow_npy_data/npyX\", Xss)\n",
    "# np.save(\"snow_npy_data/npyY\", Yss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data to confirm its existence! turning it into gpu monsters to prepare for throwing it into model to train! can prolly be done be done before saving, thouhg :/ <br>\n",
    "oh well, it takes like no time to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms, utils, datasets\n",
    "import os\n",
    "\n",
    "path = \"snow_data_tensor\"\n",
    "\n",
    "def npy_loader(path):\n",
    "    sample = torch.load(path)\n",
    "    return sample\n",
    "\n",
    "# dataset = datasets.DatasetFolder(\n",
    "#     root=path,\n",
    "#     loader=npy_loader,\n",
    "#     extensions=['.pt']\n",
    "#     )\n",
    "# tensor_x = torch.Tensor(Xss)\n",
    "# tensor_y = torch.Tensor(Yss)\n",
    "tensor_x = torch.load(\"snow_data_tensor/tsnowX.pt\")\n",
    "tensor_y = torch.load(\"snow_data_tensor/tsnowY.pt\")\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = tensor_x.float().to(device)\n",
    "values = tensor_y.float().to(device)\n",
    "\n",
    "# tdataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "tdataset = TensorDataset(inputs,values) # create your datset\n",
    "\n",
    "# print(tensor_x)\n",
    "\n",
    "# print(tdataset)\n",
    "\n",
    "# for i in tdataset:\n",
    "# #     # print(i, \"\\n\")\n",
    "#     print(i)     # printing the Ys\n",
    "# print(len(dataset)/2)\n",
    "# print(dataset)\n",
    "# inputs = tensor_x.float().to(device)\n",
    "# # values = tensor_y.float().to(device)\n",
    "# print(f\"Input device is : cuda:{tensor_x.get_device()}\")\n",
    "# print(f\"Target value device is : cuda:{tensor_y.get_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some reason, the guy who made the nvidia notebook decided it was a good idea to make the dataset generate itself while training, making the training process significantly slower than if the data was already prepared already. This works, but is not that good. I would not reccomend, 2/5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SnowballDataSet(object):\n",
    "\n",
    "    def __init__(self, max_len = 10, number_path = 1000, batch = 2, threads = 512, seed  =1999 ):\n",
    "        self.num = 0\n",
    "        self.max_length = max_len\n",
    "        self.N_PATHS = number_path\n",
    "        self.N_STEPS = 365\n",
    "        self.N_BATCH  =batch\n",
    "        # we will not be calculating a starting date since the difference is negligible and I aint rigging up\n",
    "        # a system to check if a certain day is a weekend or not\n",
    "        self.MONTHS = cupy.asnumpy([0, 31,59,90,120,151,181,212,243, 273,304,334])\n",
    "                # SHOULD THIS BE NP ARRAY INSTEAD????\n",
    "        self.snowball_path_holder =  np.zeros(self.N_BATCH*self.N_PATHS, dtype=(np.float32,self.N_STEPS+1))# extra 1 is no longer for storing payoff\n",
    "        # self.snowball_path_holder = cupy.array(self.snowball_path_holder)\n",
    "        # self.T  = np.float(365.0)         # nah id lose. \n",
    "        self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype = cupy.float32)\n",
    "        self.num_blocks  =(self.N_PATHS * self.N_BATCH -1) // threads +1\n",
    "        self.num_threads = threads\n",
    "\n",
    "        #  temp_months, snowball_path_holder both added now\n",
    "        cupy.random.seed(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.num = 0\n",
    "        return self\n",
    "\n",
    "    #   next basically takes the place of the cell running the mc. As such need to generate\n",
    "     # (d_s, s_0, Ki, Ko, mu, sigma,  pot,r, d_normals, snowball_path_holder, MONTHS, N_STEPS, N_PATHS, N_BATCH\n",
    "     # note that all but s_0, Ki, Ko, mu, sigma,  pot,r, d_normals have been generated in init due to their nonrandom nature\n",
    "    def __next__(self):\n",
    "        if self.num > self.max_length: \n",
    "            raise StopIteration      # nvidia notebook uses raise StopIteration here but p sure its deprecated???\n",
    "                                      # is used because return returns an extra None\n",
    "        # generating the variables\n",
    "        # d_normals\n",
    "        randoms = cupy.random.normal(0,1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype= cupy.float32)\n",
    "\n",
    "        Xpre = cupy.random.rand(self.N_BATCH, 7, dtype = cupy.float32)\n",
    "        #                        s_0,  Ki, Ko,  mu, sigma, pot, r\n",
    "        Xpre = Xpre * cupy.array([4,  -2,  1,  .01,  .15,  10, .01], dtype=cupy.float32)\n",
    "        X = Xpre +    cupy.array([8,   0,  0,  .02, .275,  15, .02], dtype=cupy.float32)\n",
    "        \n",
    "        # Ki and Ko will be set down here instead of the previous line to make them relative to s_0.\n",
    "        X[:, 1] = X[:,0] -1         # overriding Ki and Ko \n",
    "        X[:, 2] = X[:,0] -.2        \n",
    "        # print(X)\n",
    "        X[:, 1] += Xpre[:,1]        # adding back the offset in Xpre after it gets overrided\n",
    "        X[:, 2] += Xpre[:,2] \n",
    "\n",
    "        # making sure self.snowball_path_holder is zeroed to avoid bug\n",
    "        self.snowball_path_holder.fill(0)\n",
    "\n",
    "                                        # d_s, s_0, Ki, Ko, mu, sigma, pot,r,\n",
    "                                        # d_normals, snowball_path_holder, MONTHS,\n",
    "                                        # N_STEPS, N_PATHS, N_BATCH):\n",
    "        monte_carlo_andtheholygrail_gpu[(self.num_blocks,), (self.num_threads,)](\n",
    "                                        self.output, X[:, 0], X[:, 1], X[:, 2], X[:, 3], \n",
    "                                        X[:, 4], X[:, 5], X[:, 6],\n",
    "                                        randoms, self.snowball_path_holder, self.MONTHS,\n",
    "                                        self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
    "        \n",
    "        o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
    "        Y  =o.mean(axis =1)         # getting the average of each batch\n",
    "        self.num+=1\n",
    "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now a small test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xyzqadmin\\anaconda3\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:886: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.6349,  7.2834,  9.7545,  0.0287,  0.3059, 24.2591,  0.0276]],\n",
      "       device='cuda:0') tensor([3.9449], device='cuda:0')\n",
      "tensor([[ 8.6179,  6.7289,  8.7578,  0.0208,  0.2911, 19.6906,  0.0254]],\n",
      "       device='cuda:0') tensor([3.1118], device='cuda:0')\n",
      "tensor([[ 8.5157,  5.8327,  8.6394,  0.0262,  0.2960, 21.1468,  0.0275]],\n",
      "       device='cuda:0') tensor([4.7028], device='cuda:0')\n",
      "tensor([[ 8.8283,  7.1150,  8.9113,  0.0208,  0.3009, 18.3316,  0.0227]],\n",
      "       device='cuda:0') tensor([2.5807], device='cuda:0')\n",
      "tensor([[11.8870,  9.9710, 11.7828,  0.0285,  0.3634, 23.8967,  0.0244]],\n",
      "       device='cuda:0') tensor([2.9223], device='cuda:0')\n",
      "tensor([[11.4234,  9.4665, 11.8392,  0.0211,  0.3384, 19.8409,  0.0246]],\n",
      "       device='cuda:0') tensor([2.6680], device='cuda:0')\n",
      "tensor([[11.0768,  8.6847, 11.8625,  0.0211,  0.3254, 20.9236,  0.0287]],\n",
      "       device='cuda:0') tensor([3.6412], device='cuda:0')\n",
      "tensor([[11.1044,  9.8112, 11.2277,  0.0231,  0.3005, 19.0593,  0.0228]],\n",
      "       device='cuda:0') tensor([2.5344], device='cuda:0')\n",
      "tensor([[ 8.2412,  6.8798,  8.8896,  0.0224,  0.3100, 19.8843,  0.0288]],\n",
      "       device='cuda:0') tensor([3.4307], device='cuda:0')\n",
      "tensor([[11.8692,  9.9715, 11.9518,  0.0266,  0.3321, 16.4964,  0.0255]],\n",
      "       device='cuda:0') tensor([1.9685], device='cuda:0')\n",
      "tensor([[ 8.5575,  6.9491,  8.6427,  0.0299,  0.3498, 18.2971,  0.0205]],\n",
      "       device='cuda:0') tensor([2.4426], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ds = SnowballDataSet(10, number_path=500000, batch=16, seed=15)\n",
    "ds = SnowballDataSet(10, number_path=500000, batch=1, seed=15)\n",
    "for i in ds:\n",
    "    # print(i, \"\\n\")\n",
    "    print(i[0],i[1])     # printing the Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "\n",
    "Erm pretty default model. Just making it have functionality. normalizing it accoriding to the average value of all of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting snow_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile snow_model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden=1024):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, hidden)\n",
    "        self.fc4 = nn.Linear(hidden, hidden)\n",
    "        self.fc5 = nn.Linear(hidden, hidden)\n",
    "        self.fc6 = nn.Linear(hidden, hidden)\n",
    "        self.fc7 = nn.Linear(hidden, 1)\n",
    "        self.register_buffer('norm',\n",
    "                             torch.tensor([10.0,\n",
    "                                           8.5,\n",
    "                                           10.4,\n",
    "                                           0.025,\n",
    "                                           0.35,\n",
    "                                           0.20,\n",
    "                                           0.025]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # normalize the parameter to range [0-1] \n",
    "        x = x / self.norm\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.elu(self.fc3(x))\n",
    "        x = F.elu(self.fc4(x))\n",
    "        x = F.elu(self.fc5(x))\n",
    "        x = F.elu(self.fc6(x))\n",
    "        return self.fc7(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "YAP CENTRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda:0\n",
      "loss 0.764177680015564 average time 0.00397009100066498\n",
      "loss 1.7381147146224976 average time 0.0028936060023261236\n",
      "loss 0.827140748500824 average time 0.002560490669760232\n",
      "loss 1.6986442804336548 average time 0.002378168251598254\n",
      "loss 0.6409973502159119 average time 0.0022879632003605364\n",
      "loss 0.17690925300121307 average time 0.002230839166441001\n",
      "loss 0.6499443054199219 average time 0.0021892805705179593\n",
      "loss 0.3218832015991211 average time 0.0021643407492956614\n",
      "loss 0.6044935584068298 average time 0.002131932221430664\n",
      "loss 0.24408355355262756 average time 0.0021044413989875464\n",
      "loss 0.6188947558403015 average time 0.0018604420067276806\n",
      "loss 2.731745958328247 average time 0.001830730004585348\n",
      "loss 0.8421614766120911 average time 0.0018336793365112194\n",
      "loss 1.5340837240219116 average time 0.0018397652538260445\n",
      "loss 0.5131850838661194 average time 0.0018637948015239089\n",
      "loss 0.18646623194217682 average time 0.0018808750004973263\n",
      "loss 0.6652289032936096 average time 0.0018745725711674563\n",
      "loss 0.2536561191082001 average time 0.0018745201245474164\n",
      "loss 0.591758668422699 average time 0.0018710501105265898\n",
      "loss 0.23358364403247833 average time 0.0018662223987048491\n",
      "loss 0.611321747303009 average time 0.0018225719965994358\n",
      "loss 2.0597548484802246 average time 0.0018528144975425676\n",
      "loss 0.7846523523330688 average time 0.001869698001925523\n",
      "loss 1.4634714126586914 average time 0.0018775652514887043\n",
      "loss 0.41290268301963806 average time 0.0018724464010447263\n",
      "loss 0.18831829726696014 average time 0.0018610033342459549\n",
      "loss 0.6676002740859985 average time 0.0018525114301259497\n",
      "loss 0.22579577565193176 average time 0.0018566643759550061\n",
      "loss 0.5827178359031677 average time 0.0018606376671232282\n",
      "loss 0.22682590782642365 average time 0.001868479299475439\n",
      "loss 0.6150407195091248 average time 0.0019581569940783083\n",
      "loss 1.8399670124053955 average time 0.0018973144958727062\n",
      "loss 0.7682853937149048 average time 0.0018719026652009537\n",
      "loss 1.3474149703979492 average time 0.0018564417480956762\n",
      "loss 0.3678663969039917 average time 0.0018492385980207473\n",
      "loss 0.1847381293773651 average time 0.0018418153309418509\n",
      "loss 0.6560854911804199 average time 0.0018491201402087297\n",
      "loss 0.21461878716945648 average time 0.0018610824979259633\n",
      "loss 0.5691298246383667 average time 0.001870729109375841\n",
      "loss 0.22021709382534027 average time 0.0018830330985365436\n",
      "loss 0.6342959403991699 average time 0.001797563008731231\n",
      "loss 2.036663055419922 average time 0.0018184665055014193\n",
      "loss 0.7701675295829773 average time 0.0018507720052730293\n",
      "loss 1.3564231395721436 average time 0.0018358077525044791\n",
      "loss 0.33734723925590515 average time 0.001849160400684923\n",
      "loss 0.19596680998802185 average time 0.0018750460013203943\n",
      "loss 0.6399122476577759 average time 0.0018852215734243925\n",
      "loss 0.2007235735654831 average time 0.001880032875778852\n",
      "loss 0.5536072254180908 average time 0.0018814608898376011\n",
      "loss 0.2121330052614212 average time 0.0018795123003656044\n",
      "loss 0.6020950078964233 average time 0.0018527879973407836\n",
      "loss 1.825391411781311 average time 0.0018364009953802452\n",
      "loss 0.7601604461669922 average time 0.0018482903305751582\n",
      "loss 1.3317279815673828 average time 0.0018543312480323948\n",
      "loss 0.3218875825405121 average time 0.0018597169981803745\n",
      "loss 0.18579696118831635 average time 0.0018570934967525924\n",
      "loss 0.6221227645874023 average time 0.001852103997953236\n",
      "loss 0.1960989534854889 average time 0.001852720498573035\n",
      "loss 0.5297936797142029 average time 0.0018506532197352498\n",
      "loss 0.2037583440542221 average time 0.001854520597611554\n",
      "loss 0.5780033469200134 average time 0.0019273710018023848\n",
      "loss 1.4487757682800293 average time 0.0019074540032306686\n",
      "loss 0.7480113506317139 average time 0.0019066216676340748\n",
      "loss 1.2940678596496582 average time 0.001898670002410654\n",
      "loss 0.31714949011802673 average time 0.0018942700026091188\n",
      "loss 0.17149415612220764 average time 0.0018792390031740068\n",
      "loss 0.6047143340110779 average time 0.0018691568595490286\n",
      "loss 0.19527433812618256 average time 0.0018649291260226164\n",
      "loss 0.4980772137641907 average time 0.0018668257794342935\n",
      "loss 0.19562987983226776 average time 0.0018697348011191934\n",
      "loss 0.5669381022453308 average time 0.0019071159919258208\n",
      "loss 1.3555469512939453 average time 0.0018561309954384343\n",
      "loss 0.7133007645606995 average time 0.001866687663908427\n",
      "loss 1.2435455322265625 average time 0.0018658687503193506\n",
      "loss 0.32790300250053406 average time 0.0018677820009179414\n",
      "loss 0.16006602346897125 average time 0.0018592030019499361\n",
      "loss 0.5868279337882996 average time 0.0018682137158300195\n",
      "loss 0.19810281693935394 average time 0.0018711968764546327\n",
      "loss 0.45683884620666504 average time 0.001863808889562885\n",
      "loss 0.18669648468494415 average time 0.0018614662013715132\n",
      "loss 0.5411767959594727 average time 0.0018364119972102343\n",
      "loss 1.2973836660385132 average time 0.0018211400014115497\n",
      "loss 0.6666902899742126 average time 0.0018364826682955027\n",
      "loss 1.1747627258300781 average time 0.001829843250452541\n",
      "loss 0.35310298204421997 average time 0.0018483132000546903\n",
      "loss 0.1478310078382492 average time 0.001884102500237835\n",
      "loss 0.5611554384231567 average time 0.001904662429421608\n",
      "loss 0.20793049037456512 average time 0.0019159257518185768\n",
      "loss 0.40712371468544006 average time 0.00192756344538389\n",
      "loss 0.17656873166561127 average time 0.0019437037005554884\n",
      "loss 0.5383456945419312 average time 0.0021093290019780398\n",
      "loss 1.2283064126968384 average time 0.002132684999378398\n",
      "loss 0.6095011234283447 average time 0.0021107533314110092\n",
      "loss 1.0981000661849976 average time 0.0020942149992333723\n",
      "loss 0.37842437624931335 average time 0.0020523997999262063\n",
      "loss 0.13108931481838226 average time 0.0020318761669720214\n",
      "loss 0.5287831425666809 average time 0.0020166351423332733\n",
      "loss 0.2106868475675583 average time 0.001993150749767665\n",
      "loss 0.3613310158252716 average time 0.00199241222283389\n",
      "loss 0.16659031808376312 average time 0.002002890300587751\n",
      "loss 0.5126967430114746 average time 0.002005972001934424\n",
      "loss 1.1796905994415283 average time 0.0019292840047273785\n",
      "loss 0.5734807848930359 average time 0.0018990423382880786\n",
      "loss 1.0286445617675781 average time 0.001895908254373353\n",
      "loss 0.38555145263671875 average time 0.0018981984031852335\n",
      "loss 0.11541350185871124 average time 0.0019103383360197768\n",
      "loss 0.4981139302253723 average time 0.0019241644307372294\n",
      "loss 0.23309612274169922 average time 0.0019537502527236937\n",
      "loss 0.3246937394142151 average time 0.0019597148914666227\n",
      "loss 0.1587429642677307 average time 0.001955199102521874\n",
      "loss 0.4746241271495819 average time 0.0019080920005217195\n",
      "loss 1.0897108316421509 average time 0.001879245001473464\n",
      "loss 0.5364035964012146 average time 0.0018902536698927482\n",
      "loss 0.9679280519485474 average time 0.0018887182511389256\n",
      "loss 0.3374936580657959 average time 0.0020758176005911083\n",
      "loss 0.10679501295089722 average time 0.002226244000485167\n",
      "loss 0.4689749777317047 average time 0.0023080461429032897\n",
      "loss 0.24655158817768097 average time 0.0024004220006463585\n",
      "loss 0.3011968433856964 average time 0.0024270532227173036\n",
      "loss 0.15242557227611542 average time 0.002479402601835318\n",
      "loss 0.5956393480300903 average time 0.0030316609935835005\n",
      "loss 1.0710195302963257 average time 0.003092660992988385\n",
      "loss 0.5295757055282593 average time 0.0031041186636624235\n",
      "loss 0.9657638669013977 average time 0.0030864167492836714\n",
      "loss 0.35481730103492737 average time 0.0031075821989215912\n",
      "loss 0.10752804577350616 average time 0.0031570496650723118\n",
      "loss 0.46756115555763245 average time 0.003210088426380285\n",
      "loss 0.23761668801307678 average time 0.0032170311236404815\n",
      "loss 0.2907012701034546 average time 0.0031923265549509477\n",
      "loss 0.14865879714488983 average time 0.0031715517002157867\n",
      "loss 0.42112359404563904 average time 0.0028371770051307975\n",
      "loss 1.242560863494873 average time 0.0029220145038561895\n",
      "loss 0.47731539607048035 average time 0.0028624203354896357\n",
      "loss 0.9074591994285583 average time 0.0028663157494156623\n",
      "loss 0.284432977437973 average time 0.0028478231993503867\n",
      "loss 0.09698276966810226 average time 0.0028078743324537452\n",
      "loss 0.44788429141044617 average time 0.002860408141444038\n",
      "loss 0.23675934970378876 average time 0.0028866227479011288\n",
      "loss 0.2840059697628021 average time 0.002891026331991371\n",
      "loss 0.14514541625976562 average time 0.0028829539985163136\n",
      "loss 0.7044413685798645 average time 0.002689266997622326\n",
      "loss 1.0381109714508057 average time 0.0026568985008634627\n",
      "loss 0.49473991990089417 average time 0.002694760666927323\n",
      "loss 0.9575521349906921 average time 0.0027687260002130644\n",
      "loss 0.2953152358531952 average time 0.002800709401257336\n",
      "loss 0.09253731369972229 average time 0.0027290095014420026\n",
      "loss 0.4427177608013153 average time 0.002776583000917786\n",
      "loss 0.21761491894721985 average time 0.0027950624996447002\n",
      "loss 0.28218016028404236 average time 0.002828973999971317\n",
      "loss 0.1413494199514389 average time 0.002878335399320349\n",
      "loss 0.46815037727355957 average time 0.003022730000084266\n",
      "loss 1.0796000957489014 average time 0.0029860009992262347\n",
      "loss 0.4658298194408417 average time 0.003076414333190769\n",
      "loss 0.9301548004150391 average time 0.0029663865015027114\n",
      "loss 0.2438376247882843 average time 0.002922718200366944\n",
      "loss 0.0925711914896965 average time 0.0029577005000707385\n",
      "loss 0.43466654419898987 average time 0.0029603195727603243\n",
      "loss 0.22923123836517334 average time 0.002959129751106957\n",
      "loss 0.28491467237472534 average time 0.002969432111777779\n",
      "loss 0.1379871666431427 average time 0.002978524200269021\n",
      "loss 0.4728173315525055 average time 0.003245186005951837\n",
      "loss 1.1136873960494995 average time 0.0028281305020209403\n",
      "loss 0.43541857600212097 average time 0.0028232343366835265\n",
      "loss 0.8501195907592773 average time 0.002781963753513992\n",
      "loss 0.23770253360271454 average time 0.0027624448030255733\n",
      "loss 0.0932474434375763 average time 0.0027694740022222203\n",
      "loss 0.42631998658180237 average time 0.002780588430219463\n",
      "loss 0.20729005336761475 average time 0.0027740086273115596\n",
      "loss 0.28695595264434814 average time 0.0028051142235441754\n",
      "loss 0.13406823575496674 average time 0.00281232420145534\n",
      "loss 0.3670539855957031 average time 0.003054265002720058\n",
      "loss 0.9595282673835754 average time 0.0029002025024965406\n",
      "loss 0.4029849171638489 average time 0.0025920350023079662\n",
      "loss 0.7809736728668213 average time 0.002406683501903899\n",
      "loss 0.19358079135417938 average time 0.002333175601903349\n",
      "loss 0.09613170474767685 average time 0.0022673131665214897\n",
      "loss 0.41373834013938904 average time 0.002213712285405823\n",
      "loss 0.19946925342082977 average time 0.0021816979999130124\n",
      "loss 0.2911965548992157 average time 0.0021673698887591146\n",
      "loss 0.13061045110225677 average time 0.0021591195004293695\n",
      "loss 0.5326990485191345 average time 0.0018530160002410412\n",
      "loss 1.832078218460083 average time 0.0018995109951356426\n",
      "loss 1.3122087717056274 average time 0.0019215156642409662\n",
      "loss 1.3853490352630615 average time 0.0019388722494477406\n",
      "loss 0.2756461799144745 average time 0.0019224377977661788\n",
      "loss 0.17411020398139954 average time 0.0019066078316730757\n",
      "loss 0.5979403853416443 average time 0.0019012965702651335\n",
      "loss 0.23696498572826385 average time 0.0018996727478224785\n",
      "loss 0.48030391335487366 average time 0.0018946014428769962\n",
      "loss 0.1810648888349533 average time 0.0019031839985400438\n",
      "loss 0.635969340801239 average time 0.001914527998305857\n",
      "loss 1.2145094871520996 average time 0.0018876745010493322\n",
      "loss 0.5974431037902832 average time 0.0018569523325034727\n",
      "loss 1.0976482629776 average time 0.0018489010003395379\n",
      "loss 0.35052311420440674 average time 0.001832967599388212\n",
      "loss 0.0836390033364296 average time 0.0018262549988382185\n",
      "loss 0.47588491439819336 average time 0.0018244655711376772\n",
      "loss 0.23434452712535858 average time 0.0018359676253749057\n",
      "loss 0.30617985129356384 average time 0.001841900445546748\n",
      "loss 0.12491700798273087 average time 0.0018378510012989865\n",
      "loss 0.5376595854759216 average time 0.0018718850042205304\n",
      "loss 1.0748622417449951 average time 0.0018347615050151945\n",
      "loss 0.5049372911453247 average time 0.0018319273348121592\n",
      "loss 0.9383301734924316 average time 0.0018358812504447996\n",
      "loss 0.30858850479125977 average time 0.0018425079996231943\n",
      "loss 0.08862454444169998 average time 0.001853034000378102\n",
      "loss 0.44964534044265747 average time 0.0018663327153106886\n",
      "loss 0.24777796864509583 average time 0.0018633012498321477\n",
      "loss 0.2849835455417633 average time 0.0018675173335294757\n",
      "loss 0.12729112803936005 average time 0.001870205100160092\n",
      "loss 0.46331387758255005 average time 0.001941466002026573\n",
      "loss 0.9052478075027466 average time 0.0019999130029464142\n",
      "loss 0.4409152567386627 average time 0.0019964890007395297\n",
      "loss 0.7404822111129761 average time 0.002005604499427136\n",
      "loss 0.24080657958984375 average time 0.0020016171981114893\n",
      "loss 0.09162016212940216 average time 0.001973207831227531\n",
      "loss 0.39580997824668884 average time 0.0019731105696077327\n",
      "loss 0.23733125627040863 average time 0.0019549041239952203\n",
      "loss 0.299329936504364 average time 0.001944694665596924\n",
      "loss 0.1311313658952713 average time 0.0019345447984524071\n",
      "loss 0.46396955847740173 average time 0.001944821000797674\n",
      "loss 1.0141481161117554 average time 0.0019469894998474047\n",
      "loss 0.4954189956188202 average time 0.001956692667057117\n",
      "loss 0.6236585974693298 average time 0.001933499250735622\n",
      "loss 0.19816894829273224 average time 0.0019351756004616619\n",
      "loss 0.09721153229475021 average time 0.001923655501062361\n",
      "loss 0.3682093620300293 average time 0.0019227312871121934\n",
      "loss 0.19894324243068695 average time 0.001913686625484843\n",
      "loss 0.34071752429008484 average time 0.0019230063329450785\n",
      "loss 0.12849818170070648 average time 0.0019306779000908137\n",
      "loss 0.5274834632873535 average time 0.001912684002891183\n",
      "loss 1.2520180940628052 average time 0.0018656805012142285\n",
      "loss 0.5123903155326843 average time 0.0018526183320985486\n",
      "loss 0.6751396656036377 average time 0.0018551852510427125\n",
      "loss 0.25264856219291687 average time 0.0018552202014252544\n",
      "loss 0.10519690811634064 average time 0.0018556278352237618\n",
      "loss 0.3567270338535309 average time 0.0018657350012966033\n",
      "loss 0.18913094699382782 average time 0.001872621501388494\n",
      "loss 0.3379687964916229 average time 0.0018744434451218694\n",
      "loss 0.12219095230102539 average time 0.001870053700520657\n",
      "loss 0.5519805550575256 average time 0.0018781220016535372\n",
      "loss 0.7268097400665283 average time 0.0018513775011524559\n",
      "loss 0.6405968070030212 average time 0.0018327279994264246\n",
      "loss 0.6399518847465515 average time 0.0018249252502573654\n",
      "loss 0.20486561954021454 average time 0.0018382541995961219\n",
      "loss 0.10992266237735748 average time 0.0018598058317244675\n",
      "loss 0.3621163070201874 average time 0.0018625847125492458\n",
      "loss 0.15289351344108582 average time 0.0018611916237568948\n",
      "loss 0.41611000895500183 average time 0.0018652064428250823\n",
      "loss 0.11883856356143951 average time 0.0018653251981595531\n",
      "loss 0.54041588306427 average time 0.0018789319961797445\n",
      "loss 0.8963920474052429 average time 0.0018682689947308972\n",
      "loss 0.3306017518043518 average time 0.0019253609965865811\n",
      "loss 0.5140561461448669 average time 0.001940272746142\n",
      "loss 0.18091902136802673 average time 0.0019118117964826524\n",
      "loss 0.10892939567565918 average time 0.0018933929982207095\n",
      "loss 0.3559980094432831 average time 0.0018893734279221723\n",
      "loss 0.1559622585773468 average time 0.0018920377493486739\n",
      "loss 0.4106747806072235 average time 0.0018804021095598323\n",
      "loss 0.11417340487241745 average time 0.0018720947980182246\n",
      "loss 0.3329368531703949 average time 0.0018883720017038286\n",
      "loss 0.8048259019851685 average time 0.001874347505508922\n",
      "loss 0.5843442678451538 average time 0.0018661803383535395\n",
      "loss 0.47391277551651 average time 0.0018921960066654718\n",
      "loss 0.24094060063362122 average time 0.0018891738075762987\n",
      "loss 0.09307213872671127 average time 0.0018728353407156343\n",
      "loss 0.33790573477745056 average time 0.0018672588640557869\n",
      "loss 0.1403614729642868 average time 0.0018650290058576501\n",
      "loss 0.4870311915874481 average time 0.0018715187825728207\n",
      "loss 0.11015820503234863 average time 0.001880013004061766\n",
      "loss 0.30873343348503113 average time 0.0018610339995939285\n",
      "loss 0.7190032005310059 average time 0.0018436689965892583\n",
      "loss 0.5120950937271118 average time 0.0018364766659215092\n",
      "loss 0.4379703104496002 average time 0.0018340587508282624\n",
      "loss 0.23896487057209015 average time 0.0018423048024997116\n",
      "loss 0.10763747990131378 average time 0.0018556513361788045\n",
      "loss 0.3249532878398895 average time 0.0018745917166649764\n",
      "loss 0.13943132758140564 average time 0.0018829380020906684\n",
      "loss 0.5281121134757996 average time 0.0019005062238364998\n",
      "loss 0.10606646537780762 average time 0.0019046741018537432\n",
      "loss 0.3007010519504547 average time 0.002012844999553636\n",
      "loss 0.7332414388656616 average time 0.0019321104988921434\n",
      "loss 0.5918111205101013 average time 0.0018995273330559333\n",
      "loss 0.4383733868598938 average time 0.0018914552504429593\n",
      "loss 0.22289781272411346 average time 0.00187951419968158\n",
      "loss 0.11052383482456207 average time 0.0018805594990650813\n",
      "loss 0.32465264201164246 average time 0.0018859139972898576\n",
      "loss 0.13812009990215302 average time 0.0018915447471954394\n",
      "loss 0.548102617263794 average time 0.0018806631076667044\n",
      "loss 0.10171154886484146 average time 0.0018721572970971465\n",
      "loss 0.29043012857437134 average time 0.0018822269968222826\n",
      "loss 0.7347147464752197 average time 0.001960274501470849\n",
      "loss 0.31001758575439453 average time 0.001948312334328269\n",
      "loss 0.501965343952179 average time 0.001954763500543777\n",
      "loss 0.17478564381599426 average time 0.001935811800416559\n",
      "loss 0.11326142400503159 average time 0.0019258173341707638\n",
      "loss 0.32599279284477234 average time 0.0019139797152352652\n",
      "loss 0.13174839317798615 average time 0.0019082676254038233\n",
      "loss 0.5997068285942078 average time 0.0018978168898158604\n",
      "loss 0.09823179990053177 average time 0.0018917263007024302\n",
      "loss 0.27626246213912964 average time 0.0019003669929224998\n",
      "loss 0.7018135786056519 average time 0.001941302995546721\n",
      "loss 0.3396974503993988 average time 0.001978127331628154\n",
      "loss 0.47664523124694824 average time 0.0019660777499666436\n",
      "loss 0.16433924436569214 average time 0.0019418006010819226\n",
      "loss 0.12357669323682785 average time 0.001931960833996224\n",
      "loss 0.30927422642707825 average time 0.001923680429380121\n",
      "loss 0.12484734505414963 average time 0.0019168266252381728\n",
      "loss 0.6283661723136902 average time 0.0019193891116366204\n",
      "loss 0.09452298283576965 average time 0.0019173746006563306\n",
      "loss 0.2796584963798523 average time 0.0018621490069199353\n",
      "loss 0.8717578053474426 average time 0.0018514415045501665\n",
      "loss 0.3062211573123932 average time 0.0018533230025786906\n",
      "loss 0.47380200028419495 average time 0.001860975752933882\n",
      "loss 0.1500968486070633 average time 0.0018808512035757303\n",
      "loss 0.09239349514245987 average time 0.0018899786691569413\n",
      "loss 0.30532175302505493 average time 0.0018917094312408673\n",
      "loss 0.11568859964609146 average time 0.0018956481278291904\n",
      "loss 0.42605382204055786 average time 0.001890697892393089\n",
      "loss 0.09286268800497055 average time 0.0018849045030074193\n",
      "loss 0.2545468509197235 average time 0.0018044290004763752\n",
      "loss 0.6353596448898315 average time 0.001837418999057263\n",
      "loss 0.351123571395874 average time 0.0018407106667291374\n",
      "loss 0.41069909930229187 average time 0.0018541112486855126\n",
      "loss 0.11456499248743057 average time 0.0018628654000349342\n",
      "loss 0.1212640330195427 average time 0.001904504832928069\n",
      "loss 0.299736887216568 average time 0.0019174719987703221\n",
      "loss 0.11708414554595947 average time 0.0019115909980610014\n",
      "loss 0.6284002661705017 average time 0.001917376109243681\n",
      "loss 0.08893561363220215 average time 0.0019156128991162404\n",
      "loss 0.3060091435909271 average time 0.001969616999849677\n",
      "loss 0.9772071242332458 average time 0.001986247000168078\n",
      "loss 0.30282339453697205 average time 0.001977985333263253\n",
      "loss 0.4803294241428375 average time 0.0019609999979729766\n",
      "loss 0.14560562372207642 average time 0.0019391635973006487\n",
      "loss 0.12965047359466553 average time 0.0019271833311843996\n",
      "loss 0.29106855392456055 average time 0.001923415999127818\n",
      "loss 0.11335364729166031 average time 0.001926284624787513\n",
      "loss 0.5335990786552429 average time 0.001917898110114038\n",
      "loss 0.08698593825101852 average time 0.0019295154000865296\n",
      "loss 0.24990586936473846 average time 0.0019810119888279587\n",
      "loss 0.6394056677818298 average time 0.0019578474963782354\n",
      "loss 0.49208956956863403 average time 0.0019154409978849193\n",
      "loss 0.376225084066391 average time 0.0019084769976325333\n",
      "loss 0.0927194207906723 average time 0.0019004261957015843\n",
      "loss 0.10014431923627853 average time 0.0018937744953048726\n",
      "loss 0.29746970534324646 average time 0.001897383138670453\n",
      "loss 0.1127173900604248 average time 0.001906135747267399\n",
      "loss 0.690096914768219 average time 0.0019185863305918045\n",
      "loss 0.08438007533550262 average time 0.0019279325978131966\n",
      "loss 0.2922804355621338 average time 0.002104312004521489\n",
      "loss 1.1243025064468384 average time 0.002027419002261013\n",
      "loss 0.31456127762794495 average time 0.001980197666756188\n",
      "loss 0.6093330383300781 average time 0.0019586242505465635\n",
      "loss 0.1507895588874817 average time 0.0019535478015895932\n",
      "loss 0.09458189457654953 average time 0.0019583330016272765\n",
      "loss 0.3127882182598114 average time 0.0019590342868052953\n",
      "loss 0.1320878565311432 average time 0.0019416158754029312\n",
      "loss 0.3659517168998718 average time 0.0019343801111810738\n",
      "loss 0.08057931065559387 average time 0.0019311519996263087\n",
      "loss 0.30521032214164734 average time 0.0019042170036118477\n",
      "loss 0.9646665453910828 average time 0.0019033760071033611\n",
      "loss 0.4414002597332001 average time 0.001961274337178717\n",
      "loss 0.4545896649360657 average time 0.0019909832521807403\n",
      "loss 0.159529909491539 average time 0.001991690000751987\n",
      "loss 0.13960769772529602 average time 0.0019837508333148436\n",
      "loss 0.2763664424419403 average time 0.001988869715382212\n",
      "loss 0.11121001094579697 average time 0.001993740000325488\n",
      "loss 0.4713861644268036 average time 0.001993152666707627\n",
      "loss 0.07530975341796875 average time 0.0019899889002554117\n",
      "loss 0.21777324378490448 average time 0.0020205090020317586\n",
      "loss 0.7051301002502441 average time 0.0019943690003128723\n",
      "loss 0.5787829756736755 average time 0.0019709506654180587\n",
      "loss 0.617912769317627 average time 0.0019793174994993026\n",
      "loss 0.087103471159935 average time 0.0021639675996266306\n",
      "loss 0.12404558807611465 average time 0.002224179000283281\n",
      "loss 0.2719680368900299 average time 0.00233708528652122\n",
      "loss 0.11549166589975357 average time 0.002432015625090571\n",
      "loss 0.619411051273346 average time 0.002513077222328219\n",
      "loss 0.0736013650894165 average time 0.002585555299418047\n",
      "loss 0.1716783493757248 average time 0.00257962899049744\n",
      "loss 0.6412592530250549 average time 0.002630719998269342\n",
      "loss 0.5285553336143494 average time 0.0028134296659845857\n",
      "loss 0.5298096537590027 average time 0.0029585754981962965\n",
      "loss 0.10276418179273605 average time 0.0030402395993005486\n",
      "loss 0.10657062381505966 average time 0.0030656173342140393\n",
      "loss 0.2905366122722626 average time 0.003091070000581177\n",
      "loss 0.1068095788359642 average time 0.0030452081249677577\n",
      "loss 0.7097770571708679 average time 0.0030477097777960203\n",
      "loss 0.07032165676355362 average time 0.003072744999662973\n",
      "loss 0.222455695271492 average time 0.002780310002854094\n",
      "loss 1.1212533712387085 average time 0.0027915905008558182\n",
      "loss 0.31598615646362305 average time 0.0027677276676210264\n",
      "loss 0.6565595269203186 average time 0.002883200751966797\n",
      "loss 0.09874039888381958 average time 0.0028611920019611715\n",
      "loss 0.15481333434581757 average time 0.0029226618337755404\n",
      "loss 0.26438459753990173 average time 0.002877965142917154\n",
      "loss 0.11982718855142593 average time 0.0028307387506356463\n",
      "loss 0.5658317804336548 average time 0.002810350889986795\n",
      "loss 0.06781628727912903 average time 0.002831486000912264\n",
      "loss 0.19608037173748016 average time 0.0032517099974211307\n",
      "loss 0.6680095791816711 average time 0.0031293145031668244\n",
      "loss 0.6646287441253662 average time 0.0031171376688871534\n",
      "loss 0.41733241081237793 average time 0.003291292000503745\n",
      "loss 0.08971124142408371 average time 0.0033456403997261077\n",
      "loss 0.1002989336848259 average time 0.00325599249995624\n",
      "loss 0.29197177290916443 average time 0.0032064168567636185\n",
      "loss 0.11276625841856003 average time 0.003192016248503933\n",
      "loss 0.6689359545707703 average time 0.0031390703318174926\n",
      "loss 0.06919052451848984 average time 0.0030872677980223672\n",
      "loss 0.13325472176074982 average time 0.002677030999911949\n",
      "loss 0.6351231932640076 average time 0.002732545502949506\n",
      "loss 0.6243730187416077 average time 0.0026785756678630907\n",
      "loss 0.36702245473861694 average time 0.0027630365011282266\n",
      "loss 0.09466802328824997 average time 0.0028444702005945145\n",
      "loss 0.10682942718267441 average time 0.0028986355014300595\n",
      "loss 0.2929920256137848 average time 0.002892879143556846\n",
      "loss 0.11573977768421173 average time 0.0028722063756140416\n",
      "loss 0.6847138404846191 average time 0.0029082960006780924\n",
      "loss 0.06748221069574356 average time 0.0029115158003987745\n",
      "loss 0.1575576514005661 average time 0.002744815000332892\n",
      "loss 0.6875912547111511 average time 0.0028159009967930614\n",
      "loss 0.6211990714073181 average time 0.0028951833319539825\n",
      "loss 0.2371479719877243 average time 0.0028624504985054955\n",
      "loss 0.09091289341449738 average time 0.00283683599694632\n",
      "loss 0.10263397544622421 average time 0.0028059138326595227\n",
      "loss 0.2942163348197937 average time 0.0027789094282447226\n",
      "loss 0.11409642547369003 average time 0.0027551511245837903\n",
      "loss 0.6809883117675781 average time 0.002761680110512922\n",
      "loss 0.06628437340259552 average time 0.002768415499711409\n",
      "loss 0.15437839925289154 average time 0.002856599004007876\n",
      "loss 0.7453413009643555 average time 0.0029165590001503008\n",
      "loss 0.5877538919448853 average time 0.0027845286678833264\n",
      "loss 0.37310847640037537 average time 0.0028015900019090624\n",
      "loss 0.09289652109146118 average time 0.0027902234024368227\n",
      "loss 0.11400952190160751 average time 0.002762576667785955\n",
      "loss 0.2764941155910492 average time 0.002808098286124212\n",
      "loss 0.11370372772216797 average time 0.002851336751045892\n",
      "loss 0.6832071542739868 average time 0.002872315111906371\n",
      "loss 0.0641399696469307 average time 0.002829559600795619\n",
      "loss 0.14660528302192688 average time 0.0028896100004203616\n",
      "loss 0.7092242240905762 average time 0.002724121504579671\n",
      "loss 0.5357284545898438 average time 0.0027090626671755066\n",
      "loss 0.3358442783355713 average time 0.0026977497502230106\n",
      "loss 0.08562573045492172 average time 0.0026637685992754994\n",
      "loss 0.11969071626663208 average time 0.0027475659993554777\n",
      "loss 0.2745109796524048 average time 0.0027625628562444556\n",
      "loss 0.11478611081838608 average time 0.002806931749510113\n",
      "loss 0.6868056654930115 average time 0.0028074781101248745\n",
      "loss 0.06267688423395157 average time 0.00285572169884108\n",
      "loss 0.10512514412403107 average time 0.003000259005930275\n",
      "loss 0.7762751579284668 average time 0.002836408005096018\n",
      "loss 0.542516827583313 average time 0.002770322336970518\n",
      "loss 0.4567215144634247 average time 0.002852101504104212\n",
      "loss 0.08544664829969406 average time 0.002899323803372681\n",
      "loss 0.12436192482709885 average time 0.0028840426702906067\n",
      "loss 0.26981741189956665 average time 0.002917855289165995\n",
      "loss 0.11382170766592026 average time 0.0029077793775650206\n",
      "loss 0.6979089975357056 average time 0.002861549336230382\n",
      "loss 0.06119703873991966 average time 0.0028768293033353985\n",
      "loss 0.08942284435033798 average time 0.0031467879912815988\n",
      "loss 0.9251396059989929 average time 0.0031017864949535578\n",
      "loss 0.43282127380371094 average time 0.003096711327865099\n",
      "loss 0.5795106887817383 average time 0.0030814237470622174\n",
      "loss 0.09941964596509933 average time 0.0030135061999317257\n",
      "loss 0.1895984262228012 average time 0.0029758023330941795\n",
      "loss 0.22348594665527344 average time 0.002969694999379239\n",
      "loss 0.10893096774816513 average time 0.0029756172496126967\n",
      "loss 0.7536953687667847 average time 0.0029741311103053804\n",
      "loss 0.059526242315769196 average time 0.002949480799725279\n",
      "loss 0.16843855381011963 average time 0.0033752820023801177\n",
      "loss 0.6283990740776062 average time 0.003030166500248015\n",
      "loss 0.5302147269248962 average time 0.0027880390007824947\n",
      "loss 0.5622366666793823 average time 0.0025746125014848077\n",
      "loss 0.09157254546880722 average time 0.0024298319988884033\n",
      "loss 0.11987876892089844 average time 0.0023489844993067285\n",
      "loss 0.2685340344905853 average time 0.00231153299799189\n",
      "loss 0.1095377579331398 average time 0.0022728028740675653\n",
      "loss 0.7693902850151062 average time 0.0022327364437902965\n",
      "loss 0.05893298611044884 average time 0.00220184809982311\n",
      "loss 0.12438417971134186 average time 0.0018882600020151586\n",
      "loss 0.3932623565196991 average time 0.001864474004250951\n",
      "loss 0.5920147895812988 average time 0.0018656316698373607\n",
      "loss 0.5491400957107544 average time 0.0019006167521001772\n",
      "loss 0.08018036186695099 average time 0.0019267746016848831\n",
      "loss 0.12566988170146942 average time 0.0019221431688250352\n",
      "loss 0.25845494866371155 average time 0.0019237147155217826\n",
      "loss 0.11658252775669098 average time 0.0019147855007031467\n",
      "loss 0.6819389462471008 average time 0.0019099228886059589\n",
      "loss 0.056261032819747925 average time 0.0019037946999305858\n",
      "loss 0.08051259815692902 average time 0.0018565310072153807\n",
      "loss 0.9153534770011902 average time 0.0019141155038960279\n",
      "loss 0.6475739479064941 average time 0.001903605004772544\n",
      "loss 0.48642340302467346 average time 0.0019004307521390729\n",
      "loss 0.08186405897140503 average time 0.001889410801231861\n",
      "loss 0.11772987991571426 average time 0.0018824696688291927\n",
      "loss 0.2685486376285553 average time 0.0018777884316763707\n",
      "loss 0.11678296327590942 average time 0.0018771827543969267\n",
      "loss 0.6953620910644531 average time 0.0018803690046641148\n",
      "loss 0.05451362207531929 average time 0.0018954626038903371\n",
      "loss 0.11924963444471359 average time 0.0019366579956840723\n",
      "loss 0.4004943370819092 average time 0.001911790000158362\n",
      "loss 0.5130468606948853 average time 0.0018856449968491992\n",
      "loss 0.43748757243156433 average time 0.0018787802482256666\n",
      "loss 0.073877714574337 average time 0.001861779998522252\n",
      "loss 0.1147465929389 average time 0.001855508498653459\n",
      "loss 0.27167555689811707 average time 0.0018463569990957954\n",
      "loss 0.11518996953964233 average time 0.001853464749729028\n",
      "loss 0.7313026785850525 average time 0.0018636955553665757\n",
      "loss 0.052280981093645096 average time 0.001863221598905511\n",
      "loss 0.07555525749921799 average time 0.0019560809980612246\n",
      "loss 0.7520219683647156 average time 0.001969132001977414\n",
      "loss 0.6433922648429871 average time 0.0019354400030958156\n",
      "loss 0.7001276016235352 average time 0.0019137832534033806\n",
      "loss 0.061797358095645905 average time 0.001913849203614518\n",
      "loss 0.12108217924833298 average time 0.0019109390027976284\n",
      "loss 0.25735101103782654 average time 0.0019221510013033237\n",
      "loss 0.11732450872659683 average time 0.001922026125685079\n",
      "loss 0.7467839121818542 average time 0.001912748001438255\n",
      "loss 0.051286567002534866 average time 0.0019039360014721752\n",
      "loss 0.07626044005155563 average time 0.0018826300057116895\n",
      "loss 0.6664451360702515 average time 0.0018646310071926565\n",
      "loss 0.6237219572067261 average time 0.0018965020054019987\n",
      "loss 0.7029539942741394 average time 0.001912620004150085\n",
      "loss 0.05511600896716118 average time 0.0019276976040564478\n",
      "loss 0.1090422198176384 average time 0.0019230111684494962\n",
      "loss 0.2614003121852875 average time 0.0019325205718632788\n",
      "loss 0.11461098492145538 average time 0.001917933750228258\n",
      "loss 0.7672563791275024 average time 0.0019161387781302134\n",
      "loss 0.04945977032184601 average time 0.0019265392998931929\n",
      "loss 0.07074954360723495 average time 0.0020108210016041993\n",
      "loss 0.4931749999523163 average time 0.002046446004533209\n",
      "loss 0.485394686460495 average time 0.0020200876685945936\n",
      "loss 0.6449686884880066 average time 0.0019964825024362653\n",
      "loss 0.05009058117866516 average time 0.001968874600715935\n",
      "loss 0.08691421151161194 average time 0.001966961001162417\n",
      "loss 0.2715286612510681 average time 0.0019676781440752427\n",
      "loss 0.10852713882923126 average time 0.001962267501221504\n",
      "loss 0.769652783870697 average time 0.001962488668375752\n",
      "loss 0.05004862695932388 average time 0.0019658946017734706\n",
      "loss 0.07260411977767944 average time 0.001950147000607103\n",
      "loss 0.47227177023887634 average time 0.0019407670013606548\n",
      "loss 0.5575338006019592 average time 0.0019275926689927777\n",
      "loss 0.5670385360717773 average time 0.0019488485006149859\n",
      "loss 0.044632770121097565 average time 0.001945705600315705\n",
      "loss 0.08952449262142181 average time 0.001944886832886065\n",
      "loss 0.26308730244636536 average time 0.0019548595711655382\n",
      "loss 0.10252273082733154 average time 0.0019612726243212818\n",
      "loss 0.7892064452171326 average time 0.0019526155547808029\n",
      "loss 0.0486677847802639 average time 0.0019438242991454899\n",
      "loss 0.09567306190729141 average time 0.0018747179990168661\n",
      "loss 0.4541938304901123 average time 0.0018814589997055008\n",
      "loss 0.5788682103157043 average time 0.0018812096662198503\n",
      "loss 0.5430766344070435 average time 0.0019946999993408097\n",
      "loss 0.048930685967206955 average time 0.002236322400625795\n",
      "loss 0.08926045894622803 average time 0.002330467999757578\n",
      "loss 0.25003084540367126 average time 0.002426272427962561\n",
      "loss 0.10161061584949493 average time 0.002490196249127621\n",
      "loss 0.798621654510498 average time 0.002547502888806371\n",
      "loss 0.04758432134985924 average time 0.002564457399887033\n",
      "loss 0.08560290187597275 average time 0.002851056000217795\n",
      "loss 0.49282822012901306 average time 0.0029186049988493322\n",
      "loss 0.2684393525123596 average time 0.0029906743306977054\n",
      "loss 0.19210508465766907 average time 0.00293712024751585\n",
      "loss 0.11994452774524689 average time 0.0028857633979059755\n",
      "loss 0.09314347803592682 average time 0.0028600108318884545\n",
      "loss 0.2294342964887619 average time 0.0028566662839148195\n",
      "loss 0.10633384436368942 average time 0.0028743901230336632\n",
      "loss 0.7485509514808655 average time 0.0028766032203566285\n",
      "loss 0.045250169932842255 average time 0.0029267001979751514\n",
      "loss 0.07772133499383926 average time 0.0027085880015511065\n",
      "loss 0.4848193824291229 average time 0.0028285134985344485\n",
      "loss 0.6372410655021667 average time 0.002903792332702627\n",
      "loss 0.48821765184402466 average time 0.002939685249875765\n",
      "loss 0.039894819259643555 average time 0.0029720315989106892\n",
      "loss 0.10215865820646286 average time 0.0029818195001765466\n",
      "loss 0.25114020705223083 average time 0.0030367012859122563\n",
      "loss 0.10153643041849136 average time 0.0030975355008558835\n",
      "loss 0.8214523196220398 average time 0.0030590815557580853\n",
      "loss 0.045635856688022614 average time 0.0030599430000875145\n",
      "loss 0.1427202969789505 average time 0.003215012006694451\n",
      "loss 0.8583823442459106 average time 0.003172927505802363\n",
      "loss 0.6137505173683167 average time 0.003207158671381573\n",
      "loss 0.2996900677680969 average time 0.0032248727526166475\n",
      "loss 0.04435877129435539 average time 0.003151802801527083\n",
      "loss 0.11217935383319855 average time 0.0031333585031097753\n",
      "loss 0.2392585575580597 average time 0.00320569728783864\n",
      "loss 0.0995824858546257 average time 0.0031958441266033335\n",
      "loss 0.8390464782714844 average time 0.003155213445942435\n",
      "loss 0.04525073990225792 average time 0.0031025179009884596\n",
      "loss 0.13710619509220123 average time 0.002993113996926695\n",
      "loss 0.9355183839797974 average time 0.0028828714991686864\n",
      "loss 0.5494667291641235 average time 0.0029201093347122273\n",
      "loss 0.39432892203330994 average time 0.002869223751476966\n",
      "loss 0.03791173920035362 average time 0.002852086799684912\n",
      "loss 0.10204777866601944 average time 0.0028315883312219135\n",
      "loss 0.24305081367492676 average time 0.0027804035695070135\n",
      "loss 0.10056890547275543 average time 0.0027680858739768154\n",
      "loss 0.8329504132270813 average time 0.0027807227773074474\n",
      "loss 0.04282074421644211 average time 0.002778316900250502\n",
      "loss 0.09879671782255173 average time 0.0029453350021503865\n",
      "loss 0.41742897033691406 average time 0.0027226020029047504\n",
      "loss 0.1360621154308319 average time 0.002816738668285931\n",
      "loss 0.2489498406648636 average time 0.002784558501152787\n",
      "loss 0.033177006989717484 average time 0.0028365428019315004\n",
      "loss 0.09942501038312912 average time 0.0029134813358541577\n",
      "loss 0.2369861900806427 average time 0.0029473674300658915\n",
      "loss 0.09987174719572067 average time 0.002962207250384381\n",
      "loss 0.832135021686554 average time 0.002948251888155937\n",
      "loss 0.04286930710077286 average time 0.002984405999770388\n",
      "loss 0.3080753982067108 average time 0.0030806060042232273\n",
      "loss 0.7510424852371216 average time 0.0030736784992041067\n",
      "loss 0.5164220929145813 average time 0.0030403946658285955\n",
      "loss 0.3031027615070343 average time 0.002965316500631161\n",
      "loss 0.03568058833479881 average time 0.0029629932004027067\n",
      "loss 0.10839693993330002 average time 0.0028979766671545803\n",
      "loss 0.23188261687755585 average time 0.002900679428130388\n",
      "loss 0.09942474961280823 average time 0.0028944459999911487\n",
      "loss 0.8144623041152954 average time 0.0028508606666906014\n",
      "loss 0.045342110097408295 average time 0.00285323980008252\n",
      "loss 0.0758085772395134 average time 0.0028529909963253886\n",
      "loss 0.5844808220863342 average time 0.0028098969999700786\n",
      "loss 0.4906451404094696 average time 0.0028598426701501012\n",
      "loss 0.31975609064102173 average time 0.0027433250003377906\n",
      "loss 0.04266703873872757 average time 0.002738049000501633\n",
      "loss 0.09804879873991013 average time 0.0027583043347112835\n",
      "loss 0.2330762892961502 average time 0.0028076547149768366\n",
      "loss 0.08469519764184952 average time 0.0028115802505635657\n",
      "loss 0.8260365724563599 average time 0.0028153421120562902\n",
      "loss 0.04218355193734169 average time 0.0028041125005111097\n",
      "loss 0.09694429486989975 average time 0.0021561339881736785\n",
      "loss 0.3835132420063019 average time 0.00238718549197074\n",
      "loss 0.5129830241203308 average time 0.0025034833245445045\n",
      "loss 0.4220868647098541 average time 0.0026006019933265635\n",
      "loss 0.04429979249835014 average time 0.0026358745940960942\n",
      "loss 0.09860595315694809 average time 0.0026536669948836787\n",
      "loss 0.22507977485656738 average time 0.002721963566062706\n",
      "loss 0.08038406074047089 average time 0.0027481739949143957\n",
      "loss 0.8371680378913879 average time 0.002747586550604966\n",
      "loss 0.041195791214704514 average time 0.0027002325960202144\n",
      "loss 0.09964145720005035 average time 0.0031419449974782764\n",
      "loss 0.40182778239250183 average time 0.0030642079975223167\n",
      "loss 0.558064877986908 average time 0.0029746443335898222\n",
      "loss 0.5987142324447632 average time 0.0030002934992080553\n",
      "loss 0.04381812736392021 average time 0.0029691775995306672\n",
      "loss 0.09693498909473419 average time 0.003083304332685657\n",
      "loss 0.21428613364696503 average time 0.0030782207139834227\n",
      "loss 0.07964768260717392 average time 0.003057032124052057\n",
      "loss 0.8407208323478699 average time 0.003001430110566111\n",
      "loss 0.040570180863142014 average time 0.002962817700114101\n",
      "loss 0.08151765912771225 average time 0.0028145319991745056\n",
      "loss 0.4720158576965332 average time 0.0030164994997903703\n",
      "loss 0.4149610996246338 average time 0.0029565386675919096\n",
      "loss 0.3190019130706787 average time 0.002922023750725202\n",
      "loss 0.04106910154223442 average time 0.002856180402217433\n",
      "loss 0.09262099117040634 average time 0.002851141167532963\n",
      "loss 0.21765771508216858 average time 0.002841770286904648\n",
      "loss 0.08011993020772934 average time 0.00285381737514399\n",
      "loss 0.7966429591178894 average time 0.002892623889234124\n",
      "loss 0.04024501517415047 average time 0.0028902953998185695\n",
      "loss 0.07464201748371124 average time 0.002613356987712905\n",
      "loss 0.6909474730491638 average time 0.0026611864904407413\n",
      "loss 0.5622994303703308 average time 0.0026794946608909714\n",
      "loss 0.6127087473869324 average time 0.0027461674954975023\n",
      "loss 0.04133632779121399 average time 0.0027423727959394453\n",
      "loss 0.09231853485107422 average time 0.0027263613309090337\n",
      "loss 0.21499302983283997 average time 0.002785887141778533\n",
      "loss 0.07479365170001984 average time 0.00284379612407065\n",
      "loss 0.8258708119392395 average time 0.0028304167767055333\n",
      "loss 0.03932492807507515 average time 0.002817833899171092\n",
      "loss 0.08068873733282089 average time 0.003038821006193757\n",
      "loss 0.5960568785667419 average time 0.0031127375044161455\n",
      "loss 0.46206673979759216 average time 0.0030400506670897206\n",
      "loss 0.6166985630989075 average time 0.0030524414978572167\n",
      "loss 0.03870166465640068 average time 0.003003183597931638\n",
      "loss 0.09075682610273361 average time 0.002996594164869748\n",
      "loss 0.2197357714176178 average time 0.0029956655704881994\n",
      "loss 0.07140880078077316 average time 0.003050309124519117\n",
      "loss 0.8065878748893738 average time 0.0030246678897593586\n",
      "loss 0.039551012217998505 average time 0.003015938000869937\n",
      "loss 0.09118738025426865 average time 0.0031224289920646696\n",
      "loss 1.0478581190109253 average time 0.0029750694963149725\n",
      "loss 0.47076889872550964 average time 0.0028931553300935776\n",
      "loss 0.16313083469867706 average time 0.002629138996999245\n",
      "loss 0.03488145396113396 average time 0.002462545997928828\n",
      "loss 0.09101288765668869 average time 0.0023533703322755175\n",
      "loss 0.19689901173114777 average time 0.002286085285678772\n",
      "loss 0.07506268471479416 average time 0.002225586249696789\n",
      "loss 0.8119059801101685 average time 0.0021894656667589313\n",
      "loss 0.04031261429190636 average time 0.0021589165000477804\n",
      "loss 0.08915939182043076 average time 0.0019485350034665317\n",
      "loss 0.7632939219474792 average time 0.0019132585008628667\n",
      "loss 0.40971922874450684 average time 0.0018871563339295486\n",
      "loss 0.34844061732292175 average time 0.0018711784988408907\n",
      "loss 0.034573573619127274 average time 0.0018541931987274439\n",
      "loss 0.1011173278093338 average time 0.001849567497265525\n",
      "loss 0.194993793964386 average time 0.001851542284046965\n",
      "loss 0.07870491594076157 average time 0.0018516021239338442\n",
      "loss 0.8305878639221191 average time 0.0018520649985617234\n",
      "loss 0.040487777441740036 average time 0.0018505167990224437\n",
      "loss 0.08574101328849792 average time 0.0018098670011386275\n",
      "loss 0.918705940246582 average time 0.0018180670053698123\n",
      "loss 0.42261654138565063 average time 0.0018438286695163697\n",
      "loss 0.9963259696960449 average time 0.0018318902520695702\n",
      "loss 0.04825178533792496 average time 0.0018400056015234441\n",
      "loss 0.10846040397882462 average time 0.0018493603340660532\n",
      "loss 0.18810486793518066 average time 0.0018647371423763355\n",
      "loss 0.09415239095687866 average time 0.0018623782497888897\n",
      "loss 0.765515923500061 average time 0.0018616977776400746\n",
      "loss 0.03823511675000191 average time 0.0018598298999713734\n",
      "loss 0.07470621168613434 average time 0.0018226439866703005\n",
      "loss 1.7629737854003906 average time 0.0018360809929436072\n",
      "loss 0.26638302206993103 average time 0.0018345276615582406\n",
      "loss 0.4210513234138489 average time 0.001890461245784536\n",
      "loss 0.21483562886714935 average time 0.0019165239974390715\n",
      "loss 0.13565579056739807 average time 0.0018958523322362452\n",
      "loss 0.15741269290447235 average time 0.0018879705700757248\n",
      "loss 0.12272926419973373 average time 0.0018768769985763355\n",
      "loss 0.42412009835243225 average time 0.001870665666129854\n",
      "loss 0.040842074900865555 average time 0.0018644100992241874\n",
      "loss 0.10365622490644455 average time 0.0018932829902041704\n",
      "loss 0.8631902933120728 average time 0.0019284194969804958\n",
      "loss 0.5033673048019409 average time 0.001932269330136478\n",
      "loss 0.4371350407600403 average time 0.001919338245934341\n",
      "loss 0.05780911445617676 average time 0.0019187377968337388\n",
      "loss 0.12055521458387375 average time 0.0019142188320014006\n",
      "loss 0.14520876109600067 average time 0.0019046675701559121\n",
      "loss 0.09791885316371918 average time 0.0018952841236023232\n",
      "loss 0.680266261100769 average time 0.0019041593317201155\n",
      "loss 0.043241407722234726 average time 0.0019079702984308824\n",
      "loss 0.07912281900644302 average time 0.0018785819946788252\n",
      "loss 0.7816726565361023 average time 0.0018539089983096346\n",
      "loss 0.6011675000190735 average time 0.0018364669972409805\n",
      "loss 0.38706937432289124 average time 0.0018416514986893161\n",
      "loss 0.033724892884492874 average time 0.0018367755978833883\n",
      "loss 0.1108279749751091 average time 0.0018362649990012869\n",
      "loss 0.16300837695598602 average time 0.0018544379995936262\n",
      "loss 0.08618303388357162 average time 0.0018704856251133607\n",
      "loss 0.8234179615974426 average time 0.0018716687778942288\n",
      "loss 0.04131419584155083 average time 0.001866802300326526\n",
      "loss 0.08961807936429977 average time 0.0017997980024665593\n",
      "loss 0.3277701139450073 average time 0.0018034395028371363\n",
      "loss 0.42382174730300903 average time 0.0018155256685956072\n",
      "loss 0.16146136820316315 average time 0.0018285392512916588\n",
      "loss 0.021000919863581657 average time 0.0018443828015588225\n",
      "loss 0.08757463842630386 average time 0.0018528376690422495\n",
      "loss 0.16361217200756073 average time 0.0018425204293868905\n",
      "loss 0.08781922608613968 average time 0.0018449245012016036\n",
      "loss 0.7239601016044617 average time 0.0018521223344012266\n",
      "loss 0.04322636127471924 average time 0.0018500326016219332\n",
      "loss 0.12162601947784424 average time 0.0018149719969369471\n",
      "loss 0.5380910038948059 average time 0.0018325699970591813\n",
      "loss 0.40756756067276 average time 0.0018548493296839297\n",
      "loss 0.1311396360397339 average time 0.0018889244965976104\n",
      "loss 0.029222995042800903 average time 0.0019080531971994788\n",
      "loss 0.08492497354745865 average time 0.0019006364976909633\n",
      "loss 0.18990956246852875 average time 0.001908354998034026\n",
      "loss 0.08121898025274277 average time 0.0019027012475999073\n",
      "loss 0.7904209494590759 average time 0.0019110971086451576\n",
      "loss 0.04445295408368111 average time 0.0019173586984397844\n",
      "loss 0.09016884118318558 average time 0.001939491997472942\n",
      "loss 0.34306252002716064 average time 0.001889646997442469\n",
      "loss 0.39078137278556824 average time 0.0018624596674150476\n",
      "loss 0.12187061458826065 average time 0.00186364150227746\n",
      "loss 0.027144242078065872 average time 0.0018564222033601253\n",
      "loss 0.07576337456703186 average time 0.001846077336037221\n",
      "loss 0.18437910079956055 average time 0.0018399970022229743\n",
      "loss 0.07823656499385834 average time 0.0018423115024052096\n",
      "loss 0.7877295613288879 average time 0.0018500192248676387\n",
      "loss 0.04165898263454437 average time 0.001853167802793905\n",
      "loss 0.1179623156785965 average time 0.0018273949983995408\n",
      "loss 0.37107449769973755 average time 0.0018125974998110905\n",
      "loss 0.4963432252407074 average time 0.0018045296636410057\n",
      "loss 0.6257367730140686 average time 0.0018148387462133542\n",
      "loss 0.026537958532571793 average time 0.0018133227964863181\n",
      "loss 0.08246999233961105 average time 0.0018259774973072732\n",
      "loss 0.18179140985012054 average time 0.0018400509978112366\n",
      "loss 0.07399431616067886 average time 0.001837890372844413\n",
      "loss 0.8314196467399597 average time 0.0018368193320930004\n",
      "loss 0.04026985168457031 average time 0.0018388156986329704\n",
      "loss 0.07578425854444504 average time 0.0018378200056031347\n",
      "loss 0.3610139489173889 average time 0.001833411004045047\n",
      "loss 0.3861803710460663 average time 0.0018580253371813646\n",
      "loss 1.019505262374878 average time 0.0018842495026183315\n",
      "loss 0.042017918080091476 average time 0.0018981962017714977\n",
      "loss 0.08692292124032974 average time 0.0018855928352180247\n",
      "loss 0.17597785592079163 average time 0.0018768150011809277\n",
      "loss 0.0763106420636177 average time 0.001865888751053717\n",
      "loss 0.793220579624176 average time 0.0018574711117738238\n",
      "loss 0.04035494476556778 average time 0.0018574977001408115\n",
      "loss 0.10847192257642746 average time 0.0018235819996334612\n",
      "loss 0.2803433835506439 average time 0.0018728339998051525\n",
      "loss 0.42029228806495667 average time 0.0018697259990343204\n",
      "loss 0.608058750629425 average time 0.0019001627518446184\n",
      "loss 0.03128892928361893 average time 0.001917748001869768\n",
      "loss 0.08552885055541992 average time 0.0018969421684353924\n",
      "loss 0.1719980239868164 average time 0.0018905374299668309\n",
      "loss 0.0724678561091423 average time 0.001881128875829745\n",
      "loss 0.8099735379219055 average time 0.0018778084453919695\n",
      "loss 0.04032905772328377 average time 0.0018805756006622686\n",
      "loss 0.15365692973136902 average time 0.001888260004343465\n",
      "loss 0.9408075213432312 average time 0.0018651740020141005\n",
      "loss 0.37518104910850525 average time 0.0018469606658133367\n",
      "loss 0.8790585398674011 average time 0.0018555437499890104\n",
      "loss 0.03671311214566231 average time 0.0018452785997651517\n",
      "loss 0.08507383614778519 average time 0.0018507144995965064\n",
      "loss 0.17095978558063507 average time 0.001867242713259267\n",
      "loss 0.07760594040155411 average time 0.0018785228738852312\n",
      "loss 0.8214910626411438 average time 0.0018855471103193446\n",
      "loss 0.0402211919426918 average time 0.001877774799009785\n",
      "loss 0.12698693573474884 average time 0.001840449998853728\n",
      "loss 0.36620548367500305 average time 0.0018465509975794702\n",
      "loss 0.4581398069858551 average time 0.0018357293319422752\n",
      "loss 0.2852538526058197 average time 0.0018596899992553517\n",
      "loss 0.02815934456884861 average time 0.0018693928008433431\n",
      "loss 0.08245077729225159 average time 0.0018860828327403093\n",
      "loss 0.1666402667760849 average time 0.0018834579995434199\n",
      "loss 0.06861510872840881 average time 0.0018893345003016292\n",
      "loss 0.856203556060791 average time 0.0018794889996449154\n",
      "loss 0.03871219977736473 average time 0.0018748975000344218\n",
      "loss 0.09165927022695541 average time 0.0018918749981094152\n",
      "loss 0.34361082315444946 average time 0.0018359325011260807\n",
      "loss 0.3927777409553528 average time 0.001885301333386451\n",
      "loss 0.4721224009990692 average time 0.0018988279995392078\n",
      "loss 0.02712363936007023 average time 0.0019022161993198098\n",
      "loss 0.07917162030935287 average time 0.0018986624999282262\n",
      "loss 0.157723069190979 average time 0.001892567143409646\n",
      "loss 0.06538638472557068 average time 0.001881373624491971\n",
      "loss 0.8616198897361755 average time 0.0018878224439039413\n",
      "loss 0.039107609540224075 average time 0.0018855052002472803\n",
      "loss 0.13005712628364563 average time 0.0021919899946078658\n",
      "loss 0.9955427050590515 average time 0.0021182990015950055\n",
      "loss 0.27364131808280945 average time 0.0020820550007435183\n",
      "loss 0.47766542434692383 average time 0.0021134684994467533\n",
      "loss 0.05451875925064087 average time 0.0021275673997588457\n",
      "loss 0.0897880271077156 average time 0.002092856999952346\n",
      "loss 0.10561633110046387 average time 0.002084743285418621\n",
      "loss 0.09921287000179291 average time 0.0021093481243588032\n",
      "loss 0.5554277300834656 average time 0.002086550888294975\n",
      "loss 0.040501005947589874 average time 0.0020674776999512687\n",
      "loss 0.08877342194318771 average time 0.0018618749978486448\n",
      "loss 0.9410423636436462 average time 0.0018591389997163787\n",
      "loss 0.5026141405105591 average time 0.001862754001825427\n",
      "loss 0.17839990556240082 average time 0.0018469092537998222\n",
      "loss 0.03415033221244812 average time 0.0018422278044745327\n",
      "loss 0.09919197857379913 average time 0.0018452156705704207\n",
      "loss 0.1554015725851059 average time 0.001859514147675197\n",
      "loss 0.07696954160928726 average time 0.0018596757539489771\n",
      "loss 0.7460629343986511 average time 0.00185360167033246\n",
      "loss 0.0393422394990921 average time 0.0018514565026853233\n",
      "loss 0.11394102871417999 average time 0.001899751991732046\n",
      "loss 0.871315598487854 average time 0.00189977599773556\n",
      "loss 0.47709932923316956 average time 0.0018732753322304536\n",
      "loss 0.5363085865974426 average time 0.0018869317491771653\n",
      "loss 0.029892563819885254 average time 0.0018907708001788706\n",
      "loss 0.09021103382110596 average time 0.0018880201654974372\n",
      "loss 0.1698026955127716 average time 0.0018854672842592533\n",
      "loss 0.06693999469280243 average time 0.001886540624254849\n",
      "loss 0.8810988664627075 average time 0.001875641000094927\n",
      "loss 0.041858162730932236 average time 0.0018678738998714835\n",
      "loss 0.06489264965057373 average time 0.0018158859910909087\n",
      "loss 0.26742491126060486 average time 0.0018767209979705512\n",
      "loss 0.32923251390457153 average time 0.0019101183329864094\n",
      "loss 0.7533310651779175 average time 0.0018911749986000358\n",
      "loss 0.03099786676466465 average time 0.0018850581988226623\n",
      "loss 0.08529817312955856 average time 0.0018778196648539353\n",
      "loss 0.16510899364948273 average time 0.0018695797119289636\n",
      "loss 0.06075897440314293 average time 0.0018716772479820064\n",
      "loss 0.8892940878868103 average time 0.0018697143313733653\n",
      "loss 0.041145361959934235 average time 0.0018726534976158291\n",
      "loss 0.0863848552107811 average time 0.001961756001692265\n",
      "loss 0.3767685890197754 average time 0.0018806979974033311\n",
      "loss 0.34650179743766785 average time 0.0018873939950329562\n",
      "loss 0.3571278154850006 average time 0.0018744032469112425\n",
      "loss 0.02856789343059063 average time 0.001876595197012648\n",
      "loss 0.07721151411533356 average time 0.0018676068307831883\n",
      "loss 0.15947477519512177 average time 0.0018752099977739688\n",
      "loss 0.06175334379076958 average time 0.0018892748729558663\n",
      "loss 0.839490532875061 average time 0.001889122887643882\n",
      "loss 0.038652002811431885 average time 0.001889825198915787\n",
      "loss 0.16684095561504364 average time 0.0019247889996040612\n",
      "loss 0.676023006439209 average time 0.0019005585019476713\n",
      "loss 0.3828579783439636 average time 0.0018824169986570874\n",
      "loss 0.12971217930316925 average time 0.001871238747262396\n",
      "loss 0.0299991425126791 average time 0.001878917397931218\n",
      "loss 0.09424415975809097 average time 0.0018901519985714307\n",
      "loss 0.15096382796764374 average time 0.0018867837132087776\n",
      "loss 0.06364555656909943 average time 0.0018853772491274868\n",
      "loss 0.7935978770256042 average time 0.0018815647774479455\n",
      "loss 0.03944455087184906 average time 0.001881242899922654\n",
      "loss 0.08336039632558823 average time 0.001799300997518003\n",
      "loss 0.4567130208015442 average time 0.001827874996815808\n",
      "loss 0.4154495298862457 average time 0.001872300332179293\n",
      "loss 0.2950610816478729 average time 0.0018804982485016808\n",
      "loss 0.027844805270433426 average time 0.001875853599049151\n",
      "loss 0.08183836936950684 average time 0.0018682909983908757\n",
      "loss 0.1532236784696579 average time 0.0018844705692026764\n",
      "loss 0.0638151690363884 average time 0.0018779064976843073\n",
      "loss 0.8244280815124512 average time 0.0018692099979509497\n",
      "loss 0.03812239319086075 average time 0.0018678632982773707\n",
      "loss 0.07890737056732178 average time 0.0019050920009613037\n",
      "loss 0.941821277141571 average time 0.0019368875009240583\n",
      "loss 1.4173258543014526 average time 0.0019301063338449847\n",
      "loss 1.5963939428329468 average time 0.0019045727504999376\n",
      "loss 0.1929316371679306 average time 0.0018883068000432105\n",
      "loss 0.234140545129776 average time 0.0018759723337522397\n",
      "loss 0.4955635070800781 average time 0.0018674775706936737\n",
      "loss 0.08190330117940903 average time 0.001870486124389572\n",
      "loss 0.43042635917663574 average time 0.0018841665557637398\n",
      "loss 0.1540611833333969 average time 0.0018860038003185764\n",
      "loss 0.48769089579582214 average time 0.0018201099976431578\n",
      "loss 1.0159544944763184 average time 0.0018289580021519213\n",
      "loss 0.32233595848083496 average time 0.0018432916654273867\n",
      "loss 0.7105808258056641 average time 0.0018523567516240292\n",
      "loss 0.11252059042453766 average time 0.0018522814009338617\n",
      "loss 0.025141632184386253 average time 0.0018755365007867416\n",
      "loss 0.30763140320777893 average time 0.0018962374291316207\n",
      "loss 0.053447455167770386 average time 0.0018908177499542943\n",
      "loss 0.1556316465139389 average time 0.0018873623342046306\n",
      "loss 0.027242541313171387 average time 0.0018883513003820553\n",
      "loss 0.12305779755115509 average time 0.001834399001672864\n",
      "loss 0.480435848236084 average time 0.0018878115003462881\n",
      "loss 0.21356192231178284 average time 0.001889904336615776\n",
      "loss 0.1275487244129181 average time 0.0018780825030989944\n",
      "loss 0.05775349214673042 average time 0.001894792201463133\n",
      "loss 0.015299526043236256 average time 0.0018934923338626202\n",
      "loss 0.20743079483509064 average time 0.001885883857695652\n",
      "loss 0.06554168462753296 average time 0.0018819032503233756\n",
      "loss 0.4422174394130707 average time 0.001882838223181251\n",
      "loss 0.027953190729022026 average time 0.0018757320003351197\n",
      "loss 0.10836289077997208 average time 0.0018903280002996326\n",
      "loss 0.2164994329214096 average time 0.0019386714982101694\n",
      "loss 0.23397696018218994 average time 0.0019459506663649033\n",
      "loss 0.0761604830622673 average time 0.0019301052490482108\n",
      "loss 0.05100839212536812 average time 0.0019001045993063599\n",
      "loss 0.016915712505578995 average time 0.0018868139997357503\n",
      "loss 0.18895146250724792 average time 0.0018817579997370817\n",
      "loss 0.0540623664855957 average time 0.0018770044999837409\n",
      "loss 0.5211078524589539 average time 0.001871678554970357\n",
      "loss 0.0320032499730587 average time 0.0018785600990522653\n",
      "loss 0.11220341175794601 average time 0.001960259998450056\n",
      "loss 0.2419641762971878 average time 0.0019311049993848428\n",
      "loss 0.23936378955841064 average time 0.0019319243310019375\n",
      "loss 0.07728337496519089 average time 0.0019139679978252388\n",
      "loss 0.05386941507458687 average time 0.0018993373971898108\n",
      "loss 0.020261336117982864 average time 0.0018928653308345625\n",
      "loss 0.18046985566616058 average time 0.0018926788547209332\n",
      "loss 0.05319806560873985 average time 0.0018960548733593897\n",
      "loss 0.5916549563407898 average time 0.0018948611099686887\n",
      "loss 0.03496412932872772 average time 0.001885780499665998\n",
      "loss 0.11370624601840973 average time 0.0018302570085506887\n",
      "loss 0.26370346546173096 average time 0.0019001075043343007\n",
      "loss 0.257070392370224 average time 0.0018919593344132106\n",
      "loss 0.09572933614253998 average time 0.0019273647511727178\n",
      "loss 0.046363454312086105 average time 0.0019837384005077182\n",
      "loss 0.02371278591454029 average time 0.0019906016673970346\n",
      "loss 0.18113063275814056 average time 0.0019975665728894196\n",
      "loss 0.04925988242030144 average time 0.0019807470015075525\n",
      "loss 0.6612778902053833 average time 0.0019690115567451965\n",
      "loss 0.03490076959133148 average time 0.0019596492007840425\n",
      "loss 0.11226411163806915 average time 0.0018941729911603033\n",
      "loss 0.26817786693573 average time 0.0019450520002283157\n",
      "loss 0.27946075797080994 average time 0.0019618436647579076\n",
      "loss 0.09422799944877625 average time 0.001973916247661691\n",
      "loss 0.0487002432346344 average time 0.0019738991986960174\n",
      "loss 0.02592037059366703 average time 0.0019770569975177447\n",
      "loss 0.18275709450244904 average time 0.0019756114258364377\n",
      "loss 0.048897337168455124 average time 0.001960124998440733\n",
      "loss 0.6853134036064148 average time 0.0019530376654842661\n",
      "loss 0.03418279439210892 average time 0.00195252949942369\n",
      "loss 0.09867129474878311 average time 0.0020358690060675146\n",
      "loss 0.261513352394104 average time 0.0019866694969823584\n",
      "loss 0.30753064155578613 average time 0.0019375006602300952\n",
      "loss 0.09368237107992172 average time 0.00190385349385906\n",
      "loss 0.05548841878771782 average time 0.0019063691962510347\n",
      "loss 0.028498906642198563 average time 0.0019132124956619616\n",
      "loss 0.18068814277648926 average time 0.0019279289964054312\n",
      "loss 0.05366915464401245 average time 0.0019338289966981392\n",
      "loss 0.6677476763725281 average time 0.0019345066630436728\n",
      "loss 0.03356039151549339 average time 0.0019278146969154478\n",
      "loss 0.11661875247955322 average time 0.0019874999963212757\n",
      "loss 0.2586164176464081 average time 0.0019510504993377254\n",
      "loss 0.2863241732120514 average time 0.0019271746673621237\n",
      "loss 0.10167485475540161 average time 0.00193179075315129\n",
      "loss 0.0674169585108757 average time 0.0019229922036174685\n",
      "loss 0.03380688652396202 average time 0.001953690835701612\n",
      "loss 0.1741952747106552 average time 0.0019642987165467015\n",
      "loss 0.05950891599059105 average time 0.0019683295015420297\n",
      "loss 0.6409792304039001 average time 0.0019587417801893835\n",
      "loss 0.03364036977291107 average time 0.0019596670023165645\n",
      "loss 0.16590850055217743 average time 0.0020403079909738155\n",
      "loss 0.2538547217845917 average time 0.002032600996317342\n",
      "loss 0.33503204584121704 average time 0.00208151633075128\n",
      "loss 0.12113749235868454 average time 0.0020759612487745473\n",
      "loss 0.07691115140914917 average time 0.002069775600684807\n",
      "loss 0.04135361313819885 average time 0.0020472146692918614\n",
      "loss 0.1597016155719757 average time 0.002023437144900007\n",
      "loss 0.0629841610789299 average time 0.00202209050199599\n",
      "loss 0.6219956278800964 average time 0.0020235714459947\n",
      "loss 0.033259712159633636 average time 0.00202244740142487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 100000\n",
       "\tepoch: 100\n",
       "\tepoch_length: 1000\n",
       "\tmax_epochs: 100\n",
       "\toutput: 0.033259712159633636\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.handlers import Timer\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "\n",
    "from snow_model import Net\n",
    "# from snow_model_module import Net\n",
    "# from snow_model_2 import Net\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device used : {device}\")\n",
    "\n",
    "# from cupy_dataset import OptionDataSet\n",
    "timer = Timer(average=True)\n",
    "model = Net().cuda()\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "# dataset = OptionDataSet(max_len=10000, number_path = 1024, batch=4800)\n",
    "# dataset = SnowballDataSet(max_len = 50000, number_path = 500000, batch = 4, threads = 512, seed  =1999 )\n",
    "# dataset = datasets.DatasetFolder(\n",
    "#     root=path,\n",
    "#     loader=npy_loader,\n",
    "#     extensions=['.pt']\n",
    "# )\n",
    "# dataset = atDataset()\n",
    "dataset = DataLoader(tdataset, 1000)\n",
    "\n",
    "# dataset size is 10,000\n",
    "def train_update(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = batch[0]\n",
    "    y = batch[1]\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred[:,0], y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_update)\n",
    "log_interval = 100\n",
    "\n",
    "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "timer.attach(trainer,\n",
    "             start=Events.EPOCH_STARTED,\n",
    "             resume=Events.ITERATION_STARTED,\n",
    "             pause=Events.ITERATION_COMPLETED,\n",
    "             step=Events.ITERATION_COMPLETED)    \n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
    "    if iter % log_interval == 0:\n",
    "        print('loss', engine.state.output, 'average time', timer.value())\n",
    "\n",
    "# @trainer.on(Events.GET_BATCH_STARTED)\n",
    "# def log_training_loss(engine):\n",
    "#     print(\"EPOCH!!!!!!!!!!!!\\n\")\n",
    "        \n",
    "# trainer.run(dataset, max_epochs=100)\n",
    "trainer.run(dataset, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_cudf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mset({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe.backend\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudf\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask_cudf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdelayed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_cuda\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalCUDACluster\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dask_cudf'"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "dask.config.set({\"dataframe.backend\": \"cudf\"})\n",
    "import dask_cudf\n",
    "from dask.delayed import delayed\n",
    "from dask_cuda import LocalCUDACluster\n",
    "cluster = LocalCUDACluster()\n",
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
